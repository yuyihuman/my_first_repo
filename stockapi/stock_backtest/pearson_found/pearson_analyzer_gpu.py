"""
è‚¡ç¥¨æ•°æ®Pearsonç›¸å…³ç³»æ•°åˆ†æè„šæœ¬ - GPUæ‰¹é‡è¯„æµ‹ç‰ˆæœ¬

è¯¥è„šæœ¬æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œé€šè¿‡ä¸‰ç»´çŸ©é˜µè¿ç®—å¤§å¹…æå‡GPUåˆ©ç”¨ç‡ã€‚
ç›¸æ¯”å•æ—¥è¯„æµ‹ç‰ˆæœ¬ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œå®ç°æ›´é«˜çš„å¹¶è¡Œè®¡ç®—æ•ˆç‡ã€‚

åŠŸèƒ½ï¼š
1. æ”¯æŒæ‰¹é‡è¯„æµ‹æ—¥æœŸå‚æ•°ï¼ˆevaluation_daysï¼‰
2. ä¸‰ç»´GPUçŸ©é˜µè¿ç®—ï¼š[è¯„æµ‹æ—¥æœŸæ•°, çª—å£å¤§å°, å­—æ®µæ•°]
3. æ‰¹é‡è®¡ç®—æ‰€æœ‰è¯„æµ‹æ—¥æœŸçš„Pearsonç›¸å…³ç³»æ•°
4. æ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œé¿å…GPUå†…å­˜æº¢å‡º
5. æ‰¹é‡ç»“æœç»Ÿè®¡å’ŒCSVå¯¼å‡º
6. GPUæ˜¾å­˜ç›‘æ§å’Œè‡ªé€‚åº”åˆ†ç»„å¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
python pearson_analyzer_gpu.py 000001 --evaluation_days 100

ä½œè€…ï¼šStock Backtest System
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
GPUæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼š2024å¹´
"""

import argparse
import logging
import os
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from scipy.stats import pearsonr
from data_loader import StockDataLoader
import matplotlib.pyplot as plt
import mplfinance as mpf
from stock_config import get_comparison_stocks
import time
import threading
from collections import defaultdict
import warnings
import gc
import multiprocessing as mp
from functools import partial

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)


def _process_stock_historical_data_worker(args):
    """
    å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šå¤„ç†å•åªè‚¡ç¥¨çš„å†å²æ•°æ®
    
    Args:
        args: (stock_code, stock_data, window_size, fields, debug)
    
    Returns:
        tuple: (stock_code, historical_data_list, stats)
    """
    stock_code, stock_data, window_size, fields, debug = args
    
    historical_data = []
    stock_valid_periods = 0
    stock_invalid_periods = 0
    
    try:
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
        available_data = stock_data
        
        if len(available_data) < window_size:
            return stock_code, [], {'valid_periods': 0, 'invalid_periods': 0, 'skipped': True}
        
        # ç”Ÿæˆè¯¥è‚¡ç¥¨çš„å†å²æœŸé—´å¹¶ç›´æ¥è¿›è¡Œç­›é€‰å’Œé¢„å¤„ç†
        for i in range(len(available_data) - window_size + 1):
            period_data = available_data.iloc[i:i + window_size]
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
            if len(period_data) == window_size:
                start_date = period_data.index[0]
                end_date = period_data.index[-1]
                
                # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                historical_values = period_data[fields].values
                
                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                historical_data.append((historical_values, start_date, end_date, stock_code))
                stock_valid_periods += 1
            else:
                stock_invalid_periods += 1
        
        return stock_code, historical_data, {
            'valid_periods': stock_valid_periods, 
            'invalid_periods': stock_invalid_periods, 
            'skipped': False
        }
        
    except Exception as e:
        if debug:
            print(f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
        return stock_code, [], {'valid_periods': 0, 'invalid_periods': 0, 'error': str(e)}


class GPUBatchPearsonAnalyzer:
    def __init__(self, stock_code, log_dir='logs', window_size=15, threshold=0.85, 
                 evaluation_days=1, debug=False, comparison_stocks=None, 
                 comparison_mode='top10', backtest_date=None, 
                 csv_filename='evaluation_results.csv', use_gpu=True, 
                 batch_size=1000, gpu_memory_limit=0.8, earliest_date='2020-01-01',
                 num_processes=None):
        """
        åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            log_dir: æ—¥å¿—ç›®å½•
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥æ•°é‡ï¼‰
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡ï¼ˆä»backtest_dateå¾€å‰æ•°çš„äº¤æ˜“æ—¥æ•°ï¼‰
            debug: æ˜¯å¦å¼€å¯debugæ¨¡å¼
            comparison_stocks: è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            backtest_date: å›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
            csv_filename: CSVç»“æœæ–‡ä»¶å
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            batch_size: GPUæ‰¹å¤„ç†å¤§å°
            gpu_memory_limit: GPUå†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆ0.0-1.0ï¼‰
            earliest_date: æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (æ ¼å¼: YYYY-MM-DDï¼Œé»˜è®¤: 2020-01-01)
            num_processes: å¤šè¿›ç¨‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼‰
        """
        self.stock_code = stock_code
        
        # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
        script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
        self.log_dir = os.path.join(script_dir, 'logs')
        self.csv_results_file = os.path.join(script_dir, csv_filename)
        
        self.window_size = window_size
        self.threshold = threshold
        self.evaluation_days = evaluation_days  # æ–°å¢ï¼šè¯„æµ‹æ—¥æœŸæ•°é‡
        self.debug = debug
        self.comparison_mode = comparison_mode
        self.backtest_date = pd.to_datetime(backtest_date) if backtest_date else None
        self.earliest_date = pd.to_datetime(earliest_date)
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.gpu_memory_limit = gpu_memory_limit
        self.data_loader = None
        self.logger = None
        
        # å¤šè¿›ç¨‹è®¾ç½®
        self.num_processes = num_processes if num_processes is not None else max(1, mp.cpu_count() - 1)
        
        # GPUè®¾å¤‡è®¾ç½®
        self.device = self._setup_device()
        
        # GPUæ˜¾å­˜ç›‘æ§
        self.gpu_memory_stats = {
            'peak_allocated': 0,
            'peak_reserved': 0,
            'current_allocated': 0,
            'current_reserved': 0
        }
        
        # è®¾ç½®å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        if comparison_stocks:
            self.comparison_stocks = comparison_stocks
        elif comparison_mode == 'self_only':
            self.comparison_stocks = [stock_code]
        else:
            self.comparison_stocks = get_comparison_stocks(comparison_mode)
            if stock_code in self.comparison_stocks:
                self.comparison_stocks.remove(stock_code)
        
        # å­˜å‚¨å·²åŠ è½½çš„è‚¡ç¥¨æ•°æ®
        self.loaded_stocks_data = {}
        
        # æ€§èƒ½è®¡æ—¶å™¨
        self.performance_timers = defaultdict(list)
        self.current_timers = {}
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # è®¾ç½®CSVæ–‡ä»¶
        self._setup_csv_file()
        
        self.logger.info(f"åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {stock_code}")
        self.logger.info(f"çª—å£å¤§å°: {window_size}, é˜ˆå€¼: {threshold}, è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        self.logger.info(f"GPUå†…å­˜é™åˆ¶: {gpu_memory_limit*100:.0f}%")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {comparison_mode}, å¯¹æ¯”è‚¡ç¥¨æ•°é‡: {len(self.comparison_stocks)}")
    
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰"""
        if self.use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if self.debug:
                print(f"ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name} ({gpu_memory:.1f}GB)")
            return device
        else:
            if self.use_gpu:
                print("è­¦å‘Šï¼šCUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")
            else:
                print("ä½¿ç”¨CPUè®¡ç®—")
            return torch.device('cpu')
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        stock_log_dir = os.path.join(self.log_dir, self.stock_code)
        os.makedirs(stock_log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        thread_id = threading.get_ident()
        log_filename = f"batch_pearson_analysis_{self.stock_code}_{timestamp}_thread_{thread_id}.log"
        log_path = os.path.join(stock_log_dir, log_filename)
        
        self.logger = logging.getLogger(f'GPUBatchPearsonAnalyzer_{self.stock_code}')
        self.logger.setLevel(logging.INFO)
        
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        file_handler = logging.FileHandler(log_path, encoding='utf-8-sig')
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.info(f"æ‰¹é‡è¯„æµ‹æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_path}")
    
    def _setup_csv_file(self):
        """è®¾ç½®CSVæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(self.csv_results_file):
            # ä½¿ç”¨ä¸å•æ—¥è„šæœ¬ç›¸åŒçš„è¡¨å¤´æ ¼å¼
            header = ['ä»£ç ', 'window_size', 'é˜ˆå€¼', 'è¯„æµ‹æ—¥æœŸ', 'å¯¹æ¯”è‚¡ç¥¨æ•°é‡', 'ç›¸å…³æ•°é‡', 
                     'ä¸‹1æ—¥é«˜å¼€', 'ä¸‹1æ—¥ä¸Šæ¶¨', 'ä¸‹3æ—¥ä¸Šæ¶¨', 'ä¸‹5æ—¥ä¸Šæ¶¨', 'ä¸‹10æ—¥ä¸Šæ¶¨']
            df = pd.DataFrame(columns=header)
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                self.logger.info(f"ğŸ†• Debug: æ‰¹é‡è¯„æµ‹CSVæ–‡ä»¶åˆ›å»ºå®Œæˆ: {self.csv_results_file}")
    
    def start_timer(self, timer_name, parent_timer=None):
        """
        å¼€å§‹è®¡æ—¶
        
        Args:
            timer_name: è®¡æ—¶å™¨åç§°
            parent_timer: çˆ¶è®¡æ—¶å™¨åç§°ï¼ˆç”¨äºåˆ†å±‚æ˜¾ç¤ºï¼‰
        """
        self.current_timers[timer_name] = {
            'start_time': time.time(),
            'parent': parent_timer
        }
        if self.debug:
            self.logger.info(f"â±ï¸ å¼€å§‹è®¡æ—¶: {timer_name}")
    
    def end_timer(self, timer_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if timer_name in self.current_timers:
            timer_info = self.current_timers[timer_name]
            elapsed_time = time.time() - timer_info['start_time']
            
            # å­˜å‚¨è®¡æ—¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬çˆ¶è®¡æ—¶å™¨ä¿¡æ¯
            if timer_name not in self.performance_timers:
                self.performance_timers[timer_name] = []
            
            self.performance_timers[timer_name].append({
                'elapsed_time': elapsed_time,
                'parent': timer_info['parent'],
                'timestamp': time.time()
            })
            
            del self.current_timers[timer_name]
            if self.debug:
                self.logger.info(f"â±ï¸ ç»“æŸè®¡æ—¶: {timer_name} - è€—æ—¶: {elapsed_time:.3f}ç§’")
            return elapsed_time
        return 0
    
    def load_data(self):
        """åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®"""
        self.start_timer('target_stock_loading')
        self.logger.info("ğŸ“Š æ•°æ®åŠ è½½ä¸­...")
        self.data_loader = StockDataLoader()
        
        data = self.data_loader.load_stock_data(self.stock_code)
        
        if data is None or data.empty:
            self.logger.error(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
            self.end_timer('target_stock_loading')
            return None
        
        self.data = self._filter_data(data, self.stock_code)
        self.logger.info(f"âœ… ç›®æ ‡è‚¡ç¥¨ {self.stock_code} æ•°æ®åŠ è½½å®Œæˆ ({len(self.data)} æ¡è®°å½•)")
        self.end_timer('target_stock_loading')
        
        # åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        self._load_comparison_stocks_data()
        
        return self.data
    
    def _filter_data(self, data, stock_code):
        """è¿‡æ»¤è‚¡ç¥¨æ•°æ®ï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œæ—¥æœŸèŒƒå›´"""
        if data is None or data.empty:
            return data
            
        original_count = len(data)
        
        # é¦–å…ˆæŒ‰æ—¥æœŸè¿‡æ»¤
        data = data[data.index >= self.earliest_date]
        date_filtered_count = len(data)
        date_removed_count = original_count - date_filtered_count
        
        # ç„¶åæŒ‰æ•°æ®è´¨é‡è¿‡æ»¤
        data = data[
            (data['open'] > 0) & 
            (data['high'] > 0) & 
            (data['low'] > 0) & 
            (data['close'] > 0) & 
            (data['volume'] > 0)
        ]
        final_count = len(data)
        quality_removed_count = date_filtered_count - final_count
        
        if date_removed_count > 0:
            self.logger.debug(f"è‚¡ç¥¨ {stock_code} æ—¥æœŸè¿‡æ»¤å®Œæˆï¼Œç§»é™¤æ—©äº {self.earliest_date.strftime('%Y-%m-%d')} çš„ {date_removed_count} æ¡æ•°æ®")
        
        if quality_removed_count > 0:
            self.logger.debug(f"è‚¡ç¥¨ {stock_code} æ•°æ®è´¨é‡è¿‡æ»¤å®Œæˆï¼Œç§»é™¤ {quality_removed_count} æ¡å¼‚å¸¸æ•°æ®")
        
        if not data.empty:
            self.logger.debug(f"è‚¡ç¥¨ {stock_code} æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        
        return data
    
    def _load_comparison_stocks_data(self):
        """åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®"""
        if self.comparison_mode == 'self_only':
            self.logger.info("ğŸ“ˆ ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”æ¨¡å¼")
            return
        
        self.start_timer('comparison_stocks_loading')
        self.logger.info(f"ğŸ“ˆ åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®ä¸­... ({len(self.comparison_stocks)} åª)")
        successful_loads = 0
        
        for stock_code in self.comparison_stocks:
            try:
                if self.debug:
                    self.logger.info(f"æ­£åœ¨åŠ è½½å¯¹æ¯”è‚¡ç¥¨: {stock_code}")
                
                data = self.data_loader.load_stock_data(stock_code)
                if data is not None and not data.empty:
                    filtered_data = self._filter_data(data, stock_code)
                    if not filtered_data.empty:
                        self.loaded_stocks_data[stock_code] = filtered_data
                        successful_loads += 1
                    else:
                        if self.debug:
                            self.logger.warning(f"è‚¡ç¥¨ {stock_code} è¿‡æ»¤åæ•°æ®ä¸ºç©º")
                else:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"âœ… å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½å®Œæˆ ({successful_loads}/{len(self.comparison_stocks)} åª)")
        self.end_timer('comparison_stocks_loading')
    
    def prepare_evaluation_dates(self, end_date):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        
        Args:
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            list: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        """
        self.start_timer('evaluation_dates_preparation')
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„äº¤æ˜“æ—¥æœŸï¼ˆåŒ…å«end_dateå½“å¤©ï¼Œå¦‚æœæ•°æ®å¯ç”¨ï¼‰
        available_dates = self.data[self.data.index <= end_date].index
        
        if len(available_dates) < self.evaluation_days + self.window_size:
            self.logger.warning(f"å¯ç”¨æ•°æ®ä¸è¶³ï¼Œéœ€è¦ {self.evaluation_days + self.window_size} ä¸ªäº¤æ˜“æ—¥ï¼Œ"
                              f"å®é™…åªæœ‰ {len(available_dates)} ä¸ª")
            # è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡
            self.evaluation_days = max(1, len(available_dates) - self.window_size)
            self.logger.info(f"è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡ä¸º: {self.evaluation_days}")
        
        # é€‰æ‹©æœ€è¿‘çš„evaluation_daysä¸ªäº¤æ˜“æ—¥ä½œä¸ºè¯„æµ‹æ—¥æœŸ
        evaluation_dates = available_dates[-self.evaluation_days:].tolist()
        
        self.logger.info(f"å‡†å¤‡äº† {len(evaluation_dates)} ä¸ªè¯„æµ‹æ—¥æœŸ")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸèŒƒå›´: {evaluation_dates[0]} åˆ° {evaluation_dates[-1]}")
        
        self.end_timer('evaluation_dates_preparation')
        return evaluation_dates
    
    def prepare_batch_evaluation_data(self, evaluation_dates):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®çŸ©é˜µ
        
        Args:
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            torch.Tensor: å½¢çŠ¶ä¸º [evaluation_days, window_size, 5] çš„è¯„æµ‹æ•°æ®å¼ é‡
        """
        self.start_timer('batch_data_preparation')
        
        fields = ['open', 'high', 'low', 'close', 'volume']
        batch_data_list = []
        valid_dates = []
        
        for eval_date in evaluation_dates:
            # è·å–è¯¥è¯„æµ‹æ—¥æœŸçš„çª—å£æ•°æ®ï¼ˆåŒ…å«è¯„æµ‹æ—¥æœŸå½“å¤©ï¼‰
            recent_data = self.data[self.data.index <= eval_date].tail(self.window_size)
            
            if len(recent_data) == self.window_size:
                # æå–å­—æ®µæ•°æ®
                data_values = recent_data[fields].values  # [window_size, 5]
                batch_data_list.append(data_values)
                valid_dates.append(eval_date)
            else:
                if self.debug:
                    self.logger.warning(f"è¯„æµ‹æ—¥æœŸ {eval_date} çš„æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
        
        if not batch_data_list:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ•°æ®")
            self.end_timer('batch_data_preparation')
            return None, []
        
        # è½¬æ¢ä¸ºå¼ é‡ [evaluation_days, window_size, 5]
        batch_data = np.stack(batch_data_list, axis=0)
        batch_tensor = torch.tensor(batch_data, dtype=torch.float32, device=self.device)
        
        self.logger.info(f"æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {batch_tensor.shape}")
        self.logger.info(f"æœ‰æ•ˆè¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
        
        self.end_timer('batch_data_preparation')
        return batch_tensor, valid_dates
    
    def calculate_batch_gpu_correlation(self, batch_recent_data, historical_periods_data, evaluation_dates=None):
        """
        æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—
        
        Args:
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® [evaluation_days, window_size, 5]
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            dict: æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        """
        
        if batch_recent_data is None or len(historical_periods_data) == 0:
            return {}
        
        evaluation_days, window_size, num_fields = batch_recent_data.shape
        num_historical_periods = len(historical_periods_data)
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}, å†å²æœŸé—´æ•°: {num_historical_periods}")
        
        # å­æ­¥éª¤1/5: å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼šæ•°æ®åœ¨é˜¶æ®µ3å·²é¢„å¤„ç†ï¼‰
        self.start_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/5] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å¼€å§‹")
        
        # æ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼Œç›´æ¥æå–
        historical_data_list = []
        period_info_list = []
        
        for historical_values, start_date, end_date, stock_code in historical_periods_data:
            historical_data_list.append(historical_values)
            period_info_list.append({
                'start_date': start_date,
                'end_date': end_date,
                'stock_code': stock_code
            })
        
        valid_periods = len(historical_data_list)
        self.logger.info(f"å†å²æ•°æ®å‡†å¤‡å®Œæˆ: æœ‰æ•ˆæœŸé—´={valid_periods}ï¼ˆæ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼‰")
        self.end_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/5] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å®Œæˆ")
        
        if not historical_data_list:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return {}
        
        # å­æ­¥éª¤2/5: åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡
        self.start_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/5] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å¼€å§‹")
        self.logger.info(f"å¼ é‡å½¢çŠ¶å°†ä¸º: [{len(historical_data_list)}, {window_size}, 5]")
        
        historical_tensor = torch.tensor(
            np.stack(historical_data_list, axis=0), 
            dtype=torch.float32, 
            device=self.device
        )  # [num_historical_periods, window_size, 5]
        
        self.logger.info(f"GPUå†å²æ•°æ®å¼ é‡åˆ›å»ºå®Œæˆ: {historical_tensor.shape}, è®¾å¤‡: {historical_tensor.device}")
        self.end_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/5] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å®Œæˆ")
        
        # ç›‘æ§æ•°æ®å¼ é‡åˆ›å»ºåçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("å¼ é‡åˆ›å»ºå®Œæˆ")
        
        # å­æ­¥éª¤3/5: æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®—
        self.start_timer('gpu_step3_correlation_calculation')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/5] æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®— - å¼€å§‹")
        self.logger.info(f"è¾“å…¥å¼ é‡å½¢çŠ¶: batch_recent_data={batch_recent_data.shape}, historical_tensor={historical_tensor.shape}")
        self.logger.info(f"ç›®æ ‡è¾“å‡ºå½¢çŠ¶: [{evaluation_days}, {historical_tensor.shape[0]}, 5]")
        
        batch_correlations = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        batch_size = min(self.batch_size, evaluation_days)
        total_batches = (evaluation_days + batch_size - 1) // batch_size
        
        self.logger.info(f"åˆ†æ‰¹è®¡ç®—é…ç½®: batch_size={batch_size}, total_batches={total_batches}")
        
        for batch_idx, i in enumerate(range(0, evaluation_days, batch_size)):
            end_idx = min(i + batch_size, evaluation_days)
            current_batch = batch_recent_data[i:end_idx]  # [batch_size, window_size, 5]
            
            self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: è¯„æµ‹æ—¥æœŸ {i+1}-{end_idx} (å½¢çŠ¶: {current_batch.shape})")
            
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç›¸å…³ç³»æ•°
            batch_corr = self._compute_correlation_matrix(current_batch, historical_tensor)
            batch_correlations.append(batch_corr)
            
            # ç›‘æ§æ¯ä¸ªæ‰¹æ¬¡åçš„GPUæ˜¾å­˜
            if batch_idx % max(1, total_batches // 5) == 0:  # æ¯20%è¿›åº¦ç›‘æ§ä¸€æ¬¡
                self.monitor_gpu_memory(f"æ‰¹æ¬¡{batch_idx + 1}å®Œæˆ")
        
        self.end_timer('gpu_step3_correlation_calculation')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/5] æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®— - å®Œæˆ")
        
        # å­æ­¥éª¤4/5: åˆå¹¶æ‰¹æ¬¡ç»“æœ
        self.start_timer('gpu_step4_batch_merging')
        self.logger.info(f"  ğŸ”— [å­æ­¥éª¤4/5] åˆå¹¶æ‰¹æ¬¡ç»“æœ - å¼€å§‹")
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_correlations = torch.cat(batch_correlations, dim=0)  # [evaluation_days, num_historical_periods, 5]
        self.logger.info(f"æ‰¹æ¬¡ç»“æœåˆå¹¶å®Œæˆ: æœ€ç»ˆå½¢çŠ¶={all_correlations.shape}")
        self.end_timer('gpu_step4_batch_merging')
        self.logger.info(f"  ğŸ”— [å­æ­¥éª¤4/5] åˆå¹¶æ‰¹æ¬¡ç»“æœ - å®Œæˆ")
        
        # ç›‘æ§ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆåçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆ")
        
        self.logger.info(f"æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {all_correlations.shape}")
        
        # å­æ­¥éª¤5/5: å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        self.start_timer('gpu_step5_result_processing')
        self.logger.info(f"  ğŸ“‹ [å­æ­¥éª¤5/5] å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ - å¼€å§‹")
        self.logger.info(f"è°ƒç”¨å‡½æ•°: _process_batch_correlation_results")
        results = self._process_batch_correlation_results(
            all_correlations, period_info_list, evaluation_days,
            batch_recent_data, historical_data_list, evaluation_dates
        )
        self.end_timer('gpu_step5_result_processing')
        self.logger.info(f"  ğŸ“‹ [å­æ­¥éª¤5/5] å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ - å®Œæˆ")
        
        self.logger.info(f"æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å…¨éƒ¨å®Œæˆï¼Œè¿”å›ç»“æœåŒ…å« {len(results) if results else 0} ä¸ªå­—æ®µ")
        return results
    
    def _compute_correlation_matrix(self, recent_batch, historical_tensor):
        """
        è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        
        Args:
            recent_batch: [batch_size, window_size, 5]
            historical_tensor: [num_historical_periods, window_size, 5]
            
        Returns:
            torch.Tensor: [batch_size, num_historical_periods, 5]
        """
        batch_size, window_size, num_fields = recent_batch.shape
        num_historical_periods = historical_tensor.shape[0]
        
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] å¼€å§‹ç›¸å…³ç³»æ•°çŸ©é˜µè®¡ç®— - _compute_correlation_matrix")
            self.logger.debug(f"    è¾“å…¥å½¢çŠ¶: recent_batch={recent_batch.shape}, historical_tensor={historical_tensor.shape}")
        
        # æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­è®¡ç®—
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤1: æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­")
        recent_expanded = recent_batch.unsqueeze(1)  # [batch_size, 1, window_size, 5]
        historical_expanded = historical_tensor.unsqueeze(0)  # [1, num_historical_periods, window_size, 5]
        
        # è®¡ç®—å‡å€¼
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤2: è®¡ç®—å‡å€¼")
        recent_mean = recent_expanded.mean(dim=2, keepdim=True)  # [batch_size, 1, 1, 5]
        historical_mean = historical_expanded.mean(dim=2, keepdim=True)  # [1, num_historical_periods, 1, 5]
        
        # ä¸­å¿ƒåŒ–
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤3: æ•°æ®ä¸­å¿ƒåŒ–")
        recent_centered = recent_expanded - recent_mean
        historical_centered = historical_expanded - historical_mean
        
        # è®¡ç®—åæ–¹å·®
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤4: è®¡ç®—åæ–¹å·®")
        covariance = (recent_centered * historical_centered).sum(dim=2)  # [batch_size, num_historical_periods, 5]
        
        # è®¡ç®—æ ‡å‡†å·®
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤5: è®¡ç®—æ ‡å‡†å·®")
        recent_std = torch.sqrt((recent_centered ** 2).sum(dim=2))  # [batch_size, 1, 5]
        historical_std = torch.sqrt((historical_centered ** 2).sum(dim=2))  # [1, num_historical_periods, 5]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤6: è®¡ç®—æœ€ç»ˆç›¸å…³ç³»æ•°")
        correlation = covariance / (recent_std * historical_std + 1e-8)
        
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {correlation.shape}")
        
        return correlation
    
    def _process_batch_correlation_results(self, correlations_tensor, period_info_list, evaluation_days,
                                          batch_recent_data=None, historical_data_list=None, evaluation_dates=None):
        """
        å¤„ç†æ‰¹é‡ç›¸å…³æ€§è®¡ç®—ç»“æœï¼ˆæ•´åˆäº†é˜¶æ®µ5çš„è¯¦ç»†ç»“æœå¤„ç†å’Œä¿å­˜åŠŸèƒ½ï¼‰
        
        Args:
            correlations_tensor: [evaluation_days, num_historical_periods, 5]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            dict: å¤„ç†åçš„å®Œæ•´æœ€ç»ˆç»“æœï¼ŒåŒ…å«è¯¦ç»†ç»“æœã€ç»Ÿè®¡ä¿¡æ¯å’Œæ€§èƒ½æ•°æ®
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„è®¡æ—¶å™¨ï¼Œè¦†ç›–åŸæ¥çš„4-5å’Œ5-1æ­¥éª¤
        self.start_timer('integrated_result_processing')
        
        correlations_np = correlations_tensor.cpu().numpy()
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
        avg_correlations = correlations_np.mean(axis=2)
        
        # è¿‡æ»¤æ‰ç›¸å…³æ€§ä¸º1.0çš„ç»“æœï¼ˆè‡ªç›¸å…³ï¼‰
        # è®¾ç½®å®¹å·®ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
        self_correlation_threshold = 0.9999
        self_correlation_mask = avg_correlations >= self_correlation_threshold
        
        # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è‡ªç›¸å…³æ•°é‡
        filtered_count = self_correlation_mask.sum()
        if filtered_count > 0:
            self.logger.info(f"è¿‡æ»¤æ‰ {filtered_count} ä¸ªè‡ªç›¸å…³ç»“æœï¼ˆç›¸å…³æ€§ >= {self_correlation_threshold}ï¼‰")
        
        # å°†è‡ªç›¸å…³çš„ä½ç½®è®¾ç½®ä¸º0ï¼Œä½¿å…¶ä¸ä¼šè¢«é€‰ä¸ºé«˜ç›¸å…³æ€§æœŸé—´
        avg_correlations_filtered = avg_correlations.copy()
        avg_correlations_filtered[self_correlation_mask] = 0.0
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§æœŸé—´ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°ï¼‰
        high_corr_mask = avg_correlations_filtered > self.threshold
        
        # Debugæ¨¡å¼ä¸‹æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        if self.debug:
            self._print_detailed_evaluation_data(
                correlations_np, avg_correlations_filtered, period_info_list, 
                high_corr_mask, fields, batch_recent_data, historical_data_list, evaluation_dates
            )
        
        # æ„å»ºè¯¦ç»†ç»“æœï¼ˆæ•´åˆé˜¶æ®µ5çš„åŠŸèƒ½ï¼‰
        detailed_results = []
        
        if evaluation_dates:
            for eval_idx, eval_date in enumerate(evaluation_dates):
                if eval_idx < avg_correlations_filtered.shape[0]:
                    eval_correlations = avg_correlations_filtered[eval_idx]  # è¯¥è¯„æµ‹æ—¥æœŸçš„ç›¸å…³æ€§åˆ—è¡¨
                    
                    # æ‰¾åˆ°é«˜ç›¸å…³æ€§æœŸé—´
                    high_corr_periods = []
                    for hist_idx, correlation in enumerate(eval_correlations):
                        if correlation >= self.threshold and hist_idx < len(period_info_list):
                            period_data = period_info_list[hist_idx]
                            
                            high_corr_periods.append({
                                'start_date': period_data['start_date'],
                                'end_date': period_data['end_date'],
                                'avg_correlation': float(correlation),
                                'stock_code': period_data['stock_code'],
                                'source': 'gpu_batch'
                            })
                    
                    # è®¡ç®—è¯¥è¯„æµ‹æ—¥æœŸçš„é¢„æµ‹ç»Ÿè®¡
                    stats = self.calculate_future_performance_stats(self.data, high_corr_periods)
                    
                    detailed_results.append({
                        'evaluation_date': eval_date,
                        'high_correlation_periods': high_corr_periods,
                        'daily_high_count': len(high_corr_periods),
                        'prediction_stats': stats
                    })
        
        # æ„å»ºæ‰¹é‡ç»“æœ
        batch_results = {
            'evaluation_days': evaluation_days,
            'num_historical_periods': len(period_info_list),
            'high_correlation_counts': high_corr_mask.sum(axis=1).tolist(),  # æ¯ä¸ªè¯„æµ‹æ—¥æœŸçš„é«˜ç›¸å…³æ•°é‡
            'avg_correlations': avg_correlations_filtered.tolist(),  # ä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°
            'detailed_correlations': correlations_np.tolist(),
            'period_info': period_info_list,
            'detailed_results': detailed_results,  # æ–°å¢ï¼šè¯¦ç»†ç»“æœï¼ˆæ•´åˆé˜¶æ®µ5åŠŸèƒ½ï¼‰
            'summary': {
                'total_high_correlations': high_corr_mask.sum(),
                'avg_high_correlations_per_day': high_corr_mask.sum(axis=1).mean(),
                'max_high_correlations_per_day': high_corr_mask.sum(axis=1).max(),
                'overall_avg_correlation': avg_correlations_filtered[high_corr_mask].mean() if high_corr_mask.any() else 0,
                'filtered_self_correlations': int(filtered_count)  # æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
            }
        }
        
        # æ•´åˆåŸé˜¶æ®µ5çš„åŠŸèƒ½ï¼šæ„å»ºæœ€ç»ˆç»“æœå¹¶ä¿å­˜
        final_result = {
            'stock_code': self.stock_code,
            'backtest_date': self.backtest_date,
            'evaluation_days': len(evaluation_dates) if evaluation_dates else evaluation_days,
            'window_size': self.window_size,
            'threshold': self.threshold,
            'evaluation_dates': evaluation_dates if evaluation_dates else [],
            'batch_results': batch_results,
            'performance_stats': self._get_performance_stats()
        }
        
        # ä¿å­˜ç»“æœåˆ°CSVï¼ˆåŸé˜¶æ®µ5çš„åŠŸèƒ½ï¼‰
        if hasattr(self, 'save_results') and self.save_results:
            self.save_batch_results_to_csv(final_result)
        
        self.logger.info(f"æ‰¹é‡ç»“æœå¤„ç†å®Œæˆï¼ˆå·²æ•´åˆè¯¦ç»†ç»“æœå¤„ç†å’Œä¿å­˜åŠŸèƒ½ï¼‰")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {batch_results['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°: {batch_results['summary']['avg_high_correlations_per_day']:.2f}")
        
        self.end_timer('integrated_result_processing')
        return final_result
    
    def _print_detailed_evaluation_data(self, correlations_np, avg_correlations_filtered, 
                                       period_info_list, high_corr_mask, fields,
                                       batch_recent_data=None, historical_data_list=None, evaluation_dates=None):
        """
        æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯¹æ¯”æ•°ç»„
        
        Args:
            correlations_np: è¯¦ç»†ç›¸å…³ç³»æ•°æ•°ç»„ [evaluation_days, num_historical_periods, 5]
            avg_correlations_filtered: è¿‡æ»¤åçš„å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            high_corr_mask: é«˜ç›¸å…³æ€§æ©ç 
            fields: å­—æ®µåç§°åˆ—è¡¨
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® [evaluation_days, window_size, 5]
            historical_data_list: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        """
        self.logger.info("=" * 80)
        self.logger.info("DEBUGæ¨¡å¼ - å‰10æ¡è¯„æµ‹æ•°æ®è¯¦ç»†ä¿¡æ¯:")
        self.logger.info("=" * 80)
        
        evaluation_days, num_historical_periods, num_fields = correlations_np.shape
        max_display_count = min(10, evaluation_days * num_historical_periods)
        
        # æ”¶é›†å‰10æ¡è¯„æµ‹æ•°æ®ï¼ˆæŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºï¼‰
        all_evaluation_data = []
        count = 0
        
        # æŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºéå†ï¼Œæ¯ä¸ªè¯„æµ‹æ—¥æœŸå–ç¬¬ä¸€ä¸ªå†å²æœŸé—´çš„æ•°æ®
        for eval_idx in range(evaluation_days):
            if count >= 10:  # åªå–å‰10æ¡
                break
            for hist_idx in range(num_historical_periods):
                if count >= 10:  # åªå–å‰10æ¡
                    break
                    
                avg_corr = avg_correlations_filtered[eval_idx, hist_idx]
                detailed_corr = correlations_np[eval_idx, hist_idx]
                is_high_corr = high_corr_mask[eval_idx, hist_idx]
                
                period_info = period_info_list[hist_idx]
                
                all_evaluation_data.append({
                    'eval_idx': eval_idx,
                    'hist_idx': hist_idx,
                    'avg_correlation': avg_corr,
                    'detailed_correlations': detailed_corr,
                    'is_high_correlation': is_high_corr,
                    'period_info': period_info
                })
                count += 1
        
        # æ‰“å°å‰10æ¡æ•°æ®ï¼ˆæŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºï¼‰
        for i, data in enumerate(all_evaluation_data):
            self.logger.info(f"\nç¬¬ {i+1} æ¡è¯„æµ‹æ•°æ®:")
            self.logger.info(f"  è¯„æµ‹æ—¥æœŸç´¢å¼•: {data['eval_idx']}")
            
            # æ·»åŠ è¯„æµ‹æ•°æ®æ—¶é—´æ®µä¿¡æ¯
            if evaluation_dates and data['eval_idx'] < len(evaluation_dates):
                eval_date = evaluation_dates[data['eval_idx']]
                # è®¡ç®—è¯„æµ‹æ•°æ®çš„æ—¶é—´æ®µï¼ˆä»è¯„æµ‹æ—¥æœŸå¾€å‰æ¨window_sizeå¤©ï¼‰
                eval_start_date = eval_date - pd.Timedelta(days=self.window_size - 1)
                self.logger.info(f"  è¯„æµ‹æ•°æ®æ—¶é—´æ®µ: {eval_start_date.strftime('%Y-%m-%d')} åˆ° {eval_date.strftime('%Y-%m-%d')}")
            
            self.logger.info(f"  å†å²æœŸé—´ç´¢å¼•: {data['hist_idx']}")
            self.logger.info(f"  å†å²æœŸé—´: {data['period_info']['start_date'].strftime('%Y-%m-%d')} åˆ° {data['period_info']['end_date'].strftime('%Y-%m-%d')}")
            self.logger.info(f"  æ¥æºè‚¡ç¥¨: {data['period_info']['stock_code']}")
            self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {data['avg_correlation']:.6f}")
            self.logger.info(f"  æ˜¯å¦é«˜ç›¸å…³: {'æ˜¯' if data['is_high_correlation'] else 'å¦'}")
            
            # æ‰“å°å„å­—æ®µçš„è¯¦ç»†ç›¸å…³ç³»æ•°
            self.logger.info("  å„å­—æ®µç›¸å…³ç³»æ•°:")
            for j, field in enumerate(fields):
                self.logger.info(f"    {field}: {data['detailed_correlations'][j]:.6f}")
            
            # æ‰“å°å¯¹æ¯”æ•°ç»„ï¼ˆå¦‚æœæœ‰åŸå§‹æ•°æ®ï¼‰
            if batch_recent_data is not None and historical_data_list is not None:
                eval_idx = data['eval_idx']
                hist_idx = data['hist_idx']
                
                # è·å–è¯„æµ‹æ•°æ®ï¼ˆè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼‰
                recent_data = batch_recent_data[eval_idx]  # [window_size, 5]
                if isinstance(recent_data, torch.Tensor):
                    recent_data = recent_data.cpu().numpy()
                
                # è·å–å†å²æ•°æ®
                if hist_idx < len(historical_data_list):
                    historical_data = historical_data_list[hist_idx]  # [window_size, 5]
                    if isinstance(historical_data, torch.Tensor):
                        historical_data = historical_data.cpu().numpy()
                    
                    self.logger.info("  å¯¹æ¯”æ•°ç»„è¯¦æƒ…:")
                    self.logger.info(f"    æ•°æ®çª—å£å¤§å°: {recent_data.shape[0]} å¤©")
                    
                    # æ‰“å°å‰5å¤©å’Œå5å¤©çš„æ•°æ®å¯¹æ¯”
                    for field_idx, field in enumerate(fields):
                        self.logger.info(f"    {field} å­—æ®µå¯¹æ¯”:")
                        self.logger.info(f"      è¯„æµ‹æ•°æ®å‰5å¤©: {recent_data[:5, field_idx].tolist()}")
                        self.logger.info(f"      å†å²æ•°æ®å‰5å¤©: {historical_data[:5, field_idx].tolist()}")
                        self.logger.info(f"      è¯„æµ‹æ•°æ®å5å¤©: {recent_data[-5:, field_idx].tolist()}")
                        self.logger.info(f"      å†å²æ•°æ®å5å¤©: {historical_data[-5:, field_idx].tolist()}")
                        
                        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                        recent_mean = np.mean(recent_data[:, field_idx])
                        historical_mean = np.mean(historical_data[:, field_idx])
                        recent_std = np.std(recent_data[:, field_idx])
                        historical_std = np.std(historical_data[:, field_idx])
                        
                        self.logger.info(f"      è¯„æµ‹æ•°æ®ç»Ÿè®¡ - å‡å€¼: {recent_mean:.4f}, æ ‡å‡†å·®: {recent_std:.4f}")
                        self.logger.info(f"      å†å²æ•°æ®ç»Ÿè®¡ - å‡å€¼: {historical_mean:.4f}, æ ‡å‡†å·®: {historical_std:.4f}")
            
            self.logger.info("-" * 60)
        
        self.logger.info("=" * 80)
    
    def calculate_future_performance_stats(self, data, high_correlation_periods):
        """
        è®¡ç®—é«˜ç›¸å…³æ€§æœŸé—´çš„æœªæ¥äº¤æ˜“æ—¥è¡¨ç°ç»Ÿè®¡
        
        Args:
            data: å®Œæ•´çš„è‚¡ç¥¨æ•°æ®
            high_correlation_periods: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
            
        Returns:
            dict: ç»Ÿè®¡ç»“æœ
        """
        if not high_correlation_periods:
            return None
        
        stats = {
            'total_periods': len(high_correlation_periods),
            'next_day_gap_up': 0,  # ä¸‹1ä¸ªäº¤æ˜“æ—¥é«˜å¼€
            'next_1_day_up': 0,    # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_3_day_up': 0,    # ä¸‹3ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_5_day_up': 0,    # ä¸‹5ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_10_day_up': 0,   # ä¸‹10ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'valid_periods': {
                'next_day': 0,
                'next_3_day': 0,
                'next_5_day': 0,
                'next_10_day': 0
            }
        }
        
        for i, period in enumerate(high_correlation_periods, 1):
            end_date = period['end_date']
            start_date = period['start_date']
            avg_correlation = period['avg_correlation']
            source_stock_code = period['stock_code']
            
            # æ ¹æ®æ¥æºè‚¡ç¥¨ä»£ç è·å–æ­£ç¡®çš„æ•°æ®æº
            if source_stock_code == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                source_data = self.loaded_stocks_data.get(source_stock_code)
                if source_data is None:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {source_stock_code} çš„æ•°æ®ï¼Œè·³è¿‡æœŸé—´ #{i}")
                    continue
            
            # æ‰¾åˆ°è¯¥æœŸé—´ç»“æŸåçš„æ•°æ®ä½ç½®
            try:
                end_idx = source_data.index.get_loc(end_date)
            except KeyError:
                if self.debug:
                    self.logger.warning(f"åœ¨è‚¡ç¥¨ {source_stock_code} æ•°æ®ä¸­æ‰¾ä¸åˆ°æ—¥æœŸ {end_date}ï¼Œè·³è¿‡æœŸé—´ #{i}")
                continue
            
            # è·å–æœŸé—´æœ€åä¸€å¤©çš„æ”¶ç›˜ä»·
            period_close = source_data.iloc[end_idx]['close']
            
            # æ£€æŸ¥ä¸‹1ä¸ªäº¤æ˜“æ—¥
            if end_idx + 1 < len(source_data):
                next_day_data = source_data.iloc[end_idx + 1]
                next_day_open = next_day_data['open']
                next_day_close = next_day_data['close']
                
                stats['valid_periods']['next_day'] += 1
                
                # é«˜å¼€åˆ¤æ–­
                if next_day_open > period_close:
                    stats['next_day_gap_up'] += 1
                
                # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨åˆ¤æ–­
                if next_day_close > period_close:
                    stats['next_1_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹3ä¸ªäº¤æ˜“æ—¥
            if end_idx + 3 < len(source_data):
                day_3_close = source_data.iloc[end_idx + 3]['close']
                stats['valid_periods']['next_3_day'] += 1
                
                if day_3_close > period_close:
                    stats['next_3_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹5ä¸ªäº¤æ˜“æ—¥
            if end_idx + 5 < len(source_data):
                day_5_close = source_data.iloc[end_idx + 5]['close']
                stats['valid_periods']['next_5_day'] += 1
                
                if day_5_close > period_close:
                    stats['next_5_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹10ä¸ªäº¤æ˜“æ—¥
            if end_idx + 10 < len(source_data):
                day_10_close = source_data.iloc[end_idx + 10]['close']
                stats['valid_periods']['next_10_day'] += 1
                
                if day_10_close > period_close:
                    stats['next_10_day_up'] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        stats['ratios'] = {}
        if stats['valid_periods']['next_day'] > 0:
            stats['ratios']['next_day_gap_up'] = stats['next_day_gap_up'] / stats['valid_periods']['next_day']
            stats['ratios']['next_1_day_up'] = stats['next_1_day_up'] / stats['valid_periods']['next_day']
        
        if stats['valid_periods']['next_3_day'] > 0:
            stats['ratios']['next_3_day_up'] = stats['next_3_day_up'] / stats['valid_periods']['next_3_day']
        
        if stats['valid_periods']['next_5_day'] > 0:
            stats['ratios']['next_5_day_up'] = stats['next_5_day_up'] / stats['valid_periods']['next_5_day']
        
        if stats['valid_periods']['next_10_day'] > 0:
            stats['ratios']['next_10_day_up'] = stats['next_10_day_up'] / stats['valid_periods']['next_10_day']
        
        return stats
    

    
    def analyze_batch(self, backtest_date=None, evaluation_days=None, window_size=None, 
                     threshold=None, comparison_mode=None, comparison_stocks=None, debug=None):
        """
        æ‰¹é‡åˆ†æä¸»å‡½æ•°
        
        Args:
            backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            window_size: çª—å£å¤§å°
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            debug: è°ƒè¯•æ¨¡å¼
            
        Returns:
            dict: æ‰¹é‡åˆ†æç»“æœ
        """
        self.start_timer('total_batch_analysis')
        
        # æ›´æ–°å‚æ•°
        if backtest_date is not None:
            self.backtest_date = pd.to_datetime(backtest_date)
        if evaluation_days is not None:
            self.evaluation_days = evaluation_days
        if window_size is not None:
            self.window_size = window_size
        if threshold is not None:
            self.threshold = threshold
        if comparison_mode is not None:
            self.comparison_mode = comparison_mode
        if comparison_stocks is not None:
            self.comparison_stocks = comparison_stocks
        if debug is not None:
            self.debug = debug
        
        self.logger.info("=" * 80)
        self.logger.info(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ")
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"å›æµ‹ç»“æŸæ—¥æœŸ: {self.backtest_date}")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {self.evaluation_days}")
        self.logger.info(f"çª—å£å¤§å°: {self.window_size}")
        self.logger.info(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {self.threshold}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {self.comparison_mode}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}")
        self.logger.info("=" * 80)
        
        # åˆå§‹GPUæ˜¾å­˜ç›‘æ§
        self.monitor_gpu_memory("åˆ†æå¼€å§‹")
        
        # ğŸ”„ ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½ - å¼€å§‹
        self.logger.info("ğŸ”„ [é˜¶æ®µ1/6] æ•°æ®åŠ è½½ - å¼€å§‹")
        if not hasattr(self, 'data') or self.data is None:
            self.data = self.load_data()
            if self.data is None:
                self.logger.error("æ•°æ®åŠ è½½å¤±è´¥")
                return None
        self.logger.info("ğŸ”„ [é˜¶æ®µ1/5] æ•°æ®åŠ è½½ - å®Œæˆ")
        
        # ğŸ“‹ ç¬¬2é˜¶æ®µï¼šæ•°æ®å‡†å¤‡ - å¼€å§‹
        self.logger.info("ğŸ“‹ [é˜¶æ®µ2/5] æ•°æ®å‡†å¤‡ - å¼€å§‹")
        evaluation_dates = self.prepare_evaluation_dates(self.backtest_date)
        
        if not evaluation_dates:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ—¥æœŸ")
            return None
        
        # å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®
        batch_recent_data, valid_dates = self.prepare_batch_evaluation_data(evaluation_dates)
        
        if batch_recent_data is None:
            self.logger.error("æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å¤±è´¥")
            return None
        
        # ç›‘æ§æ•°æ®å‡†å¤‡åçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("æ•°æ®å‡†å¤‡å®Œæˆ")
        self.logger.info("ğŸ“‹ [é˜¶æ®µ2/5] æ•°æ®å‡†å¤‡ - å®Œæˆ")
        
        # ğŸ“š ç¬¬3é˜¶æ®µï¼šå†å²æ•°æ®æ”¶é›† - å¼€å§‹
        self.logger.info("ğŸ“š [é˜¶æ®µ3/5] å†å²æ•°æ®æ”¶é›† - å¼€å§‹")
        earliest_eval_date = min(valid_dates)
        historical_periods_data = self._collect_historical_periods_data(earliest_eval_date)
        
        if not historical_periods_data:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return None
        self.logger.info("ğŸ“š [é˜¶æ®µ3/5] å†å²æ•°æ®æ”¶é›† - å®Œæˆ")
        
        # ğŸš€ ç¬¬4é˜¶æ®µï¼šGPUè®¡ç®—ä¸ç»“æœå¤„ç† - å¼€å§‹ï¼ˆæ•´åˆäº†åŸé˜¶æ®µ4-5å’Œ5ï¼‰
        self.logger.info("ğŸš€ [é˜¶æ®µ4/5] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å¼€å§‹")
        self.monitor_gpu_memory("GPUè®¡ç®—å¼€å§‹")
        batch_correlations = self.calculate_batch_gpu_correlation(batch_recent_data, historical_periods_data, valid_dates)
        self.monitor_gpu_memory("GPUè®¡ç®—å®Œæˆ")
        self.logger.info("ğŸš€ [é˜¶æ®µ4/5] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å®Œæˆ")
        
        if not batch_correlations:
            self.logger.error("æ‰¹é‡ç›¸å…³æ€§è®¡ç®—å¤±è´¥")
            return None
        
        # ğŸ“Š ç¬¬5é˜¶æ®µï¼šæœ€ç»ˆå¤„ç† - å·²æ•´åˆåˆ°é˜¶æ®µ4-5ä¸­
        self.logger.info("ğŸ“Š [é˜¶æ®µ5/5] æœ€ç»ˆå¤„ç† - å·²æ•´åˆå®Œæˆ")
        
        # ç›´æ¥ä½¿ç”¨é˜¶æ®µ4-5çš„æ•´åˆç»“æœï¼ˆå·²åŒ…å«ä¿å­˜å’Œæœ€ç»ˆç»“æœæ„å»ºï¼‰
        final_result = batch_correlations
        
        self.end_timer('total_batch_analysis')
        
        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        self._log_performance_summary()
        
        # æœ€ç»ˆGPUæ˜¾å­˜ç›‘æ§
        self.monitor_gpu_memory("åˆ†æå®Œæˆ")
        self.logger.info("ğŸ“Š [é˜¶æ®µ5/5] æœ€ç»ˆå¤„ç† - å®Œæˆ")
        
        # è¾“å‡ºåˆ†ææ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("æ‰¹é‡åˆ†æç»“æœæ€»ç»“:")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {final_result['batch_results']['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {final_result['batch_results']['summary']['avg_high_correlations_per_day']:.2f}")
        self.logger.info(f"æœ€å¤§æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {final_result['batch_results']['summary']['max_high_correlations_per_day']}")
        if final_result['batch_results']['summary']['overall_avg_correlation'] > 0:
            self.logger.info(f"æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°: {final_result['batch_results']['summary']['overall_avg_correlation']:.4f}")
        
        # æŸ¥æ‰¾å¹¶æ‰“å°ç›¸å…³ç³»æ•°æœ€å¤§çš„æ¡ç›®
        max_correlation = 0
        max_correlation_item = None
        max_eval_date = None
        
        for result in final_result['batch_results']['detailed_results']:
            for period in result['high_correlation_periods']:
                if period['avg_correlation'] > max_correlation:
                    max_correlation = period['avg_correlation']
                    max_correlation_item = period
                    max_eval_date = result['evaluation_date']
        
        if max_correlation_item:
            self.logger.info("=" * 40)
            self.logger.info("ç›¸å…³ç³»æ•°æœ€å¤§çš„æ¡ç›®:")
            self.logger.info(f"è¯„æµ‹æ—¥æœŸ: {max_eval_date.strftime('%Y-%m-%d')}")
            self.logger.info(f"å†å²æœŸé—´: {max_correlation_item['start_date'].strftime('%Y-%m-%d')} åˆ° {max_correlation_item['end_date'].strftime('%Y-%m-%d')}")
            self.logger.info(f"ç›¸å…³ç³»æ•°: {max_correlation_item['avg_correlation']:.6f}")
            self.logger.info(f"æ¥æºè‚¡ç¥¨: {max_correlation_item['stock_code']}")
            self.logger.info(f"æ•°æ®æ¥æº: {max_correlation_item['source']}")
            self.logger.info("=" * 40)
        
        self.logger.info("=" * 80)
        
        return final_result
    
    def _collect_historical_periods_data(self, earliest_eval_date):
        """æ”¶é›†å†å²æœŸé—´æ•°æ®"""
        self.start_timer('historical_data_collection')
        
        historical_periods_data = []
        
        # åœ¨allæ¨¡å¼ä¸‹ï¼Œè‡ªèº«å†å²æ•°æ®å·²ç»åŒ…å«åœ¨å¯¹æ¯”è‚¡ç¥¨æ•°æ®ä¸­ï¼Œæ— éœ€å•ç‹¬æ”¶é›†
        if self.comparison_mode != 'all':
            # æ”¶é›†è‡ªèº«å†å²æ•°æ®
            self_historical_data = self._collect_self_historical_data(earliest_eval_date)
            historical_periods_data.extend(self_historical_data)
        
        # æ”¶é›†å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        if self.comparison_mode != 'self_only':
            # æ ¹æ®è‚¡ç¥¨æ•°é‡å†³å®šæ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
            if len(self.loaded_stocks_data) >= 10 and self.num_processes > 1:
                comparison_historical_data = self._collect_comparison_historical_data_multiprocess(earliest_eval_date)
            else:
                comparison_historical_data = self._collect_comparison_historical_data(earliest_eval_date)
            historical_periods_data.extend(comparison_historical_data)
        
        self.logger.info(f"æ”¶é›†åˆ° {len(historical_periods_data)} ä¸ªå†å²æœŸé—´æ•°æ®")
        self.end_timer('historical_data_collection')
        return historical_periods_data
    
    def _collect_self_historical_data(self, earliest_eval_date):
        """æ”¶é›†è‡ªèº«å†å²æ•°æ®ï¼ˆå·²ä¼˜åŒ–ï¼šç›´æ¥ç­›é€‰å’Œé¢„å¤„ç†ï¼‰"""
        historical_data = []
        valid_periods = 0
        invalid_periods = 0
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸è¿›è¡Œæ—¥æœŸæˆªæ–­
        available_data = self.data
        
        if len(available_data) < self.window_size:
            self.logger.info(f"è‡ªèº«æ•°æ®é•¿åº¦ {len(available_data)} å°äºçª—å£å¤§å° {self.window_size}ï¼Œè·³è¿‡")
            return historical_data
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # ç”Ÿæˆå†å²æœŸé—´å¹¶ç›´æ¥è¿›è¡Œç­›é€‰å’Œé¢„å¤„ç†
        for i in range(len(available_data) - self.window_size + 1):
            period_data = available_data.iloc[i:i + self.window_size]
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
            if len(period_data) == self.window_size:
                start_date = period_data.index[0]
                end_date = period_data.index[-1]
                
                # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                historical_values = period_data[fields].values
                
                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                historical_data.append((historical_values, start_date, end_date, self.stock_code))
                valid_periods += 1
            else:
                invalid_periods += 1
        
        self.logger.info(f"è‡ªèº«å†å²æ•°æ®æ”¶é›†å®Œæˆ: æœ‰æ•ˆæœŸé—´={valid_periods}, æ— æ•ˆæœŸé—´={invalid_periods}")
        return historical_data
    
    def _collect_comparison_historical_data(self, earliest_eval_date):
        """æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå·²ä¼˜åŒ–ï¼šç›´æ¥ç­›é€‰å’Œé¢„å¤„ç†ï¼‰"""
        historical_data = []
        total_valid_periods = 0
        total_invalid_periods = 0
        processed_stocks = 0
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        for stock_code, stock_data in self.loaded_stocks_data.items():
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸è¿›è¡Œæ—¥æœŸæˆªæ–­
            available_data = stock_data
            
            if len(available_data) < self.window_size:
                if self.debug:
                    self.logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®é•¿åº¦ {len(available_data)} å°äºçª—å£å¤§å° {self.window_size}ï¼Œè·³è¿‡")
                continue
            
            stock_valid_periods = 0
            stock_invalid_periods = 0
            
            # ç”Ÿæˆè¯¥è‚¡ç¥¨çš„å†å²æœŸé—´å¹¶ç›´æ¥è¿›è¡Œç­›é€‰å’Œé¢„å¤„ç†
            for i in range(len(available_data) - self.window_size + 1):
                period_data = available_data.iloc[i:i + self.window_size]
                
                # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
                if len(period_data) == self.window_size:
                    start_date = period_data.index[0]
                    end_date = period_data.index[-1]
                    
                    # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                    historical_values = period_data[fields].values
                    
                    # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                    historical_data.append((historical_values, start_date, end_date, stock_code))
                    stock_valid_periods += 1
                    total_valid_periods += 1
                else:
                    stock_invalid_periods += 1
                    total_invalid_periods += 1
            
            processed_stocks += 1
            
            # æ¯å¤„ç†100åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            if processed_stocks % 100 == 0:
                self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨æ•°æ®æ”¶é›†è¿›åº¦: {processed_stocks}/{len(self.loaded_stocks_data)} åªè‚¡ç¥¨")
        
        self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®æ”¶é›†å®Œæˆ: å¤„ç†è‚¡ç¥¨={processed_stocks}, æœ‰æ•ˆæœŸé—´={total_valid_periods}, æ— æ•ˆæœŸé—´={total_invalid_periods}")
        return historical_data
    
    def _collect_comparison_historical_data_multiprocess(self, earliest_eval_date):
        """æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
        if not self.loaded_stocks_data:
            return []
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # å‡†å¤‡å¤šè¿›ç¨‹ä»»åŠ¡å‚æ•°
        tasks = []
        for stock_code, stock_data in self.loaded_stocks_data.items():
            tasks.append((stock_code, stock_data, self.window_size, fields, self.debug))
        
        self.logger.info(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†: {len(tasks)} åªè‚¡ç¥¨ï¼Œ{self.num_processes} ä¸ªè¿›ç¨‹")
        
        historical_data = []
        total_valid_periods = 0
        total_invalid_periods = 0
        processed_stocks = 0
        
        try:
            # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†ä»»åŠ¡
            with mp.Pool(processes=self.num_processes) as pool:
                # åˆ†æ‰¹å¤„ç†ä»¥æ˜¾ç¤ºè¿›åº¦
                batch_size = max(1, len(tasks) // 10)  # åˆ†æˆ10æ‰¹æ˜¾ç¤ºè¿›åº¦
                
                for i in range(0, len(tasks), batch_size):
                    batch_tasks = tasks[i:i + batch_size]
                    batch_results = pool.map(_process_stock_historical_data_worker, batch_tasks)
                    
                    # å¤„ç†æ‰¹æ¬¡ç»“æœ
                    for stock_code, stock_historical_data, stats in batch_results:
                        if 'error' in stats:
                            if self.debug:
                                self.logger.warning(f"è‚¡ç¥¨ {stock_code} å¤„ç†å‡ºé”™: {stats['error']}")
                            continue
                        
                        if stats.get('skipped', False):
                            if self.debug:
                                self.logger.debug(f"è‚¡ç¥¨ {stock_code} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                            continue
                        
                        # æ·»åŠ åˆ°æ€»ç»“æœä¸­
                        historical_data.extend(stock_historical_data)
                        total_valid_periods += stats['valid_periods']
                        total_invalid_periods += stats['invalid_periods']
                        processed_stocks += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = min(i + batch_size, len(tasks))
                    self.logger.info(f"ğŸ“Š å¤šè¿›ç¨‹å¤„ç†è¿›åº¦: {progress}/{len(tasks)} åªè‚¡ç¥¨ ({progress/len(tasks)*100:.1f}%)")
        
        except Exception as e:
            self.logger.error(f"å¤šè¿›ç¨‹å¤„ç†å‡ºé”™ï¼Œå›é€€åˆ°å•è¿›ç¨‹æ¨¡å¼: {str(e)}")
            return self._collect_comparison_historical_data(earliest_eval_date)
        
        self.logger.info(f"âœ… å¤šè¿›ç¨‹å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®æ”¶é›†å®Œæˆ: å¤„ç†è‚¡ç¥¨={processed_stocks}, æœ‰æ•ˆæœŸé—´={total_valid_periods}, æ— æ•ˆæœŸé—´={total_invalid_periods}")
        return historical_data
    

    

    

    

    

    
    def monitor_gpu_memory(self, stage_name):
        """ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == 'cuda':
            # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            current_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            current_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            
            # æ›´æ–°å³°å€¼è®°å½•
            self.gpu_memory_stats['peak_allocated'] = max(
                self.gpu_memory_stats['peak_allocated'], current_allocated
            )
            self.gpu_memory_stats['peak_reserved'] = max(
                self.gpu_memory_stats['peak_reserved'], current_reserved
            )
            
            # æ›´æ–°å½“å‰å€¼
            self.gpu_memory_stats['current_allocated'] = current_allocated
            self.gpu_memory_stats['current_reserved'] = current_reserved
            
            # è®°å½•æ—¥å¿—
            self.logger.info(f"ğŸ” GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]:")
            self.logger.info(f"   å½“å‰å·²åˆ†é…: {current_allocated:.2f}GB")
            self.logger.info(f"   å½“å‰å·²ä¿ç•™: {current_reserved:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ç‡
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            usage_rate = current_allocated / total_memory
            
            if usage_rate > 0.8:
                self.logger.warning(f"âš ï¸ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {usage_rate*100:.1f}%")
            elif usage_rate > 0.9:
                self.logger.error(f"âŒ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_rate*100:.1f}%ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º")
        else:
            self.logger.info(f"ğŸ” CPUæ¨¡å¼ï¼Œè·³è¿‡GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]")
    
    def estimate_memory_requirement(self, evaluation_days, num_stocks, window_size, num_fields):
        """ä¼°ç®—æ˜¾å­˜éœ€æ±‚ï¼ˆGBï¼‰"""
        # è®¡ç®—å¼ é‡å¤§å°
        # ç›®æ ‡æ•°æ®: [evaluation_days, window_size, num_fields]
        target_size = evaluation_days * window_size * num_fields * 4  # float32 = 4 bytes
        
        # å¯¹æ¯”æ•°æ®: [num_stocks, evaluation_days, window_size, num_fields]
        comparison_size = num_stocks * evaluation_days * window_size * num_fields * 4
        
        # ç›¸å…³ç³»æ•°ç»“æœ: [num_stocks, evaluation_days, num_fields]
        correlation_size = num_stocks * evaluation_days * num_fields * 4
        
        # ä¸­é—´è®¡ç®—ç¼“å­˜ï¼ˆä¼°ç®—ä¸º2å€ï¼‰
        intermediate_size = (target_size + comparison_size) * 2
        
        # æ€»æ˜¾å­˜éœ€æ±‚
        total_bytes = target_size + comparison_size + correlation_size + intermediate_size
        total_gb = total_bytes / 1024**3
        
        self.logger.info(f"ğŸ“Š æ˜¾å­˜éœ€æ±‚ä¼°ç®—:")
        self.logger.info(f"   è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"   è‚¡ç¥¨æ•°é‡: {num_stocks}")
        self.logger.info(f"   çª—å£å¤§å°: {window_size}")
        self.logger.info(f"   é¢„è®¡æ˜¾å­˜éœ€æ±‚: {total_gb:.2f}GB")
        
        return total_gb
    
    def check_gpu_memory_limit(self, required_memory_gb):
        """æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if self.device.type != 'cuda':
            return True  # CPUæ¨¡å¼ä¸å—æ˜¾å­˜é™åˆ¶
        
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        available_memory = total_memory * self.gpu_memory_limit
        
        self.logger.info(f"ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
        self.logger.info(f"   æ€»æ˜¾å­˜: {total_memory:.2f}GB")
        self.logger.info(f"   å¯ç”¨æ˜¾å­˜: {available_memory:.2f}GB (é™åˆ¶: {self.gpu_memory_limit*100:.0f}%)")
        self.logger.info(f"   éœ€æ±‚æ˜¾å­˜: {required_memory_gb:.2f}GB")
        
        if required_memory_gb <= available_memory:
            self.logger.info(f"âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ç›´æ¥å¤„ç†")
            return True
        else:
            self.logger.warning(f"âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†")
            return False
    
    def _get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for timer_name, timer_records in self.performance_timers.items():
            if timer_records:
                # å¤„ç†æ–°çš„æ•°æ®ç»“æ„
                if isinstance(timer_records[0], dict):
                    elapsed_times = [record['elapsed_time'] for record in timer_records]
                    stats[timer_name] = {
                        'total_time': sum(elapsed_times),
                        'avg_time': sum(elapsed_times) / len(elapsed_times),
                        'max_time': max(elapsed_times),
                        'min_time': min(elapsed_times),
                        'count': len(elapsed_times),
                        'parent': timer_records[0]['parent'],
                        'timestamp': timer_records[0]['timestamp']
                    }
                else:
                    # å…¼å®¹æ—§çš„æ•°æ®ç»“æ„
                    stats[timer_name] = {
                        'total_time': sum(timer_records),
                        'avg_time': sum(timer_records) / len(timer_records),
                        'max_time': max(timer_records),
                        'min_time': min(timer_records),
                        'count': len(timer_records),
                        'parent': None,
                        'timestamp': time.time()
                    }
        
        # æ·»åŠ GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            stats['gpu_memory'] = self.gpu_memory_stats.copy()
        
        return stats
    
    def _log_performance_summary(self):
        """è¾“å‡ºåˆ†å±‚æ€§èƒ½æ€»ç»“"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š åˆ†å±‚æ€§èƒ½ç»Ÿè®¡æ€»ç»“ (æŒ‰æ‰§è¡Œé¡ºåº)")
        self.logger.info("=" * 80)
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = self._get_performance_stats()
        
        # å®šä¹‰æ­¥éª¤æ˜ å°„å’Œæ˜¾ç¤ºé¡ºåº
        step_mapping = {
            # ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½
            'target_stock_loading': ('1-1', 'ç›®æ ‡è‚¡ç¥¨æ•°æ®åŠ è½½'),
            'comparison_stocks_loading': ('1-2', 'å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½'),
            
            # ç¬¬2é˜¶æ®µï¼šæ•°æ®å‡†å¤‡
            'evaluation_dates_preparation': ('2-1', 'è¯„æµ‹æ—¥æœŸå‡†å¤‡'),
            'batch_data_preparation': ('2-2', 'æ‰¹é‡æ•°æ®å‡†å¤‡'),
            
            # ç¬¬3é˜¶æ®µï¼šå†å²æ•°æ®æ”¶é›†
            'historical_data_collection': ('3-1', 'å†å²æ•°æ®æ”¶é›†'),
            
            # ç¬¬4é˜¶æ®µï¼šGPUè®¡ç®—ï¼ˆè¯¦ç»†æ‹†åˆ†ä¸º5ä¸ªå­æ­¥éª¤ï¼‰
            'gpu_step1_data_preparation': ('4-1', 'å†å²æ•°æ®å‡†å¤‡å’Œç­›é€‰'),
            'gpu_step2_tensor_creation': ('4-2', 'åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡'),
            'gpu_step3_correlation_calculation': ('4-3', 'æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®—'),
            'gpu_step4_batch_merging': ('4-4', 'åˆå¹¶æ‰¹æ¬¡ç»“æœ'),
            'gpu_step5_result_processing': ('4-5', 'å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ'),
            
            # ç¬¬5é˜¶æ®µï¼šç»“æœå¤„ç†
            'batch_result_processing': ('5-1', 'ç›¸å…³æ€§ç»“æœå¤„ç†'),
            
            # ç¬¬6é˜¶æ®µï¼šæœ€ç»ˆå¤„ç†
            'batch_results_processing': ('6-1', 'æ‰¹é‡ç»“æœæ•´åˆ'),
            
            # æ€»ä½“ç»Ÿè®¡
            'total_batch_analysis': ('æ€»è®¡', 'å®Œæ•´æ‰¹é‡åˆ†æ')
        }
        
        # æŒ‰æ­¥éª¤é¡ºåºæ˜¾ç¤º
        current_stage = 0
        stage_names = {
            1: "ğŸ”„ ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½",
            2: "ğŸ“‹ ç¬¬2é˜¶æ®µï¼šæ•°æ®å‡†å¤‡", 
            3: "ğŸ“š ç¬¬3é˜¶æ®µï¼šå†å²æ•°æ®æ”¶é›†",
            4: "ğŸš€ ç¬¬4é˜¶æ®µï¼šGPUè®¡ç®—",
            5: "âš™ï¸  ç¬¬5é˜¶æ®µï¼šç»“æœå¤„ç†",
            6: "ğŸ“Š ç¬¬6é˜¶æ®µï¼šæœ€ç»ˆå¤„ç†"
        }
        
        for timer_name, (step_id, step_name) in step_mapping.items():
            if timer_name in stats:
                stat = stats[timer_name]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºæ–°çš„é˜¶æ®µæ ‡é¢˜
                if step_id != 'æ€»è®¡':
                    stage_num = int(step_id.split('-')[0])
                    if stage_num != current_stage:
                        if current_stage > 0:
                            self.logger.info("")  # ç©ºè¡Œåˆ†éš”
                        self.logger.info(stage_names[stage_num])
                        current_stage = stage_num
                
                # æ˜¾ç¤ºæ­¥éª¤ç»Ÿè®¡
                if step_id == 'æ€»è®¡':
                    self.logger.info("")
                    self.logger.info("=" * 40)
                    self.logger.info(f"ğŸ“ˆ {step_id} - {step_name}:")
                else:
                    self.logger.info(f"  {step_id} {step_name}:")
                
                self.logger.info(f"      æ€»è€—æ—¶: {stat['total_time']:.3f}ç§’")
                self.logger.info(f"      å¹³å‡è€—æ—¶: {stat['avg_time']:.3f}ç§’") 
                self.logger.info(f"      æ‰§è¡Œæ¬¡æ•°: {stat['count']}")
                
                # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆç›¸å¯¹äºæ€»æ—¶é—´ï¼‰
                if 'total_batch_analysis' in stats:
                    total_time = stats['total_batch_analysis']['total_time']
                    percentage = (stat['total_time'] / total_time) * 100
                    self.logger.info(f"      å æ¯”: {percentage:.1f}%")
        
        # æ˜¾ç¤ºå…¶ä»–æœªæ˜ å°„çš„è®¡æ—¶å™¨
        unmapped_timers = set(stats.keys()) - set(step_mapping.keys()) - {'gpu_memory'}
        if unmapped_timers:
            self.logger.info("")
            self.logger.info("ğŸ”§ å…¶ä»–è®¡æ—¶å™¨:")
            for timer_name in sorted(unmapped_timers):
                stat = stats[timer_name]
                self.logger.info(f"  {timer_name}: æ€»è€—æ—¶={stat['total_time']:.3f}ç§’, "
                               f"å¹³å‡={stat['avg_time']:.3f}ç§’, æ¬¡æ•°={stat['count']}")
        
        # GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            self.logger.info("")
            self.logger.info("ğŸ’¾ GPUæ˜¾å­˜ç»Ÿè®¡:")
            self.logger.info(f"  å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"  å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²åˆ†é…: {self.gpu_memory_stats['current_allocated']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²ä¿ç•™: {self.gpu_memory_stats['current_reserved']:.2f}GB")
        
        self.logger.info("=" * 80)
    
    def save_batch_results_to_csv(self, result):
        """ä¿å­˜æ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶ - é€æ—¥è¯¦ç»†è®°å½•"""
        try:
            batch_results = result['batch_results']
            evaluation_dates = result['evaluation_dates']
            
            # è¯»å–ç°æœ‰CSVæ–‡ä»¶
            if os.path.exists(self.csv_results_file):
                df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
            else:
                df = pd.DataFrame()
            
            # ä¸ºæ¯ä¸ªè¯„æµ‹æ—¥æœŸåˆ›å»ºä¸€è¡Œè®°å½•
            new_rows = []
            for i, daily_result in enumerate(batch_results['detailed_results']):
                evaluation_date = evaluation_dates[i]
                prediction_stats = daily_result.get('prediction_stats', {})
                
                # è®¡ç®—å¯¹æ¯”è‚¡ç¥¨æ•°é‡
                # åœ¨self_onlyæ¨¡å¼ä¸‹ï¼Œåªå¯¹æ¯”è‡ªèº«å†å²æ•°æ®ï¼Œä¸éœ€è¦é¢å¤–åŠ 1
                # åœ¨å…¶ä»–æ¨¡å¼ä¸‹ï¼Œéœ€è¦åŠ ä¸Šç›®æ ‡è‚¡ç¥¨è‡ªèº«
                if self.comparison_mode == 'self_only':
                    comparison_stock_count = len(self.comparison_stocks)
                else:
                    comparison_stock_count = len(self.comparison_stocks) + 1
                
                # å‡†å¤‡å•æ—¥ç»“æœæ•°æ®
                row_data = {
                    'ä»£ç ': str(result['stock_code']),
                    'window_size': result['window_size'],
                    'é˜ˆå€¼': result['threshold'],
                    'è¯„æµ‹æ—¥æœŸ': evaluation_date.strftime('%Y-%m-%d'),
                    'å¯¹æ¯”è‚¡ç¥¨æ•°é‡': comparison_stock_count,
                    'ç›¸å…³æ•°é‡': daily_result.get('daily_high_count', 0),
                    'ä¸‹1æ—¥é«˜å¼€': f"{prediction_stats.get('ratios', {}).get('next_day_gap_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹1æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_1_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹3æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_3_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹5æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_5_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹10æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_10_day_up', 0):.2%}" if prediction_stats else 'N/A'
                }
                new_rows.append(row_data)
            
            # æ·»åŠ æ‰€æœ‰æ–°è¡Œ
            if new_rows:
                new_df = pd.DataFrame(new_rows)
                df = pd.concat([df, new_df], ignore_index=True)
                
                # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df['ä»£ç '] = df['ä»£ç '].astype(str)
                
                # æŒ‰è¯„æµ‹æ—¥æœŸé™åºæ’åˆ—ï¼ˆæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
                df['è¯„æµ‹æ—¥æœŸ_æ’åº'] = pd.to_datetime(df['è¯„æµ‹æ—¥æœŸ'])
                df = df.sort_values('è¯„æµ‹æ—¥æœŸ_æ’åº', ascending=False)
                df = df.drop('è¯„æµ‹æ—¥æœŸ_æ’åº', axis=1)  # åˆ é™¤ä¸´æ—¶æ’åºåˆ—
                df = df.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
                
                # ä¿å­˜CSVæ–‡ä»¶
                df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
                
                self.logger.info(f"âœ… æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
                self.logger.info(f"âœ… å…±ä¿å­˜ {len(new_rows)} æ¡é€æ—¥è¯„æµ‹è®°å½•")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœéœ€è¦ä¿å­˜")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")


def analyze_pearson_correlation_gpu_batch(stock_code, backtest_date=None, evaluation_days=1, 
                                         window_size=15, threshold=0.85, comparison_mode='default', 
                                         comparison_stocks=None, debug=False, csv_filename=None, 
                                         use_gpu=True, batch_size=1000, earliest_date='2020-01-01',
                                         num_processes=None):
    """
    GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æçš„ä¾¿æ·å‡½æ•°
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
        evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
        window_size: çª—å£å¤§å°
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
        comparison_mode: å¯¹æ¯”æ¨¡å¼
        comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        debug: è°ƒè¯•æ¨¡å¼
        csv_filename: CSVæ–‡ä»¶å
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        batch_size: æ‰¹å¤„ç†å¤§å°
        earliest_date: æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (æ ¼å¼: YYYY-MM-DDï¼Œé»˜è®¤: 2020-01-01)
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    if backtest_date is None:
        backtest_date = datetime.now().strftime('%Y-%m-%d')
    
    if csv_filename is None:
        csv_filename = 'batch_evaluation_results.csv'
    
    analyzer = GPUBatchPearsonAnalyzer(
        stock_code=stock_code,
        window_size=window_size,
        threshold=threshold,
        evaluation_days=evaluation_days,
        debug=debug,
        comparison_stocks=comparison_stocks,
        comparison_mode=comparison_mode,
        backtest_date=backtest_date,
        csv_filename=csv_filename,
        use_gpu=use_gpu,
        batch_size=batch_size,
        earliest_date=earliest_date,
        num_processes=num_processes
    )
    
    result = analyzer.analyze_batch()
    
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ')
    parser.add_argument('stock_code', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--backtest_date', type=str, help='å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--evaluation_days', type=int, default=1, help='è¯„æµ‹æ—¥æœŸæ•°é‡ (é»˜è®¤: 1)')
    parser.add_argument('--window_size', type=int, default=15, help='åˆ†æçª—å£å¤§å° (é»˜è®¤: 15)')
    parser.add_argument('--threshold', type=float, default=0.85, help='ç›¸å…³ç³»æ•°é˜ˆå€¼ (é»˜è®¤: 0.85)')
    parser.add_argument('--comparison_mode', type=str, default='top10', 
                       choices=['top10', 'industry', 'custom', 'self_only', 'all'],
                       help='å¯¹æ¯”æ¨¡å¼: top10(å¸‚å€¼å‰10), industry(è¡Œä¸šè‚¡ç¥¨), custom(è‡ªå®šä¹‰), self_only(ä»…è‡ªèº«å†å²), all(å…¨éƒ¨Aè‚¡) (é»˜è®¤: top10)')
    parser.add_argument('--comparison_stocks', nargs='*', 
                       help='è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš” (ä»…åœ¨comparison_mode=customæ—¶æœ‰æ•ˆ)')
    parser.add_argument('--debug', action='store_true', help='å¼€å¯è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--csv_filename', type=str, default='evaluation_results.csv', help='CSVç»“æœæ–‡ä»¶å (é»˜è®¤: evaluation_results.csv)')
    parser.add_argument('--no_gpu', action='store_true', help='ç¦ç”¨GPUåŠ é€Ÿ (é»˜è®¤å¯ç”¨GPU)')
    parser.add_argument('--batch_size', type=int, default=1000, 
                       help='GPUæ‰¹å¤„ç†å¤§å° - æ§åˆ¶å•æ¬¡GPUè®¡ç®—çš„æ•°æ®é‡ï¼Œå½±å“å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚'
                            'æ¨èå€¼ï¼šRTX 3060(8GB)=500-1000, RTX 3080(10GB)=1000-2000, RTX 4090(24GB)=2000-5000 (é»˜è®¤: 1000)')
    parser.add_argument('--earliest_date', type=str, default='2022-01-01', 
                       help='æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (YYYY-MM-DD)ï¼Œæ—©äºæ­¤æ—¥æœŸçš„æ•°æ®å°†è¢«è¿‡æ»¤æ‰ (é»˜è®¤: 2022-01-01)')
    parser.add_argument('--num_processes', type=int, default=None,
                       help='å¤šè¿›ç¨‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼‰')

    args = parser.parse_args()
    
    print(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹åˆ†æï¼Œè‚¡ç¥¨ä»£ç : {args.stock_code}")
    print(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {args.evaluation_days}")
    print(f"çª—å£å¤§å°: {args.window_size}")
    print(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {args.threshold}")
    
    result = analyze_pearson_correlation_gpu_batch(
        stock_code=args.stock_code,
        backtest_date=args.backtest_date,
        evaluation_days=args.evaluation_days,
        window_size=args.window_size,
        threshold=args.threshold,
        comparison_mode=args.comparison_mode,
        comparison_stocks=args.comparison_stocks,
        debug=args.debug,
        csv_filename=args.csv_filename,
        use_gpu=not args.no_gpu,
        batch_size=args.batch_size,
        earliest_date=args.earliest_date,
        num_processes=args.num_processes
    )
    
    if result:
        print(f"åˆ†æå®Œæˆï¼Œè¯„æµ‹äº† {result['evaluation_days']} ä¸ªæ—¥æœŸ")
        print(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {result['batch_results']['summary']['total_high_correlations']}")
        print(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {result['batch_results']['summary']['avg_high_correlations_per_day']:.2f}")
    else:
        print("åˆ†æå¤±è´¥")