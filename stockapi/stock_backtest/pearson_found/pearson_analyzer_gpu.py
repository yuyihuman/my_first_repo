"""
è‚¡ç¥¨æ•°æ®Pearsonç›¸å…³ç³»æ•°åˆ†æè„šæœ¬ - GPUæ‰¹é‡è¯„æµ‹ç‰ˆæœ¬

è¯¥è„šæœ¬æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œé€šè¿‡ä¸‰ç»´çŸ©é˜µè¿ç®—å¤§å¹…æå‡GPUåˆ©ç”¨ç‡ã€‚
ç›¸æ¯”å•æ—¥è¯„æµ‹ç‰ˆæœ¬ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œå®ç°æ›´é«˜çš„å¹¶è¡Œè®¡ç®—æ•ˆç‡ã€‚

åŠŸèƒ½ï¼š
1. æ”¯æŒæ‰¹é‡è¯„æµ‹æ—¥æœŸå‚æ•°ï¼ˆevaluation_daysï¼‰
2. ä¸‰ç»´GPUçŸ©é˜µè¿ç®—ï¼š[è¯„æµ‹æ—¥æœŸæ•°, çª—å£å¤§å°, å­—æ®µæ•°]
3. æ‰¹é‡è®¡ç®—æ‰€æœ‰è¯„æµ‹æ—¥æœŸçš„Pearsonç›¸å…³ç³»æ•°
4. æ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œé¿å…GPUå†…å­˜æº¢å‡º
5. æ‰¹é‡ç»“æœç»Ÿè®¡å’ŒCSVå¯¼å‡º
6. GPUæ˜¾å­˜ç›‘æ§å’Œè‡ªé€‚åº”åˆ†ç»„å¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
python pearson_analyzer_gpu.py 000001 --evaluation_days 100

ä½œè€…ï¼šStock Backtest System
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
GPUæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼š2024å¹´
"""

import argparse
import logging
import os
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from scipy.stats import pearsonr
from data_loader import StockDataLoader
import matplotlib.pyplot as plt
import mplfinance as mpf
from stock_config import get_comparison_stocks
import time
import threading
from collections import defaultdict
import warnings
import gc
import multiprocessing as mp
from functools import partial

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)


def _process_stock_historical_data_worker(args):
    """
    å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šå¤„ç†å•åªè‚¡ç¥¨çš„å†å²æ•°æ®
    
    Args:
        args: (stock_code, stock_data, window_size, fields, debug)
    
    Returns:
        tuple: (stock_code, historical_data_list, stats)
    """
    stock_code, stock_data, window_size, fields, debug = args
    
    historical_data = []
    stock_valid_periods = 0
    stock_invalid_periods = 0
    
    try:
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
        available_data = stock_data
        
        if len(available_data) < window_size:
            return stock_code, [], {'valid_periods': 0, 'invalid_periods': 0, 'skipped': True}
        
        # ç”Ÿæˆè¯¥è‚¡ç¥¨çš„å†å²æœŸé—´å¹¶ç›´æ¥è¿›è¡Œç­›é€‰å’Œé¢„å¤„ç†
        for i in range(len(available_data) - window_size + 1):
            period_data = available_data.iloc[i:i + window_size]
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
            if len(period_data) == window_size:
                start_date = period_data.index[0]
                end_date = period_data.index[-1]
                
                # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                historical_values = period_data[fields].values
                
                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                historical_data.append((historical_values, start_date, end_date, stock_code))
                stock_valid_periods += 1
            else:
                stock_invalid_periods += 1
        
        return stock_code, historical_data, {
            'valid_periods': stock_valid_periods, 
            'invalid_periods': stock_invalid_periods, 
            'skipped': False
        }
        
    except Exception as e:
        if debug:
            print(f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
        return stock_code, [], {'valid_periods': 0, 'invalid_periods': 0, 'error': str(e)}


class GPUBatchPearsonAnalyzer:
    def __init__(self, stock_code, log_dir='logs', window_size=15, threshold=0.85, 
                 evaluation_days=1, debug=False, comparison_stocks=None, 
                 comparison_mode='top10', backtest_date=None, 
                 csv_filename='evaluation_results.csv', use_gpu=True, 
                 batch_size=1000, gpu_memory_limit=0.8, earliest_date='2020-01-01',
                 num_processes=None, evaluation_batch_size=20):
        """
        åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç ï¼Œæ”¯æŒå•ä¸ªè‚¡ç¥¨æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªè‚¡ç¥¨
            log_dir: æ—¥å¿—ç›®å½•
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥æ•°é‡ï¼‰
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡ï¼ˆä»backtest_dateå¾€å‰æ•°çš„äº¤æ˜“æ—¥æ•°ï¼‰
            debug: æ˜¯å¦å¼€å¯debugæ¨¡å¼
            comparison_stocks: è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            backtest_date: å›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
            csv_filename: CSVç»“æœæ–‡ä»¶å
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            batch_size: GPUæ‰¹å¤„ç†å¤§å°
            gpu_memory_limit: GPUå†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆ0.0-1.0ï¼‰
            earliest_date: æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (æ ¼å¼: YYYY-MM-DDï¼Œé»˜è®¤: 2020-01-01)
            num_processes: å¤šè¿›ç¨‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼‰
            evaluation_batch_size: æ¯æ‰¹æ¬¡å¤„ç†çš„è®¡ç®—å•å…ƒæ•°é‡ï¼Œç”¨äºæ§åˆ¶GPUå†…å­˜ä½¿ç”¨
                              å•è‚¡ç¥¨æ¨¡å¼: ç›´æ¥è¡¨ç¤ºè¯„æµ‹æ—¥æœŸæ•°é‡
                              å¤šè‚¡ç¥¨æ¨¡å¼: è¡¨ç¤ºæ€»è®¡ç®—å•å…ƒæ•° (è‚¡ç¥¨æ•° Ã— è¯„æµ‹æ—¥æœŸæ•°)
                              ä¾‹å¦‚: 100è‚¡ç¥¨Ã—15è¯„æµ‹æ—¥æœŸ=1500è®¡ç®—å•å…ƒï¼Œbatch_size=20æ—¶åˆ†75æ‰¹å¤„ç† (é»˜è®¤: 20)
        """
        # æ”¯æŒå¤šä¸ªè‚¡ç¥¨ä»£ç 
        if isinstance(stock_code, str):
            if ',' in stock_code:
                self.stock_codes = [code.strip() for code in stock_code.split(',')]
            else:
                self.stock_codes = [stock_code]
        elif isinstance(stock_code, list):
            self.stock_codes = stock_code
        else:
            self.stock_codes = [str(stock_code)]
        
        self.stock_code = self.stock_codes[0]  # ä¿æŒå‘åå…¼å®¹æ€§ï¼Œä¸»è¦è‚¡ç¥¨ä»£ç 
        self.is_multi_stock = len(self.stock_codes) > 1
        
        # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
        script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
        self.log_dir = os.path.join(script_dir, 'logs')
        self.csv_results_file = os.path.join(script_dir, csv_filename)
        
        self.window_size = window_size
        self.threshold = threshold
        self.evaluation_days = evaluation_days  # æ–°å¢ï¼šè¯„æµ‹æ—¥æœŸæ•°é‡
        self.evaluation_batch_size = evaluation_batch_size  # æ¯æ‰¹æ¬¡å¤„ç†çš„è¯„æµ‹æ—¥æœŸæ•°é‡
        self.debug = debug
        self.comparison_mode = comparison_mode
        self.backtest_date = pd.to_datetime(backtest_date) if backtest_date else None
        self.earliest_date = pd.to_datetime(earliest_date)
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.gpu_memory_limit = gpu_memory_limit
        self.data_loader = None
        self.logger = None
        
        # å¤šè¿›ç¨‹è®¾ç½®
        self.num_processes = num_processes if num_processes is not None else max(1, mp.cpu_count() - 1)
        
        # è®¾ç½®CSVä¿å­˜åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        self.save_results = True
        
        # GPUè®¾å¤‡è®¾ç½®
        self.device = self._setup_device()
        
        # GPUæ˜¾å­˜ç›‘æ§
        self.gpu_memory_stats = {
            'peak_allocated': 0,
            'peak_reserved': 0,
            'current_allocated': 0,
            'current_reserved': 0
        }
        
        # è®¾ç½®å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        if comparison_stocks:
            self.comparison_stocks = comparison_stocks
        elif comparison_mode == 'self_only':
            self.comparison_stocks = [stock_code]
        else:
            self.comparison_stocks = get_comparison_stocks(comparison_mode)
            # ç¡®ä¿ç›®æ ‡è‚¡ç¥¨ä¸åœ¨å¯¹æ¯”åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
            # if stock_code in self.comparison_stocks:
            #     self.comparison_stocks.remove(stock_code)
        
        # å­˜å‚¨å·²åŠ è½½çš„è‚¡ç¥¨æ•°æ®
        self.loaded_stocks_data = {}
        
        # æ€§èƒ½è®¡æ—¶å™¨
        self.performance_timers = defaultdict(list)
        self.current_timers = {}
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # è®¾ç½®CSVæ–‡ä»¶
        self._setup_csv_file()
        
        if self.is_multi_stock:
            self.logger.info(f"åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {self.stock_codes} (å¤šè‚¡ç¥¨æ¨¡å¼)")
        else:
            self.logger.info(f"åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"çª—å£å¤§å°: {window_size}, é˜ˆå€¼: {threshold}, è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        self.logger.info(f"GPUå†…å­˜é™åˆ¶: {gpu_memory_limit*100:.0f}%")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {comparison_mode}, å¯¹æ¯”è‚¡ç¥¨æ•°é‡: {len(self.comparison_stocks)}")
    
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰"""
        if self.use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if self.debug:
                print(f"ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name} ({gpu_memory:.1f}GB)")
            return device
        else:
            if self.use_gpu:
                print("è­¦å‘Šï¼šCUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")
            else:
                print("ä½¿ç”¨CPUè®¡ç®—")
            return torch.device('cpu')
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # ç›´æ¥ä½¿ç”¨logsæ ¹ç›®å½•ï¼Œä¸åˆ›å»ºå­æ–‡ä»¶å¤¹
        os.makedirs(self.log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        thread_id = threading.get_ident()
        
        # æ ¹æ®æ˜¯å¦ä¸ºå¤šè‚¡ç¥¨æ¨¡å¼å†³å®šæ—¥å¿—æ–‡ä»¶å
        if self.is_multi_stock:
            log_filename = f"batch_pearson_analysis_list_{timestamp}_thread_{thread_id}.log"
            logger_name = 'GPUBatchPearsonAnalyzer_list'
        else:
            log_filename = f"batch_pearson_analysis_{self.stock_code}_{timestamp}_thread_{thread_id}.log"
            logger_name = f'GPUBatchPearsonAnalyzer_{self.stock_code}'
        
        log_path = os.path.join(self.log_dir, log_filename)
        
        self.logger = logging.getLogger(logger_name)
        self.logger.setLevel(logging.INFO)
        
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        file_handler = logging.FileHandler(log_path, encoding='utf-8-sig')
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.info(f"æ‰¹é‡è¯„æµ‹æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_path}")
    
    def _setup_csv_file(self):
        """è®¾ç½®CSVæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        self.logger.info(f"ğŸ“‹ å¼€å§‹è®¾ç½®CSVæ–‡ä»¶: {self.csv_results_file}")
        
        if not os.path.exists(self.csv_results_file):
            self.logger.info("ğŸ“‹ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»ºæ–°æ–‡ä»¶...")
            
            # ä½¿ç”¨ä¸å•æ—¥è„šæœ¬ç›¸åŒçš„è¡¨å¤´æ ¼å¼
            header = ['ä»£ç ', 'window_size', 'é˜ˆå€¼', 'è¯„æµ‹æ—¥æœŸ', 'å¯¹æ¯”è‚¡ç¥¨æ•°é‡', 'ç›¸å…³æ•°é‡', 
                     'ä¸‹1æ—¥é«˜å¼€', 'ä¸‹1æ—¥ä¸Šæ¶¨', 'ä¸‹3æ—¥ä¸Šæ¶¨', 'ä¸‹5æ—¥ä¸Šæ¶¨', 'ä¸‹10æ—¥ä¸Šæ¶¨']
            
            self.logger.info(f"ğŸ“‹ CSVè¡¨å¤´å­—æ®µ: {header}")
            self.logger.info(f"ğŸ“‹ CSVè¡¨å¤´å­—æ®µæ•°é‡: {len(header)}")
            
            df = pd.DataFrame(columns=header)
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            
            try:
                df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
                
                # éªŒè¯æ–‡ä»¶åˆ›å»ºæˆåŠŸ
                if os.path.exists(self.csv_results_file):
                    file_size = os.path.getsize(self.csv_results_file)
                    self.logger.info(f"âœ… CSVæ–‡ä»¶åˆ›å»ºæˆåŠŸ: {self.csv_results_file}")
                    self.logger.info(f"âœ… åˆå§‹æ–‡ä»¶å¤§å°: {file_size} bytes")
                    self.logger.info(f"âœ… ç¼–ç æ ¼å¼: utf-8-sig")
                else:
                    self.logger.error("âŒ CSVæ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼šæ–‡ä»¶ä¸å­˜åœ¨")
                    
            except Exception as e:
                self.logger.error(f"âŒ CSVæ–‡ä»¶åˆ›å»ºæ—¶å‡ºé”™: {str(e)}")
                raise
        else:
            # æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶çŠ¶æ€
            try:
                file_size = os.path.getsize(self.csv_results_file)
                existing_df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
                row_count = len(existing_df)
                
                self.logger.info(f"ğŸ“‹ CSVæ–‡ä»¶å·²å­˜åœ¨: {self.csv_results_file}")
                self.logger.info(f"ğŸ“‹ ç°æœ‰æ–‡ä»¶å¤§å°: {file_size} bytes")
                self.logger.info(f"ğŸ“‹ ç°æœ‰è®°å½•æ•°é‡: {row_count} è¡Œ")
                
                if row_count > 0:
                    self.logger.info(f"ğŸ“‹ ç°æœ‰æ•°æ®åˆ—å: {list(existing_df.columns)}")
                    # æ˜¾ç¤ºæœ€è¿‘çš„å‡ æ¡è®°å½•ä½œä¸ºå‚è€ƒ
                    if row_count <= 3:
                        self.logger.info(f"ğŸ“‹ ç°æœ‰æ•°æ®é¢„è§ˆ: \n{existing_df.to_string()}")
                    else:
                        self.logger.info(f"ğŸ“‹ æœ€æ–°3æ¡è®°å½•é¢„è§ˆ: \n{existing_df.head(3).to_string()}")
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¯»å–ç°æœ‰CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                
        self.logger.info("ğŸ“‹ CSVæ–‡ä»¶è®¾ç½®å®Œæˆ")
    
    def start_timer(self, timer_name, parent_timer=None):
        """
        å¼€å§‹è®¡æ—¶
        
        Args:
            timer_name: è®¡æ—¶å™¨åç§°
            parent_timer: çˆ¶è®¡æ—¶å™¨åç§°ï¼ˆç”¨äºåˆ†å±‚æ˜¾ç¤ºï¼‰
        """
        self.current_timers[timer_name] = {
            'start_time': time.time(),
            'parent': parent_timer
        }
        if self.debug:
            self.logger.info(f"â±ï¸ å¼€å§‹è®¡æ—¶: {timer_name}")
    
    def end_timer(self, timer_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if timer_name in self.current_timers:
            timer_info = self.current_timers[timer_name]
            elapsed_time = time.time() - timer_info['start_time']
            
            # å­˜å‚¨è®¡æ—¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬çˆ¶è®¡æ—¶å™¨ä¿¡æ¯
            if timer_name not in self.performance_timers:
                self.performance_timers[timer_name] = []
            
            self.performance_timers[timer_name].append({
                'elapsed_time': elapsed_time,
                'parent': timer_info['parent'],
                'timestamp': time.time()
            })
            
            del self.current_timers[timer_name]
            if self.debug:
                self.logger.info(f"â±ï¸ ç»“æŸè®¡æ—¶: {timer_name} - è€—æ—¶: {elapsed_time:.3f}ç§’")
            return elapsed_time
        return 0
    
    def load_data(self):
        """ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ç›®æ ‡è‚¡ç¥¨å’Œå¯¹æ¯”è‚¡ç¥¨æ•°æ®ï¼Œå®ç°çœŸæ­£çš„æ‰¹é‡å¤„ç†"""
        self.start_timer('all_data_loading')
        
        if self.is_multi_stock:
            self.logger.info(f"ğŸ“Š æ‰¹é‡æ•°æ®åŠ è½½ä¸­: {len(self.stock_codes)} ä¸ªç›®æ ‡è‚¡ç¥¨ + {len(self.comparison_stocks)} ä¸ªå¯¹æ¯”è‚¡ç¥¨")
        else:
            self.logger.info(f"ğŸ“Š æ•°æ®åŠ è½½ä¸­: 1 ä¸ªç›®æ ‡è‚¡ç¥¨ + {len(self.comparison_stocks)} ä¸ªå¯¹æ¯”è‚¡ç¥¨")
        
        self.data_loader = StockDataLoader()
        
        # å­˜å‚¨æ‰€æœ‰ç›®æ ‡è‚¡ç¥¨çš„æ•°æ®
        self.multi_stock_data = {}
        # å­˜å‚¨æ‰€æœ‰å¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        self.loaded_stocks_data = {}
        
        # 1. é¦–å…ˆåŠ è½½æ‰€æœ‰å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        self.logger.info(f"ğŸ“ˆ [1/2] åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®...")
        successful_comparison_loads = 0
        for stock_code in self.comparison_stocks:
            try:
                data = self.data_loader.load_stock_data(stock_code)
                if data is not None and not data.empty:
                    filtered_data = self._filter_data(data, stock_code, is_target_stock=False)
                    if not filtered_data.empty:
                        self.loaded_stocks_data[stock_code] = filtered_data
                        successful_comparison_loads += 1
                    else:
                        if self.debug:
                            self.logger.warning(f"å¯¹æ¯”è‚¡ç¥¨ {stock_code} è¿‡æ»¤åæ•°æ®ä¸ºç©º")
                else:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•åŠ è½½å¯¹æ¯”è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"åŠ è½½å¯¹æ¯”è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # 2. ç„¶ååŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®ï¼ˆæ£€æŸ¥æ˜¯å¦å·²åœ¨å¯¹æ¯”è‚¡ç¥¨ä¸­ï¼‰
        self.logger.info(f"ğŸ“ˆ [2/2] åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®...")
        successful_target_loads = 0
        for stock_code in self.stock_codes:
            try:
                # æ£€æŸ¥ç›®æ ‡è‚¡ç¥¨æ˜¯å¦å·²ç»åœ¨å¯¹æ¯”è‚¡ç¥¨æ•°æ®ä¸­
                if stock_code in self.loaded_stocks_data:
                    # å¦‚æœå·²ç»åœ¨å¯¹æ¯”è‚¡ç¥¨ä¸­ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä½†éœ€è¦é‡æ–°è¿‡æ»¤ä¸ºç›®æ ‡è‚¡ç¥¨æ ¼å¼
                    original_data = self.data_loader.load_stock_data(stock_code)
                    if original_data is not None and not original_data.empty:
                        filtered_data = self._filter_data(original_data, stock_code, is_target_stock=True)
                        self.multi_stock_data[stock_code] = filtered_data
                        successful_target_loads += 1
                        self.logger.info(f"âœ… ç›®æ ‡è‚¡ç¥¨ {stock_code} æ•°æ®å·²å­˜åœ¨äºå¯¹æ¯”è‚¡ç¥¨ä¸­ï¼Œé‡æ–°è¿‡æ»¤å®Œæˆ ({len(filtered_data)} æ¡è®°å½•)")
                    else:
                        self.logger.error(f"æ— æ³•é‡æ–°åŠ è½½ç›®æ ‡è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        continue
                else:
                    # å¦‚æœä¸åœ¨å¯¹æ¯”è‚¡ç¥¨ä¸­ï¼Œå•ç‹¬åŠ è½½
                    data = self.data_loader.load_stock_data(stock_code)
                    
                    if data is None or data.empty:
                        self.logger.error(f"æ— æ³•åŠ è½½ç›®æ ‡è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        continue
                    
                    filtered_data = self._filter_data(data, stock_code, is_target_stock=True)
                    self.multi_stock_data[stock_code] = filtered_data
                    successful_target_loads += 1
                    self.logger.info(f"âœ… ç›®æ ‡è‚¡ç¥¨ {stock_code} æ•°æ®å•ç‹¬åŠ è½½å®Œæˆ ({len(filtered_data)} æ¡è®°å½•)")
            except Exception as e:
                self.logger.error(f"åŠ è½½ç›®æ ‡è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        if not self.multi_stock_data:
            self.logger.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç›®æ ‡è‚¡ç¥¨æ•°æ®")
            self.end_timer('all_data_loading')
            return None
        
        # ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œå°†ç¬¬ä¸€ä¸ªè‚¡ç¥¨çš„æ•°æ®è®¾ä¸ºä¸»æ•°æ®
        self.data = self.multi_stock_data[self.stock_codes[0]]
        
        self.logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {successful_target_loads}/{len(self.stock_codes)} ä¸ªç›®æ ‡è‚¡ç¥¨, {successful_comparison_loads}/{len(self.comparison_stocks)} ä¸ªå¯¹æ¯”è‚¡ç¥¨")
        self.end_timer('all_data_loading')
        return self.data
    
    def _filter_data(self, data, stock_code, is_target_stock=False):
        """è¿‡æ»¤è‚¡ç¥¨æ•°æ®ï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œæ—¥æœŸèŒƒå›´
        
        Args:
            data: è‚¡ç¥¨æ•°æ®DataFrame
            stock_code: è‚¡ç¥¨ä»£ç 
            is_target_stock: æ˜¯å¦ä¸ºç›®æ ‡è‚¡ç¥¨ï¼Œç›®æ ‡è‚¡ç¥¨ä¸å—earliest_dateé™åˆ¶
        """
        if data is None or data.empty:
            return data
            
        original_count = len(data)
        date_filtered_count = original_count
        date_removed_count = 0
        
        # åªå¯¹å¯¹æ¯”è‚¡ç¥¨åº”ç”¨æ—¥æœŸè¿‡æ»¤ï¼Œç›®æ ‡è‚¡ç¥¨ä½¿ç”¨å®Œæ•´å†å²æ•°æ®
        if not is_target_stock:
            data = data[data.index >= self.earliest_date]
            date_filtered_count = len(data)
            date_removed_count = original_count - date_filtered_count
        
        # æ•°æ®è´¨é‡è¿‡æ»¤ï¼ˆå¯¹æ‰€æœ‰è‚¡ç¥¨éƒ½åº”ç”¨ï¼‰
        data = data[
            (data['open'] > 1) & 
            (data['high'] > 1) & 
            (data['low'] > 1) & 
            (data['close'] > 1) & 
            (data['volume'] > 1)
        ]
        final_count = len(data)
        quality_removed_count = date_filtered_count - final_count
        
        if date_removed_count > 0:
            self.logger.debug(f"å¯¹æ¯”è‚¡ç¥¨ {stock_code} æ—¥æœŸè¿‡æ»¤å®Œæˆï¼Œç§»é™¤æ—©äº {self.earliest_date.strftime('%Y-%m-%d')} çš„ {date_removed_count} æ¡æ•°æ®")
        elif is_target_stock:
            self.logger.debug(f"ç›®æ ‡è‚¡ç¥¨ {stock_code} ä½¿ç”¨å®Œæ•´å†å²æ•°æ®ï¼Œæ— æ—¥æœŸè¿‡æ»¤")
        
        if quality_removed_count > 0:
            self.logger.debug(f"è‚¡ç¥¨ {stock_code} æ•°æ®è´¨é‡è¿‡æ»¤å®Œæˆï¼Œç§»é™¤ {quality_removed_count} æ¡å¼‚å¸¸æ•°æ®")
        
        if not data.empty:
            self.logger.debug(f"è‚¡ç¥¨ {stock_code} æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        
        return data
    

    
    def prepare_evaluation_dates(self, end_date):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        
        Args:
            end_date: ç»“æŸæ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            
        Returns:
            list: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        """
        self.start_timer('evaluation_dates_preparation')
        
        # å¦‚æœend_dateä¸ºNoneï¼Œä½¿ç”¨æ•°æ®çš„æœ€æ–°æ—¥æœŸ
        if end_date is None:
            end_date = self.data.index.max()
            self.logger.info(f"æœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æ•°æ®æœ€æ–°æ—¥æœŸ: {end_date}")
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„äº¤æ˜“æ—¥æœŸï¼ˆåŒ…å«end_dateå½“å¤©ï¼Œå¦‚æœæ•°æ®å¯ç”¨ï¼‰
        available_dates = self.data[self.data.index <= end_date].index
        
        if len(available_dates) < self.evaluation_days + self.window_size:
            self.logger.warning(f"å¯ç”¨æ•°æ®ä¸è¶³ï¼Œéœ€è¦ {self.evaluation_days + self.window_size} ä¸ªäº¤æ˜“æ—¥ï¼Œ"
                              f"å®é™…åªæœ‰ {len(available_dates)} ä¸ª")
            # è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡
            self.evaluation_days = max(1, len(available_dates) - self.window_size)
            self.logger.info(f"è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡ä¸º: {self.evaluation_days}")
        
        # é€‰æ‹©æœ€è¿‘çš„evaluation_daysä¸ªäº¤æ˜“æ—¥ä½œä¸ºè¯„æµ‹æ—¥æœŸ
        evaluation_dates = available_dates[-self.evaluation_days:].tolist()
        
        self.logger.info(f"å‡†å¤‡äº† {len(evaluation_dates)} ä¸ªè¯„æµ‹æ—¥æœŸ")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸèŒƒå›´: {evaluation_dates[0]} åˆ° {evaluation_dates[-1]}")
        
        self.end_timer('evaluation_dates_preparation')
        return evaluation_dates
    
    def prepare_batch_evaluation_data(self, evaluation_dates):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®çŸ©é˜µï¼Œæ”¯æŒå¤šè‚¡ç¥¨
        
        Args:
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            torch.Tensor: å½¢çŠ¶ä¸º [num_stocks, evaluation_days, window_size, 5] çš„è¯„æµ‹æ•°æ®å¼ é‡ï¼ˆå¤šè‚¡ç¥¨ï¼‰
                         æˆ– [evaluation_days, window_size, 5] çš„è¯„æµ‹æ•°æ®å¼ é‡ï¼ˆå•è‚¡ç¥¨ï¼‰
            list: æœ‰æ•ˆè¯„æµ‹æ—¥æœŸåˆ—è¡¨
            list: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¤šè‚¡ç¥¨æ¨¡å¼ï¼‰
        """
        self.start_timer('batch_data_preparation')
        
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šä¸ºæ¯ä¸ªè‚¡ç¥¨æ„å»ºæ•°æ®
            multi_stock_batch_data = []
            valid_stock_codes = []
            common_valid_dates = None
            
            for stock_code in self.stock_codes:
                if stock_code not in self.multi_stock_data:
                    self.logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡")
                    continue
                
                stock_data = self.multi_stock_data[stock_code]
                batch_data_list = []
                valid_dates = []
                
                for eval_date in evaluation_dates:
                    # è·å–è¯¥è¯„æµ‹æ—¥æœŸçš„çª—å£æ•°æ®ï¼ˆåŒ…å«è¯„æµ‹æ—¥æœŸå½“å¤©ï¼‰
                    recent_data = stock_data[stock_data.index <= eval_date].tail(self.window_size)
                    
                    if len(recent_data) == self.window_size:
                        # æå–å­—æ®µæ•°æ®
                        data_values = recent_data[fields].values  # [window_size, 5]
                        batch_data_list.append(data_values)
                        valid_dates.append(eval_date)
                    else:
                        if self.debug:
                            self.logger.warning(f"è‚¡ç¥¨ {stock_code} è¯„æµ‹æ—¥æœŸ {eval_date} çš„æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                
                if batch_data_list:
                    # è½¬æ¢ä¸ºæ•°ç»„ [evaluation_days, window_size, 5]
                    stock_batch_data = np.stack(batch_data_list, axis=0)
                    multi_stock_batch_data.append(stock_batch_data)
                    valid_stock_codes.append(stock_code)
                    
                    # ç¡®ä¿æ‰€æœ‰è‚¡ç¥¨ä½¿ç”¨ç›¸åŒçš„æœ‰æ•ˆæ—¥æœŸ
                    if common_valid_dates is None:
                        common_valid_dates = valid_dates
                    else:
                        # å–äº¤é›†ï¼Œç¡®ä¿æ‰€æœ‰è‚¡ç¥¨éƒ½æœ‰æ•°æ®çš„æ—¥æœŸ
                        common_valid_dates = [date for date in common_valid_dates if date in valid_dates]
            
            if not multi_stock_batch_data:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å¤šè‚¡ç¥¨è¯„æµ‹æ•°æ®")
                self.end_timer('batch_data_preparation')
                return None, [], []
            
            # é‡æ–°ç­›é€‰æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰è‚¡ç¥¨ä½¿ç”¨ç›¸åŒçš„æ—¥æœŸ
            final_multi_stock_data = []
            for i, stock_code in enumerate(valid_stock_codes):
                stock_data = multi_stock_batch_data[i]
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ—¥æœŸé¡ºåºä¸€è‡´
                final_multi_stock_data.append(stock_data[:len(common_valid_dates)])
            
            # è½¬æ¢ä¸ºå¼ é‡ [num_stocks, evaluation_days, window_size, 5]
            batch_data = np.stack(final_multi_stock_data, axis=0)
            batch_tensor = torch.tensor(batch_data, dtype=torch.float32, device=self.device)
            
            self.logger.info(f"å¤šè‚¡ç¥¨æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {batch_tensor.shape}")
            self.logger.info(f"æœ‰æ•ˆè‚¡ç¥¨æ•°é‡: {len(valid_stock_codes)}")
            self.logger.info(f"æœ‰æ•ˆè¯„æµ‹æ—¥æœŸæ•°é‡: {len(common_valid_dates)}")
            
            self.end_timer('batch_data_preparation')
            return batch_tensor, common_valid_dates, valid_stock_codes
        
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
            batch_data_list = []
            valid_dates = []
            
            for eval_date in evaluation_dates:
                # è·å–è¯¥è¯„æµ‹æ—¥æœŸçš„çª—å£æ•°æ®ï¼ˆåŒ…å«è¯„æµ‹æ—¥æœŸå½“å¤©ï¼‰
                recent_data = self.data[self.data.index <= eval_date].tail(self.window_size)
                
                if len(recent_data) == self.window_size:
                    # æå–å­—æ®µæ•°æ®
                    data_values = recent_data[fields].values  # [window_size, 5]
                    batch_data_list.append(data_values)
                    valid_dates.append(eval_date)
                else:
                    if self.debug:
                        self.logger.warning(f"è¯„æµ‹æ—¥æœŸ {eval_date} çš„æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
            
            if not batch_data_list:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ•°æ®")
                self.end_timer('batch_data_preparation')
                return None, [], []
            
            # è½¬æ¢ä¸ºå¼ é‡ [evaluation_days, window_size, 5]
            batch_data = np.stack(batch_data_list, axis=0)
            batch_tensor = torch.tensor(batch_data, dtype=torch.float32, device=self.device)
            
            self.logger.info(f"æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {batch_tensor.shape}")
            self.logger.info(f"æœ‰æ•ˆè¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
            
            self.end_timer('batch_data_preparation')
            return batch_tensor, valid_dates, [self.stock_code]
    
    def calculate_batch_gpu_correlation(self, batch_recent_data, historical_periods_data, evaluation_dates=None):
        """
        æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—
        
        Args:
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® [evaluation_days, window_size, 5]
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            dict: æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        """
        
        if batch_recent_data is None or len(historical_periods_data) == 0:
            return {}
        
        # æ”¯æŒå¤šè‚¡ç¥¨å’Œå•è‚¡ç¥¨æ¨¡å¼
        if self.is_multi_stock:
            num_stocks, evaluation_days, window_size, num_fields = batch_recent_data.shape
            self.logger.info(f"å¼€å§‹å¤šè‚¡ç¥¨æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—")
            self.logger.info(f"è‚¡ç¥¨æ•°: {num_stocks}, è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}, å†å²æœŸé—´æ•°: {len(historical_periods_data)}")
        else:
            evaluation_days, window_size, num_fields = batch_recent_data.shape
            num_stocks = 1
            self.logger.info(f"å¼€å§‹å•è‚¡ç¥¨æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—")
            self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}, å†å²æœŸé—´æ•°: {len(historical_periods_data)}")
        
        num_historical_periods = len(historical_periods_data)
        
        # å­æ­¥éª¤1/5: å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼šæ•°æ®åœ¨é˜¶æ®µ3å·²é¢„å¤„ç†ï¼‰
        self.start_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/5] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å¼€å§‹")
        
        # æ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼Œç›´æ¥æå–
        historical_data_list = []
        period_info_list = []
        
        for historical_values, start_date, end_date, stock_code in historical_periods_data:
            historical_data_list.append(historical_values)
            period_info_list.append({
                'start_date': start_date,
                'end_date': end_date,
                'stock_code': stock_code
            })
        
        valid_periods = len(historical_data_list)
        self.logger.info(f"å†å²æ•°æ®å‡†å¤‡å®Œæˆ: æœ‰æ•ˆæœŸé—´={valid_periods}ï¼ˆæ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼‰")
        self.end_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/5] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å®Œæˆ")
        
        if not historical_data_list:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return {}
        
        # å­æ­¥éª¤2/5: åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡
        self.start_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/5] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å¼€å§‹")
        self.logger.info(f"å¼ é‡å½¢çŠ¶å°†ä¸º: [{len(historical_data_list)}, {window_size}, 5]")
        
        historical_tensor = torch.tensor(
            np.stack(historical_data_list, axis=0), 
            dtype=torch.float32, 
            device=self.device
        )  # [num_historical_periods, window_size, 5]
        
        self.logger.info(f"GPUå†å²æ•°æ®å¼ é‡åˆ›å»ºå®Œæˆ: {historical_tensor.shape}, è®¾å¤‡: {historical_tensor.device}")
        self.end_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/5] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å®Œæˆ")
        
        # ç›‘æ§æ•°æ®å¼ é‡åˆ›å»ºåçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("å¼ é‡åˆ›å»ºå®Œæˆ")
        
        # å­æ­¥éª¤3/5: æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®—
        self.start_timer('gpu_step3_correlation_calculation')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/5] æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®— - å¼€å§‹")
        self.logger.info(f"è¾“å…¥å¼ é‡å½¢çŠ¶: batch_recent_data={batch_recent_data.shape}, historical_tensor={historical_tensor.shape}")
        
        if self.is_multi_stock:
            self.logger.info(f"ç›®æ ‡è¾“å‡ºå½¢çŠ¶: [{num_stocks}, {evaluation_days}, {historical_tensor.shape[0]}, 5]")
        else:
            self.logger.info(f"ç›®æ ‡è¾“å‡ºå½¢çŠ¶: [{evaluation_days}, {historical_tensor.shape[0]}, 5]")
        
        batch_correlations = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        batch_size = min(self.batch_size, evaluation_days)
        total_batches = (evaluation_days + batch_size - 1) // batch_size
        
        self.logger.info(f"åˆ†æ‰¹è®¡ç®—é…ç½®: batch_size={batch_size}, total_batches={total_batches}")
        
        for batch_idx, i in enumerate(range(0, evaluation_days, batch_size)):
            end_idx = min(i + batch_size, evaluation_days)
            
            if self.is_multi_stock:
                # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, batch_size, window_size, 5]
                current_batch = batch_recent_data[:, i:end_idx]
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: è¯„æµ‹æ—¥æœŸ {i+1}-{end_idx} (å½¢çŠ¶: {current_batch.shape})")
                
                # ä¸ºæ¯ä¸ªè‚¡ç¥¨è®¡ç®—ç›¸å…³ç³»æ•°
                stock_batch_correlations = []
                for stock_idx in range(num_stocks):
                    stock_batch = current_batch[stock_idx]  # [batch_size, window_size, 5]
                    stock_corr = self._compute_correlation_matrix(stock_batch, historical_tensor)
                    stock_batch_correlations.append(stock_corr)
                
                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„ç»“æœ: [num_stocks, batch_size, num_historical_periods, 5]
                multi_stock_batch_corr = torch.stack(stock_batch_correlations, dim=0)
                batch_correlations.append(multi_stock_batch_corr)
            else:
                # å•è‚¡ç¥¨æ¨¡å¼: [batch_size, window_size, 5]
                current_batch = batch_recent_data[i:end_idx]
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: è¯„æµ‹æ—¥æœŸ {i+1}-{end_idx} (å½¢çŠ¶: {current_batch.shape})")
                
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç›¸å…³ç³»æ•°
                batch_corr = self._compute_correlation_matrix(current_batch, historical_tensor)
                batch_correlations.append(batch_corr)
            
            # ç›‘æ§æ¯ä¸ªæ‰¹æ¬¡åçš„GPUæ˜¾å­˜
            if batch_idx % max(1, total_batches // 5) == 0:  # æ¯20%è¿›åº¦ç›‘æ§ä¸€æ¬¡
                self.monitor_gpu_memory(f"æ‰¹æ¬¡{batch_idx + 1}å®Œæˆ")
        
        self.end_timer('gpu_step3_correlation_calculation')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/5] æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®— - å®Œæˆ")
        
        # å­æ­¥éª¤4/5: åˆå¹¶æ‰¹æ¬¡ç»“æœ
        self.start_timer('gpu_step4_batch_merging')
        self.logger.info(f"  ğŸ”— [å­æ­¥éª¤4/5] åˆå¹¶æ‰¹æ¬¡ç»“æœ - å¼€å§‹")
        
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼: åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
            # batch_correlationsä¸­æ¯ä¸ªå…ƒç´ å½¢çŠ¶: [num_stocks, batch_size, num_historical_periods, 5]
            # éœ€è¦åœ¨ç¬¬äºŒä¸ªç»´åº¦ï¼ˆevaluation_daysç»´åº¦ï¼‰ä¸Šåˆå¹¶
            all_correlations = torch.cat(batch_correlations, dim=1)  # [num_stocks, evaluation_days, num_historical_periods, 5]
            self.logger.info(f"å¤šè‚¡ç¥¨æ‰¹æ¬¡ç»“æœåˆå¹¶å®Œæˆ: æœ€ç»ˆå½¢çŠ¶={all_correlations.shape}")
        else:
            # å•è‚¡ç¥¨æ¨¡å¼: åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
            all_correlations = torch.cat(batch_correlations, dim=0)  # [evaluation_days, num_historical_periods, 5]
            self.logger.info(f"å•è‚¡ç¥¨æ‰¹æ¬¡ç»“æœåˆå¹¶å®Œæˆ: æœ€ç»ˆå½¢çŠ¶={all_correlations.shape}")
        
        self.end_timer('gpu_step4_batch_merging')
        self.logger.info(f"  ğŸ”— [å­æ­¥éª¤4/5] åˆå¹¶æ‰¹æ¬¡ç»“æœ - å®Œæˆ")
        
        # ç›‘æ§ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆåçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆ")
        
        self.logger.info(f"æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {all_correlations.shape}")
        
        # å­æ­¥éª¤5/5: å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        self.start_timer('gpu_step5_result_processing')
        self.logger.info(f"  ğŸ“‹ [å­æ­¥éª¤5/5] å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ - å¼€å§‹")
        self.logger.info(f"è°ƒç”¨å‡½æ•°: _process_batch_correlation_results")
        
        # ä¼ é€’è‚¡ç¥¨ä»£ç ä¿¡æ¯
        if self.is_multi_stock:
            # ä»analyze_batchæ–¹æ³•ä¼ é€’çš„stock_codeså‚æ•°è·å–
            # è¿™é‡Œéœ€è¦ä»è°ƒç”¨æ ˆä¸­è·å–stock_codesï¼Œæš‚æ—¶ä½¿ç”¨self.stock_codes
            target_stock_codes = self.stock_codes
        else:
            target_stock_codes = [self.stock_code]
        
        results = self._process_batch_correlation_results(
            all_correlations, period_info_list, evaluation_days,
            batch_recent_data, historical_data_list, evaluation_dates,
            target_stock_codes
        )
        self.end_timer('gpu_step5_result_processing')
        self.logger.info(f"  ğŸ“‹ [å­æ­¥éª¤5/5] å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ - å®Œæˆ")
        
        self.logger.info(f"æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å…¨éƒ¨å®Œæˆï¼Œè¿”å›ç»“æœåŒ…å« {len(results) if results else 0} ä¸ªå­—æ®µ")
        return results
    
    def _compute_correlation_matrix(self, recent_batch, historical_tensor):
        """
        è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        
        Args:
            recent_batch: [batch_size, window_size, 5]
            historical_tensor: [num_historical_periods, window_size, 5]
            
        Returns:
            torch.Tensor: [batch_size, num_historical_periods, 5]
        """
        batch_size, window_size, num_fields = recent_batch.shape
        num_historical_periods = historical_tensor.shape[0]
        
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] å¼€å§‹ç›¸å…³ç³»æ•°çŸ©é˜µè®¡ç®— - _compute_correlation_matrix")
            self.logger.debug(f"    è¾“å…¥å½¢çŠ¶: recent_batch={recent_batch.shape}, historical_tensor={historical_tensor.shape}")
        
        # æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­è®¡ç®—
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤1: æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­")
        recent_expanded = recent_batch.unsqueeze(1)  # [batch_size, 1, window_size, 5]
        historical_expanded = historical_tensor.unsqueeze(0)  # [1, num_historical_periods, window_size, 5]
        
        # è®¡ç®—å‡å€¼
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤2: è®¡ç®—å‡å€¼")
        recent_mean = recent_expanded.mean(dim=2, keepdim=True)  # [batch_size, 1, 1, 5]
        historical_mean = historical_expanded.mean(dim=2, keepdim=True)  # [1, num_historical_periods, 1, 5]
        
        # ä¸­å¿ƒåŒ–
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤3: æ•°æ®ä¸­å¿ƒåŒ–")
        recent_centered = recent_expanded - recent_mean
        historical_centered = historical_expanded - historical_mean
        
        # è®¡ç®—åæ–¹å·®
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤4: è®¡ç®—åæ–¹å·®")
        covariance = (recent_centered * historical_centered).sum(dim=2)  # [batch_size, num_historical_periods, 5]
        
        # è®¡ç®—æ ‡å‡†å·®
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤5: è®¡ç®—æ ‡å‡†å·®")
        recent_std = torch.sqrt((recent_centered ** 2).sum(dim=2))  # [batch_size, 1, 5]
        historical_std = torch.sqrt((historical_centered ** 2).sum(dim=2))  # [1, num_historical_periods, 5]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] æ­¥éª¤6: è®¡ç®—æœ€ç»ˆç›¸å…³ç³»æ•°")
        correlation = covariance / (recent_std * historical_std + 1e-8)
        
        if self.debug:
            self.logger.debug(f"    [GPUè®¡ç®—] ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {correlation.shape}")
        
        return correlation

    def _compute_correlation_matrix_multi_stock(self, recent_batch, historical_tensor):
        """
        è®¡ç®—å¤šè‚¡ç¥¨ç›¸å…³ç³»æ•°çŸ©é˜µ
        
        Args:
            recent_batch: [num_stocks, batch_size, window_size, 5]
            historical_tensor: [num_historical_periods, window_size, 5]
            
        Returns:
            torch.Tensor: [num_stocks, batch_size, num_historical_periods, 5]
        """
        num_stocks, batch_size, window_size, num_fields = recent_batch.shape
        num_historical_periods = historical_tensor.shape[0]
        
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] å¼€å§‹ç›¸å…³ç³»æ•°çŸ©é˜µè®¡ç®— - _compute_correlation_matrix_multi_stock")
            self.logger.debug(f"    è¾“å…¥å½¢çŠ¶: recent_batch={recent_batch.shape}, historical_tensor={historical_tensor.shape}")
        
        # æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­è®¡ç®—
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤1: æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­")
        recent_expanded = recent_batch.unsqueeze(2)  # [num_stocks, batch_size, 1, window_size, 5]
        historical_expanded = historical_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, num_historical_periods, window_size, 5]
        
        # è®¡ç®—å‡å€¼
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤2: è®¡ç®—å‡å€¼")
        recent_mean = recent_expanded.mean(dim=3, keepdim=True)  # [num_stocks, batch_size, 1, 1, 5]
        historical_mean = historical_expanded.mean(dim=3, keepdim=True)  # [1, 1, num_historical_periods, 1, 5]
        
        # ä¸­å¿ƒåŒ–
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤3: æ•°æ®ä¸­å¿ƒåŒ–")
        recent_centered = recent_expanded - recent_mean
        historical_centered = historical_expanded - historical_mean
        
        # è®¡ç®—åæ–¹å·®
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤4: è®¡ç®—åæ–¹å·®")
        covariance = (recent_centered * historical_centered).sum(dim=3)  # [num_stocks, batch_size, num_historical_periods, 5]
        
        # è®¡ç®—æ ‡å‡†å·®
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤5: è®¡ç®—æ ‡å‡†å·®")
        recent_std = torch.sqrt((recent_centered ** 2).sum(dim=3))  # [num_stocks, batch_size, 1, 5]
        historical_std = torch.sqrt((historical_centered ** 2).sum(dim=3))  # [1, 1, num_historical_periods, 5]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] æ­¥éª¤6: è®¡ç®—æœ€ç»ˆç›¸å…³ç³»æ•°")
        correlation = covariance / (recent_std * historical_std + 1e-8)
        
        if self.debug:
            self.logger.debug(f"    [GPUå¤šè‚¡ç¥¨è®¡ç®—] ç›¸å…³ç³»æ•°è®¡ç®—å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {correlation.shape}")
        
        return correlation
    
    def _process_single_stock_results(self, stock_correlations, avg_correlations_filtered, high_corr_mask,
                                     period_info_list, evaluation_dates, stock_code, fields):
        """
        å¤„ç†å•ä¸ªè‚¡ç¥¨çš„ç›¸å…³æ€§ç»“æœ
        
        Args:
            stock_correlations: å•ä¸ªè‚¡ç¥¨çš„ç›¸å…³æ€§æ•°æ® [evaluation_days, num_historical_periods, 5]
            avg_correlations_filtered: è¿‡æ»¤åçš„å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
            high_corr_mask: é«˜ç›¸å…³æ€§æ©ç 
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            stock_code: è‚¡ç¥¨ä»£ç 
            fields: å­—æ®µåˆ—è¡¨
            
        Returns:
            list: è¯¦ç»†ç»“æœåˆ—è¡¨
        """
        detailed_results = []
        
        if evaluation_dates:
            for eval_idx, eval_date in enumerate(evaluation_dates):
                if eval_idx < avg_correlations_filtered.shape[0]:
                    eval_correlations = avg_correlations_filtered[eval_idx]  # è¯¥è¯„æµ‹æ—¥æœŸçš„ç›¸å…³æ€§åˆ—è¡¨
                    
                    # æ‰¾åˆ°é«˜ç›¸å…³æ€§æœŸé—´
                    high_corr_periods = []
                    for hist_idx, correlation in enumerate(eval_correlations):
                        if correlation >= self.threshold and hist_idx < len(period_info_list):
                            period_data = period_info_list[hist_idx]
                            
                            high_corr_periods.append({
                                'start_date': period_data['start_date'],
                                'end_date': period_data['end_date'],
                                'avg_correlation': float(correlation),
                                'stock_code': period_data['stock_code'],
                                'target_stock_code': stock_code,  # æ·»åŠ ç›®æ ‡è‚¡ç¥¨ä»£ç 
                                'source': 'gpu_batch'
                            })
                    
                    # è®¡ç®—è¯¥è¯„æµ‹æ—¥æœŸçš„é¢„æµ‹ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰æ•°æ®çš„è¯ï¼‰
                    stats = {}
                    if hasattr(self, 'data') and stock_code == self.stock_code:
                        # åªæœ‰å½“å‰è‚¡ç¥¨æ‰è®¡ç®—é¢„æµ‹ç»Ÿè®¡
                        stats = self.calculate_future_performance_stats(self.data, high_corr_periods)
                    elif hasattr(self, 'multi_stock_data') and stock_code in self.multi_stock_data:
                        # å¤šè‚¡ç¥¨æ¨¡å¼ä¸‹è®¡ç®—å¯¹åº”è‚¡ç¥¨çš„é¢„æµ‹ç»Ÿè®¡
                        stats = self.calculate_future_performance_stats(self.multi_stock_data[stock_code], high_corr_periods)
                    
                    detailed_results.append({
                        'evaluation_date': eval_date,
                        'target_stock_code': stock_code,  # æ·»åŠ ç›®æ ‡è‚¡ç¥¨ä»£ç 
                        'high_correlation_periods': high_corr_periods,
                        'daily_high_count': len(high_corr_periods),
                        'prediction_stats': stats
                    })
        
        return detailed_results

    def _process_batch_correlation_results(self, correlations_tensor, period_info_list, evaluation_days,
                                          batch_recent_data=None, historical_data_list=None, evaluation_dates=None,
                                          target_stock_codes=None):
        """
        å¤„ç†æ‰¹é‡ç›¸å…³æ€§è®¡ç®—ç»“æœï¼ˆæ•´åˆäº†é˜¶æ®µ5çš„è¯¦ç»†ç»“æœå¤„ç†å’Œä¿å­˜åŠŸèƒ½ï¼‰ï¼Œæ”¯æŒå¤šè‚¡ç¥¨
        
        Args:
            correlations_tensor: [evaluation_days, num_historical_periods, 5] (å•è‚¡ç¥¨)
                                æˆ– [num_stocks, evaluation_days, num_historical_periods, 5] (å¤šè‚¡ç¥¨)
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            target_stock_codes: ç›®æ ‡è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            dict: å¤„ç†åçš„å®Œæ•´æœ€ç»ˆç»“æœï¼ŒåŒ…å«è¯¦ç»†ç»“æœã€ç»Ÿè®¡ä¿¡æ¯å’Œæ€§èƒ½æ•°æ®
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„è®¡æ—¶å™¨ï¼Œè¦†ç›–åŸæ¥çš„4-5å’Œ5-1æ­¥éª¤
        self.start_timer('integrated_result_processing')
        
        correlations_np = correlations_tensor.cpu().numpy()
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # ç¡®ä¿target_stock_codesæœ‰å€¼
        if target_stock_codes is None:
            target_stock_codes = [self.stock_code] if not self.is_multi_stock else self.stock_codes
        
        # æ”¯æŒå¤šè‚¡ç¥¨å’Œå•è‚¡ç¥¨æ¨¡å¼
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼: correlations_npå½¢çŠ¶ä¸º [num_stocks, evaluation_days, num_historical_periods, 5]
            num_stocks = correlations_np.shape[0]
            self.logger.info(f"å¤„ç†å¤šè‚¡ç¥¨ç›¸å…³æ€§ç»“æœ: {num_stocks}åªè‚¡ç¥¨, {evaluation_days}ä¸ªè¯„æµ‹æ—¥æœŸ")
            
            # ä¸ºæ¯ä¸ªè‚¡ç¥¨åˆ†åˆ«å¤„ç†
            all_stock_results = {}
            all_detailed_results = []
            
            for stock_idx, stock_code in enumerate(target_stock_codes):
                self.logger.info(f"å¤„ç†è‚¡ç¥¨ {stock_code} ({stock_idx + 1}/{num_stocks})")
                
                # æå–å½“å‰è‚¡ç¥¨çš„ç›¸å…³æ€§æ•°æ® [evaluation_days, num_historical_periods, 5]
                stock_correlations = correlations_np[stock_idx]
                
                # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
                avg_correlations = stock_correlations.mean(axis=2)
                
                # è¿‡æ»¤æ‰ç›¸å…³æ€§ä¸º1.0çš„ç»“æœï¼ˆè‡ªç›¸å…³ï¼‰
                self_correlation_threshold = 0.9999
                self_correlation_mask = avg_correlations >= self_correlation_threshold
                
                # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è‡ªç›¸å…³æ•°é‡
                filtered_count = self_correlation_mask.sum()
                if filtered_count > 0:
                    self.logger.info(f"è‚¡ç¥¨ {stock_code}: è¿‡æ»¤æ‰ {filtered_count} ä¸ªè‡ªç›¸å…³ç»“æœï¼ˆç›¸å…³æ€§ >= {self_correlation_threshold}ï¼‰")
                
                # å°†è‡ªç›¸å…³çš„ä½ç½®è®¾ç½®ä¸º0ï¼Œä½¿å…¶ä¸ä¼šè¢«é€‰ä¸ºé«˜ç›¸å…³æ€§æœŸé—´
                avg_correlations_filtered = avg_correlations.copy()
                avg_correlations_filtered[self_correlation_mask] = 0.0
                
                # æ‰¾å‡ºé«˜ç›¸å…³æ€§æœŸé—´ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°ï¼‰
                high_corr_mask = avg_correlations_filtered > self.threshold
                
                # å¤„ç†å½“å‰è‚¡ç¥¨çš„è¯¦ç»†ç»“æœ
                stock_detailed_results = self._process_single_stock_results(
                    stock_correlations, avg_correlations_filtered, high_corr_mask,
                    period_info_list, evaluation_dates, stock_code, fields
                )
                
                all_detailed_results.extend(stock_detailed_results)
                all_stock_results[stock_code] = {
                    'high_corr_count': high_corr_mask.sum(),
                    'avg_correlation': avg_correlations_filtered[avg_correlations_filtered > 0].mean() if (avg_correlations_filtered > 0).any() else 0.0,
                    'max_correlation': avg_correlations_filtered.max()
                }
            
            # æ±‡æ€»å¤šè‚¡ç¥¨ç»“æœ
            total_high_corr = sum(result['high_corr_count'] for result in all_stock_results.values())
            overall_avg_corr = np.mean([result['avg_correlation'] for result in all_stock_results.values() if result['avg_correlation'] > 0])
            overall_max_corr = max([result['max_correlation'] for result in all_stock_results.values()])
            
        else:
            # å•è‚¡ç¥¨æ¨¡å¼: ä¿æŒåŸæœ‰é€»è¾‘
            # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
            avg_correlations = correlations_np.mean(axis=2)
            
            # è¿‡æ»¤æ‰ç›¸å…³æ€§ä¸º1.0çš„ç»“æœï¼ˆè‡ªç›¸å…³ï¼‰
            # è®¾ç½®å®¹å·®ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
            self_correlation_threshold = 0.9999
            self_correlation_mask = avg_correlations >= self_correlation_threshold
            
            # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è‡ªç›¸å…³æ•°é‡
            filtered_count = self_correlation_mask.sum()
            if filtered_count > 0:
                self.logger.info(f"è¿‡æ»¤æ‰ {filtered_count} ä¸ªè‡ªç›¸å…³ç»“æœï¼ˆç›¸å…³æ€§ >= {self_correlation_threshold}ï¼‰")
            
            # å°†è‡ªç›¸å…³çš„ä½ç½®è®¾ç½®ä¸º0ï¼Œä½¿å…¶ä¸ä¼šè¢«é€‰ä¸ºé«˜ç›¸å…³æ€§æœŸé—´
            avg_correlations_filtered = avg_correlations.copy()
            avg_correlations_filtered[self_correlation_mask] = 0.0
            
            # æ‰¾å‡ºé«˜ç›¸å…³æ€§æœŸé—´ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°ï¼‰
            high_corr_mask = avg_correlations_filtered > self.threshold
            
            # å¤„ç†å•è‚¡ç¥¨çš„è¯¦ç»†ç»“æœ
            all_detailed_results = self._process_single_stock_results(
                correlations_np, avg_correlations_filtered, high_corr_mask,
                period_info_list, evaluation_dates, target_stock_codes[0], fields
            )
            
            total_high_corr = high_corr_mask.sum()
            overall_avg_corr = avg_correlations_filtered[avg_correlations_filtered > 0].mean() if (avg_correlations_filtered > 0).any() else 0.0
            overall_max_corr = avg_correlations_filtered.max()
        
        # Debugæ¨¡å¼ä¸‹æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        if self.debug and not self.is_multi_stock:
            # å•è‚¡ç¥¨æ¨¡å¼ä¸‹æ‰æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œå¤šè‚¡ç¥¨æ¨¡å¼ä¸‹ä¿¡æ¯å¤ªå¤š
            self._print_detailed_evaluation_data(
                correlations_np, avg_correlations_filtered, period_info_list, 
                high_corr_mask, fields, batch_recent_data, historical_data_list, evaluation_dates
            )
        
        # æ„å»ºæ‰¹é‡ç»“æœï¼ˆæ”¯æŒå¤šè‚¡ç¥¨æ¨¡å¼ï¼‰
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šæ±‡æ€»æ‰€æœ‰è‚¡ç¥¨çš„ç»“æœ
            batch_results = {
                'evaluation_days': evaluation_days,
                'num_historical_periods': len(period_info_list),
                'stock_codes': target_stock_codes,
                'detailed_results': detailed_results,  # åŒ…å«æ‰€æœ‰è‚¡ç¥¨çš„è¯¦ç»†ç»“æœ
                'summary': {
                    'total_stocks': len(target_stock_codes),
                    'total_high_correlations': sum(result.get('total_high_correlations', 0) for result in detailed_results),
                    'avg_high_correlations_per_stock': sum(result.get('total_high_correlations', 0) for result in detailed_results) / len(target_stock_codes) if target_stock_codes else 0,
                    'filtered_self_correlations': int(filtered_count)
                }
            }
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰æ ¼å¼
            batch_results = {
                'evaluation_days': evaluation_days,
                'num_historical_periods': len(period_info_list),
                'high_correlation_counts': high_corr_mask.sum(axis=1).tolist(),  # æ¯ä¸ªè¯„æµ‹æ—¥æœŸçš„é«˜ç›¸å…³æ•°é‡
                'avg_correlations': avg_correlations_filtered.tolist(),  # ä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°
                'detailed_correlations': correlations_np.tolist(),
                'period_info': period_info_list,
                'detailed_results': detailed_results,  # æ–°å¢ï¼šè¯¦ç»†ç»“æœï¼ˆæ•´åˆé˜¶æ®µ5åŠŸèƒ½ï¼‰
                'summary': {
                    'total_high_correlations': high_corr_mask.sum(),
                    'avg_high_correlations_per_day': high_corr_mask.sum(axis=1).mean(),
                    'max_high_correlations_per_day': high_corr_mask.sum(axis=1).max(),
                    'overall_avg_correlation': avg_correlations_filtered[high_corr_mask].mean() if high_corr_mask.any() else 0,
                    'filtered_self_correlations': int(filtered_count)  # æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
                }
            }
        
        # æ•´åˆåŸé˜¶æ®µ5çš„åŠŸèƒ½ï¼šæ„å»ºæœ€ç»ˆç»“æœå¹¶ä¿å­˜
        final_result = {
            'stock_code': self.stock_code if not self.is_multi_stock else ','.join(target_stock_codes),
            'backtest_date': self.backtest_date,
            'evaluation_days': len(evaluation_dates) if evaluation_dates else evaluation_days,
            'window_size': self.window_size,
            'threshold': self.threshold,
            'evaluation_dates': evaluation_dates if evaluation_dates else [],
            'batch_results': batch_results,
            'performance_stats': self._get_performance_stats(),
            'is_multi_stock': self.is_multi_stock
        }
        
        # ä¿å­˜ç»“æœåˆ°CSVï¼ˆåŸé˜¶æ®µ5çš„åŠŸèƒ½ï¼‰
        if getattr(self, 'save_results', True):  # é»˜è®¤ä¸ºTrueï¼Œç¡®ä¿CSVä¿å­˜åŠŸèƒ½æ­£å¸¸å·¥ä½œ
            self.logger.info("å¼€å§‹ä¿å­˜æ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶...")
            self.save_batch_results_to_csv(final_result)
        else:
            self.logger.warning("CSVä¿å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡ä¿å­˜æ­¥éª¤")
        
        self.logger.info(f"æ‰¹é‡ç»“æœå¤„ç†å®Œæˆï¼ˆå·²æ•´åˆè¯¦ç»†ç»“æœå¤„ç†å’Œä¿å­˜åŠŸèƒ½ï¼‰")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {batch_results['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°: {batch_results['summary']['avg_high_correlations_per_day']:.2f}")
        
        self.end_timer('integrated_result_processing')
        return final_result

    def calculate_batch_gpu_correlation_optimized(self, batch_recent_data, historical_periods_data, evaluation_dates=None):
        """
        ä¼˜åŒ–ç‰ˆæ‰¹é‡GPUç›¸å…³æ€§è®¡ç®— - æ”¯æŒå¤šç›®æ ‡è‚¡ç¥¨åŒæ—¶å¤„ç†
        
        Args:
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® 
                - å•è‚¡ç¥¨æ¨¡å¼: [evaluation_days, window_size, 5]
                - å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, window_size, 5]
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            dict: æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        """
        
        if batch_recent_data is None or len(historical_periods_data) == 0:
            return {}

        # ä½¿ç”¨ä¼ å‚åˆ¤æ–­æ¨¡å¼ï¼Œä¸ä¾èµ–çŸ©é˜µå½¢çŠ¶
        is_multi_stock = self.is_multi_stock
        if is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, window_size, 5]
            num_stocks, evaluation_days, window_size, num_fields = batch_recent_data.shape
            self.logger.info(f"å¤šè‚¡ç¥¨æ¨¡å¼: {num_stocks} ä¸ªè‚¡ç¥¨")
        else:
            # å•è‚¡ç¥¨æ¨¡å¼: å¯èƒ½æ˜¯ [evaluation_days, window_size, 5] æˆ–å·²è½¬æ¢çš„ [1, evaluation_days, window_size, 5]
            if len(batch_recent_data.shape) == 3:
                evaluation_days, window_size, num_fields = batch_recent_data.shape
                # ä¸ºäº†ç»Ÿä¸€å¤„ç†ï¼Œå°†å•è‚¡ç¥¨æ•°æ®æ‰©å±•ä¸€ä¸ªç»´åº¦
                batch_recent_data = batch_recent_data.unsqueeze(0)  # [1, evaluation_days, window_size, 5]
                self.logger.info(f"å•è‚¡ç¥¨æ¨¡å¼ï¼Œå·²è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼")
            else:
                # å·²ç»æ˜¯4ç»´æ ¼å¼ [1, evaluation_days, window_size, 5]
                num_stocks, evaluation_days, window_size, num_fields = batch_recent_data.shape
                self.logger.info(f"å•è‚¡ç¥¨æ¨¡å¼ï¼ˆå·²ä¸ºç»Ÿä¸€æ ¼å¼ï¼‰")
            num_stocks = 1
        
        num_historical_periods = len(historical_periods_data)
        
        self.logger.info(f"å¼€å§‹ä¼˜åŒ–ç‰ˆæ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—")
        self.logger.info(f"è‚¡ç¥¨æ•°: {num_stocks}, è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}, å†å²æœŸé—´æ•°: {num_historical_periods}")
        
        # å­æ­¥éª¤1/3: å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼šæ•°æ®åœ¨é˜¶æ®µ3å·²é¢„å¤„ç†ï¼‰
        self.start_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/3] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å¼€å§‹")
        
        # æ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼Œç›´æ¥æå–
        historical_data_list = []
        period_info_list = []
        
        for historical_values, start_date, end_date, stock_code in historical_periods_data:
            historical_data_list.append(historical_values)
            period_info_list.append({
                'start_date': start_date,
                'end_date': end_date,
                'stock_code': stock_code
            })
        
        valid_periods = len(historical_data_list)
        self.logger.info(f"å†å²æ•°æ®å‡†å¤‡å®Œæˆ: æœ‰æ•ˆæœŸé—´={valid_periods}ï¼ˆæ•°æ®å·²åœ¨é˜¶æ®µ3é¢„å¤„ç†ï¼‰")
        self.end_timer('gpu_step1_data_preparation')
        self.logger.info(f"  ğŸ” [å­æ­¥éª¤1/3] å†å²æ•°æ®å‡†å¤‡ï¼ˆå·²ä¼˜åŒ–ï¼‰ - å®Œæˆ")
        
        if not historical_data_list:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return {}
        
        # å­æ­¥éª¤2/3: åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡
        self.start_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/3] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å¼€å§‹")
        self.logger.info(f"å¼ é‡å½¢çŠ¶å°†ä¸º: [{len(historical_data_list)}, {window_size}, 5]")
        
        historical_tensor = torch.tensor(
            np.stack(historical_data_list, axis=0), 
            dtype=torch.float32, 
            device=self.device
        )  # [num_historical_periods, window_size, 5]
        
        self.logger.info(f"GPUå†å²æ•°æ®å¼ é‡åˆ›å»ºå®Œæˆ: {historical_tensor.shape}, è®¾å¤‡: {historical_tensor.device}")
        self.end_timer('gpu_step2_tensor_creation')
        self.logger.info(f"  ğŸ“Š [å­æ­¥éª¤2/3] åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ - å®Œæˆ")
        
        # ç›‘æ§æ•°æ®å¼ é‡åˆ›å»ºåçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("å¼ é‡åˆ›å»ºå®Œæˆ")
        
        # å­æ­¥éª¤3/3: åˆå¹¶çš„GPUç›¸å…³ç³»æ•°è®¡ç®—å’Œç»“æœå¤„ç†
        self.start_timer('gpu_step3_integrated_correlation_processing')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/3] åˆå¹¶çš„GPUç›¸å…³ç³»æ•°è®¡ç®—å’Œç»“æœå¤„ç† - å¼€å§‹")
        self.logger.info(f"è¾“å…¥å¼ é‡å½¢çŠ¶: batch_recent_data={batch_recent_data.shape}, historical_tensor={historical_tensor.shape}")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„GPUç«¯ä¸€ä½“åŒ–å¤„ç†
        results = self._compute_and_process_correlations_gpu(
            batch_recent_data, historical_tensor, period_info_list, 
            evaluation_days, evaluation_dates, num_stocks, is_multi_stock
        )
        
        self.end_timer('gpu_step3_integrated_correlation_processing')
        self.logger.info(f"  âš¡ [å­æ­¥éª¤3/3] åˆå¹¶çš„GPUç›¸å…³ç³»æ•°è®¡ç®—å’Œç»“æœå¤„ç† - å®Œæˆ")
        
        self.logger.info(f"ä¼˜åŒ–ç‰ˆæ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å…¨éƒ¨å®Œæˆï¼Œè¿”å›ç»“æœåŒ…å« {len(results) if results else 0} ä¸ªå­—æ®µ")
        return results

    def _calculate_batch_gpu_correlation_no_timer(self, batch_recent_data, historical_periods_data, evaluation_dates=None):
        """
        æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—ï¼ˆä¸å¸¦è®¡æ—¶å™¨ç‰ˆæœ¬ï¼‰- ç”¨äºå¤šè‚¡ç¥¨åˆ†æ‰¹å¤„ç†
        
        Args:
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ®
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            dict: æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        """
        
        if batch_recent_data is None or len(historical_periods_data) == 0:
            return {}
        
        # ä½¿ç”¨ä¼ å‚åˆ¤æ–­æ¨¡å¼ï¼Œä¸ä¾èµ–çŸ©é˜µå½¢çŠ¶
        is_multi_stock = self.is_multi_stock
        if is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, window_size, 5]
            num_stocks, evaluation_days, window_size, num_fields = batch_recent_data.shape
        else:
            # å•è‚¡ç¥¨æ¨¡å¼: å¯èƒ½æ˜¯ [evaluation_days, window_size, 5] æˆ–å·²è½¬æ¢çš„ [1, evaluation_days, window_size, 5]
            if len(batch_recent_data.shape) == 3:
                evaluation_days, window_size, num_fields = batch_recent_data.shape
                # ä¸ºäº†ç»Ÿä¸€å¤„ç†ï¼Œå°†å•è‚¡ç¥¨æ•°æ®æ‰©å±•ä¸€ä¸ªç»´åº¦
                batch_recent_data = batch_recent_data.unsqueeze(0)  # [1, evaluation_days, window_size, 5]
            else:
                # å·²ç»æ˜¯4ç»´æ ¼å¼ [1, evaluation_days, window_size, 5]
                num_stocks, evaluation_days, window_size, num_fields = batch_recent_data.shape
            num_stocks = 1
        
        num_historical_periods = len(historical_periods_data)
        
        # å†å²æ•°æ®å‡†å¤‡ï¼ˆä¸è®¡æ—¶ï¼‰
        historical_data_list = []
        period_info_list = []
        
        for historical_values, start_date, end_date, stock_code in historical_periods_data:
            historical_data_list.append(historical_values)
            period_info_list.append({
                'start_date': start_date,
                'end_date': end_date,
                'stock_code': stock_code
            })
        
        if not historical_data_list:
            return {}
        
        # åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡ï¼ˆä¸è®¡æ—¶ï¼‰
        historical_tensor = torch.tensor(
            np.stack(historical_data_list, axis=0), 
            dtype=torch.float32, 
            device=self.device
        )  # [num_historical_periods, window_size, 5]
        
        # GPUç›¸å…³ç³»æ•°è®¡ç®—å’Œç»“æœå¤„ç†ï¼ˆä¸è®¡æ—¶ï¼‰
        results = self._compute_and_process_correlations_gpu(
            batch_recent_data, historical_tensor, period_info_list, 
            evaluation_days, evaluation_dates, num_stocks, is_multi_stock
        )
        
        return results

    def _compute_and_process_correlations_gpu(self, batch_recent_data, historical_tensor, 
                                            period_info_list, evaluation_days, evaluation_dates, 
                                            num_stocks, is_multi_stock):
        """
        GPUç«¯ä¸€ä½“åŒ–ç›¸å…³ç³»æ•°è®¡ç®—å’Œç»“æœå¤„ç† - æ”¯æŒå¤šè‚¡ç¥¨
        
        Args:
            batch_recent_data: [num_stocks, evaluation_days, window_size, 5]
            historical_tensor: [num_historical_periods, window_size, 5]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            num_stocks: è‚¡ç¥¨æ•°é‡
            is_multi_stock: æ˜¯å¦ä¸ºå¤šè‚¡ç¥¨æ¨¡å¼
            
        Returns:
            dict: å¤„ç†åçš„å®Œæ•´æœ€ç»ˆç»“æœ
        """
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        batch_size = min(self.batch_size, evaluation_days)
        total_batches = (evaluation_days + batch_size - 1) // batch_size
        
        self.logger.info(f"GPUä¸€ä½“åŒ–å¤„ç†é…ç½®: batch_size={batch_size}, total_batches={total_batches}")
        self.logger.info(f"å¤šè‚¡ç¥¨å¤„ç†: {num_stocks} ä¸ªè‚¡ç¥¨åŒæ—¶å¤„ç†")
        
        # GPUç«¯å­˜å‚¨æ‰€æœ‰ç»“æœ - æ”¯æŒå¤šè‚¡ç¥¨
        all_avg_correlations = []  # æ¯ä¸ªå…ƒç´ : [num_stocks, batch_size, num_historical_periods]
        all_high_corr_masks = []   # æ¯ä¸ªå…ƒç´ : [num_stocks, batch_size, num_historical_periods]
        all_high_corr_counts = []  # æ¯ä¸ªå…ƒç´ : [num_stocks, batch_size]
        
        # åˆ›å»ºé˜ˆå€¼å¼ é‡ï¼ˆåœ¨GPUä¸Šï¼‰
        threshold_tensor = torch.tensor(self.threshold, device=self.device, dtype=torch.float32)
        self_correlation_threshold = torch.tensor(0.9999, device=self.device, dtype=torch.float32)
        
        for batch_idx, i in enumerate(range(0, evaluation_days, batch_size)):
            end_idx = min(i + batch_size, evaluation_days)
            current_batch = batch_recent_data[:, i:end_idx]  # [num_stocks, batch_size, window_size, 5]
            
            self.logger.info(f"GPUå¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: è¯„æµ‹æ—¥æœŸ {i+1}-{end_idx} (å½¢çŠ¶: {current_batch.shape})")
            
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç›¸å…³ç³»æ•° - æ”¯æŒå¤šè‚¡ç¥¨
            batch_correlations = self._compute_correlation_matrix_multi_stock(current_batch, historical_tensor)
            # batch_correlations: [num_stocks, batch_size, num_historical_periods, 5]
            
            # GPUç«¯è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°
            batch_avg_correlations = batch_correlations.mean(dim=3)  # [num_stocks, batch_size, num_historical_periods]
            
            # GPUç«¯è¿‡æ»¤è‡ªç›¸å…³ï¼ˆç›¸å…³æ€§ >= 0.9999ï¼‰
            self_corr_mask = batch_avg_correlations >= self_correlation_threshold
            batch_avg_correlations_filtered = batch_avg_correlations.clone()
            batch_avg_correlations_filtered[self_corr_mask] = 0.0
            
            # GPUç«¯è®¡ç®—é«˜ç›¸å…³æ€§æ©ç 
            batch_high_corr_mask = batch_avg_correlations_filtered > threshold_tensor
            
            # GPUç«¯è®¡ç®—æ¯ä¸ªè¯„æµ‹æ—¥æœŸçš„é«˜ç›¸å…³æ•°é‡
            batch_high_corr_counts = batch_high_corr_mask.sum(dim=2)  # [num_stocks, batch_size]
            
            # ğŸ” Debugæ¨¡å¼ï¼šä¸ºç¬¬ä¸€ä¸ªè¯„æµ‹æ—¥æœŸæ‰“å°è¯¦ç»†ä¿¡æ¯
            if self.debug and batch_idx == 0 and evaluation_dates and len(evaluation_dates) > 0:
                self._log_first_evaluation_debug_info(
                    batch_avg_correlations_filtered, batch_high_corr_mask, 
                    period_info_list, evaluation_dates, current_batch, historical_tensor, i, is_multi_stock
                )
            
            # å­˜å‚¨æ‰¹æ¬¡ç»“æœï¼ˆä»åœ¨GPUä¸Šï¼‰
            all_avg_correlations.append(batch_avg_correlations_filtered)
            all_high_corr_masks.append(batch_high_corr_mask)
            all_high_corr_counts.append(batch_high_corr_counts)
            
            # ç›‘æ§æ¯ä¸ªæ‰¹æ¬¡åçš„GPUæ˜¾å­˜
            if batch_idx % max(1, total_batches // 5) == 0:  # æ¯20%è¿›åº¦ç›‘æ§ä¸€æ¬¡
                self.monitor_gpu_memory(f"GPUæ‰¹æ¬¡{batch_idx + 1}å®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœï¼ˆä»åœ¨GPUä¸Šï¼‰- æ”¯æŒå¤šè‚¡ç¥¨
        all_avg_correlations_tensor = torch.cat(all_avg_correlations, dim=1)  # [num_stocks, evaluation_days, num_historical_periods]
        all_high_corr_masks_tensor = torch.cat(all_high_corr_masks, dim=1)    # [num_stocks, evaluation_days, num_historical_periods]
        all_high_corr_counts_tensor = torch.cat(all_high_corr_counts, dim=1)  # [num_stocks, evaluation_days]
        
        # GPUç«¯è®¡ç®—å…¨å±€ç»Ÿè®¡ - æ”¯æŒå¤šè‚¡ç¥¨
        if is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„æ€»ä½“ç»Ÿè®¡
            total_high_correlations = all_high_corr_masks_tensor.sum()
            
            # æ£€æŸ¥å¼ é‡æ˜¯å¦ä¸ºç©ºï¼Œé¿å…ç©ºå¼ é‡è°ƒç”¨ç»Ÿè®¡æ–¹æ³•
            if all_high_corr_counts_tensor.numel() > 0:
                avg_high_correlations_per_day = all_high_corr_counts_tensor.float().mean()
                max_high_correlations_per_day = all_high_corr_counts_tensor.max()
            else:
                avg_high_correlations_per_day = torch.tensor(0.0, device=self.device)
                max_high_correlations_per_day = torch.tensor(0, device=self.device)
            
            # è®¡ç®—æ¯ä¸ªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
            stock_summary = {}
            for stock_idx in range(num_stocks):
                stock_code = self.stock_codes[stock_idx] if hasattr(self, 'stock_codes') and stock_idx < len(self.stock_codes) else f"stock_{stock_idx}"
                stock_high_corr_count = all_high_corr_masks_tensor[stock_idx].sum()
                stock_high_corr_values = all_avg_correlations_tensor[stock_idx][all_high_corr_masks_tensor[stock_idx]]
                stock_avg_correlation = stock_high_corr_values.mean() if stock_high_corr_values.numel() > 0 else torch.tensor(0.0, device=self.device)
                
                stock_summary[stock_code] = {
                    'high_correlations': int(stock_high_corr_count.item()),
                    'avg_correlation': float(stock_avg_correlation.item())
                }
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
            all_avg_correlations_tensor = all_avg_correlations_tensor.squeeze(0)  # [evaluation_days, num_historical_periods]
            all_high_corr_masks_tensor = all_high_corr_masks_tensor.squeeze(0)    # [evaluation_days, num_historical_periods]
            all_high_corr_counts_tensor = all_high_corr_counts_tensor.squeeze(0)  # [evaluation_days]
            
            total_high_correlations = all_high_corr_masks_tensor.sum()
            
            # æ£€æŸ¥å¼ é‡æ˜¯å¦ä¸ºç©ºï¼Œé¿å…ç©ºå¼ é‡è°ƒç”¨ç»Ÿè®¡æ–¹æ³•
            if all_high_corr_counts_tensor.numel() > 0:
                avg_high_correlations_per_day = all_high_corr_counts_tensor.float().mean()
                max_high_correlations_per_day = all_high_corr_counts_tensor.max()
            else:
                avg_high_correlations_per_day = torch.tensor(0.0, device=self.device)
                max_high_correlations_per_day = torch.tensor(0, device=self.device)
            stock_summary = None
        
        # è®¡ç®—æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°ï¼ˆåªå¯¹é«˜ç›¸å…³æ€§çš„ï¼‰
        high_corr_values = all_avg_correlations_tensor[all_high_corr_masks_tensor]
        overall_avg_correlation = high_corr_values.mean() if high_corr_values.numel() > 0 else torch.tensor(0.0, device=self.device)
        
        self.logger.info(f"GPUç«¯ç»Ÿè®¡å®Œæˆ - æ€»é«˜ç›¸å…³æ•°: {total_high_correlations.item()}, "
                        f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°: {avg_high_correlations_per_day.item():.2f}")
        
        # åªåœ¨éœ€è¦è¯¦ç»†ç»“æœæ—¶æ‰ä¼ è¾“åˆ°CPU - æ”¯æŒå¤šè‚¡ç¥¨
        if evaluation_dates and len(evaluation_dates) > 0:
            # ä¼ è¾“å¿…è¦çš„æ•°æ®åˆ°CPUè¿›è¡Œè¯¦ç»†ç»“æœæ„å»º
            avg_correlations_cpu = all_avg_correlations_tensor.cpu().numpy()
            high_corr_masks_cpu = all_high_corr_masks_tensor.cpu().numpy()
            
            if is_multi_stock:
                # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šä¸ºæ¯ä¸ªè‚¡ç¥¨æ„å»ºè¯¦ç»†ç»“æœ
                detailed_results = {}
                for stock_idx in range(num_stocks):
                    stock_code = self.stock_codes[stock_idx] if hasattr(self, 'stock_codes') and stock_idx < len(self.stock_codes) else f"stock_{stock_idx}"
                    stock_avg_correlations = avg_correlations_cpu[stock_idx]
                    stock_high_corr_masks = high_corr_masks_cpu[stock_idx]
                    
                    detailed_results[stock_code] = self._build_detailed_results_cpu(
                        stock_avg_correlations, stock_high_corr_masks, period_info_list, evaluation_dates
                    )
            else:
                # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
                detailed_results = self._build_detailed_results_cpu(
                    avg_correlations_cpu, high_corr_masks_cpu, period_info_list, evaluation_dates
                )
        else:
            detailed_results = {} if is_multi_stock else []
        
        # æ„å»ºæœ€ç»ˆç»“æœï¼ˆå¤§éƒ¨åˆ†æ•°æ®å·²åœ¨GPUä¸Šè®¡ç®—å®Œæˆï¼‰- æ”¯æŒå¤šè‚¡ç¥¨
        batch_results = {
            'evaluation_days': evaluation_days,
            'num_historical_periods': len(period_info_list),
            'high_correlation_counts': all_high_corr_counts_tensor.cpu().tolist(),
            'avg_correlations': all_avg_correlations_tensor.cpu().tolist(),
            'period_info': period_info_list,
            'detailed_results': detailed_results,
            'summary': {
                'total_high_correlations': int(total_high_correlations.item()),
                'avg_high_correlations_per_day': float(avg_high_correlations_per_day.item()),
                'max_high_correlations_per_day': int(max_high_correlations_per_day.item()),
                'overall_avg_correlation': float(overall_avg_correlation.item()),
                'stock_summary': stock_summary  # æ·»åŠ æ¯ä¸ªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
            }
        }
        
        # æ„å»ºæœ€ç»ˆç»“æœ - æ”¯æŒå¤šè‚¡ç¥¨
        if is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰è‚¡ç¥¨çš„ç»“æœ
            final_result = {
                'stock_codes': self.stock_codes if hasattr(self, 'stock_codes') else [f"stock_{i}" for i in range(num_stocks)],
                'backtest_date': self.backtest_date,
                'evaluation_days': len(evaluation_dates) if evaluation_dates else evaluation_days,
                'window_size': self.window_size,
                'threshold': self.threshold,
                'evaluation_dates': evaluation_dates if evaluation_dates else [],
                'batch_results': batch_results,
                'performance_stats': self._get_performance_stats(),
                'is_multi_stock': True
            }
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰ç»“æ„
            final_result = {
                'stock_code': self.stock_code,
                'backtest_date': self.backtest_date,
                'evaluation_days': len(evaluation_dates) if evaluation_dates else evaluation_days,
                'window_size': self.window_size,
                'threshold': self.threshold,
                'evaluation_dates': evaluation_dates if evaluation_dates else [],
                'batch_results': batch_results,
                'performance_stats': self._get_performance_stats(),
                'is_multi_stock': False
            }
        
        # ä¿å­˜ç»“æœåˆ°CSV
        if getattr(self, 'save_results', True):
            self.logger.info("å¼€å§‹ä¿å­˜ä¼˜åŒ–ç‰ˆæ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶...")
            self.save_batch_results_to_csv(final_result)
        
        return final_result

    def _build_detailed_results_cpu(self, avg_correlations_cpu, high_corr_masks_cpu, 
                                   period_info_list, evaluation_dates):
        """
        åœ¨CPUä¸Šæ„å»ºè¯¦ç»†ç»“æœï¼ˆä»…åœ¨éœ€è¦æ—¶è°ƒç”¨ï¼‰
        
        Args:
            avg_correlations_cpu: CPUä¸Šçš„å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
            high_corr_masks_cpu: CPUä¸Šçš„é«˜ç›¸å…³æ€§æ©ç  [evaluation_days, num_historical_periods]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            list: è¯¦ç»†ç»“æœåˆ—è¡¨
        """
        detailed_results = []
        
        for eval_idx, eval_date in enumerate(evaluation_dates):
            if eval_idx < avg_correlations_cpu.shape[0]:
                eval_correlations = avg_correlations_cpu[eval_idx]
                eval_high_corr_mask = high_corr_masks_cpu[eval_idx]
                
                # æ‰¾åˆ°é«˜ç›¸å…³æ€§æœŸé—´
                high_corr_periods = []
                high_corr_indices = np.where(eval_high_corr_mask)[0]
                
                for hist_idx in high_corr_indices:
                    if hist_idx < len(period_info_list):
                        period_data = period_info_list[hist_idx]
                        correlation = eval_correlations[hist_idx]
                        
                        high_corr_periods.append({
                            'start_date': period_data['start_date'],
                            'end_date': period_data['end_date'],
                            'avg_correlation': float(correlation),
                            'stock_code': period_data['stock_code'],
                            'source': 'gpu_optimized'
                        })
                
                # è®¡ç®—è¯¥è¯„æµ‹æ—¥æœŸçš„é¢„æµ‹ç»Ÿè®¡
                stats = self.calculate_future_performance_stats(self.data, high_corr_periods)
                
                detailed_results.append({
                    'evaluation_date': eval_date,
                    'high_correlation_periods': high_corr_periods,
                    'daily_high_count': len(high_corr_periods),
                    'prediction_stats': stats
                })
        
        return detailed_results
    
    def _print_detailed_evaluation_data(self, correlations_np, avg_correlations_filtered, 
                                       period_info_list, high_corr_mask, fields,
                                       batch_recent_data=None, historical_data_list=None, evaluation_dates=None):
        """
        æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯¹æ¯”æ•°ç»„
        
        Args:
            correlations_np: è¯¦ç»†ç›¸å…³ç³»æ•°æ•°ç»„ [evaluation_days, num_historical_periods, 5]
            avg_correlations_filtered: è¿‡æ»¤åçš„å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            high_corr_mask: é«˜ç›¸å…³æ€§æ©ç 
            fields: å­—æ®µåç§°åˆ—è¡¨
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® [evaluation_days, window_size, 5]
            historical_data_list: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        """
        self.logger.info("=" * 80)
        self.logger.info("DEBUGæ¨¡å¼ - å‰10æ¡è¯„æµ‹æ•°æ®è¯¦ç»†ä¿¡æ¯:")
        self.logger.info("=" * 80)
        
        evaluation_days, num_historical_periods, num_fields = correlations_np.shape
        max_display_count = min(10, evaluation_days * num_historical_periods)
        
        # æ”¶é›†å‰10æ¡è¯„æµ‹æ•°æ®ï¼ˆæŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºï¼‰
        all_evaluation_data = []
        count = 0
        
        # æŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºéå†ï¼Œæ¯ä¸ªè¯„æµ‹æ—¥æœŸå–ç¬¬ä¸€ä¸ªå†å²æœŸé—´çš„æ•°æ®
        for eval_idx in range(evaluation_days):
            if count >= 10:  # åªå–å‰10æ¡
                break
            for hist_idx in range(num_historical_periods):
                if count >= 10:  # åªå–å‰10æ¡
                    break
                    
                avg_corr = avg_correlations_filtered[eval_idx, hist_idx]
                detailed_corr = correlations_np[eval_idx, hist_idx]
                is_high_corr = high_corr_mask[eval_idx, hist_idx]
                
                period_info = period_info_list[hist_idx]
                
                all_evaluation_data.append({
                    'eval_idx': eval_idx,
                    'hist_idx': hist_idx,
                    'avg_correlation': avg_corr,
                    'detailed_correlations': detailed_corr,
                    'is_high_correlation': is_high_corr,
                    'period_info': period_info
                })
                count += 1
        
        # æ‰“å°å‰10æ¡æ•°æ®ï¼ˆæŒ‰è¯„æµ‹æ—¥æœŸé¡ºåºï¼‰
        for i, data in enumerate(all_evaluation_data):
            self.logger.info(f"\nç¬¬ {i+1} æ¡è¯„æµ‹æ•°æ®:")
            self.logger.info(f"  è¯„æµ‹æ—¥æœŸç´¢å¼•: {data['eval_idx']}")
            
            # æ·»åŠ è¯„æµ‹æ•°æ®æ—¶é—´æ®µä¿¡æ¯
            if evaluation_dates and data['eval_idx'] < len(evaluation_dates):
                eval_date = evaluation_dates[data['eval_idx']]
                # è®¡ç®—è¯„æµ‹æ•°æ®çš„æ—¶é—´æ®µï¼ˆä»è¯„æµ‹æ—¥æœŸå¾€å‰æ¨window_sizeå¤©ï¼‰
                eval_start_date = eval_date - pd.Timedelta(days=self.window_size - 1)
                self.logger.info(f"  è¯„æµ‹æ•°æ®æ—¶é—´æ®µ: {eval_start_date.strftime('%Y-%m-%d')} åˆ° {eval_date.strftime('%Y-%m-%d')}")
            
            self.logger.info(f"  å†å²æœŸé—´ç´¢å¼•: {data['hist_idx']}")
            self.logger.info(f"  å†å²æœŸé—´: {data['period_info']['start_date'].strftime('%Y-%m-%d')} åˆ° {data['period_info']['end_date'].strftime('%Y-%m-%d')}")
            self.logger.info(f"  æ¥æºè‚¡ç¥¨: {data['period_info']['stock_code']}")
            self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {data['avg_correlation']:.6f}")
            self.logger.info(f"  æ˜¯å¦é«˜ç›¸å…³: {'æ˜¯' if data['is_high_correlation'] else 'å¦'}")
            
            # æ‰“å°å„å­—æ®µçš„è¯¦ç»†ç›¸å…³ç³»æ•°
            self.logger.info("  å„å­—æ®µç›¸å…³ç³»æ•°:")
            for j, field in enumerate(fields):
                self.logger.info(f"    {field}: {data['detailed_correlations'][j]:.6f}")
            
            # æ‰“å°å¯¹æ¯”æ•°ç»„ï¼ˆå¦‚æœæœ‰åŸå§‹æ•°æ®ï¼‰
            if batch_recent_data is not None and historical_data_list is not None:
                eval_idx = data['eval_idx']
                hist_idx = data['hist_idx']
                
                # è·å–è¯„æµ‹æ•°æ®ï¼ˆè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼‰
                recent_data = batch_recent_data[eval_idx]  # [window_size, 5]
                if isinstance(recent_data, torch.Tensor):
                    recent_data = recent_data.cpu().numpy()
                
                # è·å–å†å²æ•°æ®
                if hist_idx < len(historical_data_list):
                    historical_data = historical_data_list[hist_idx]  # [window_size, 5]
                    if isinstance(historical_data, torch.Tensor):
                        historical_data = historical_data.cpu().numpy()
                    
                    self.logger.info("  å¯¹æ¯”æ•°ç»„è¯¦æƒ…:")
                    self.logger.info(f"    æ•°æ®çª—å£å¤§å°: {recent_data.shape[0]} å¤©")
                    
                    # æ‰“å°å‰5å¤©å’Œå5å¤©çš„æ•°æ®å¯¹æ¯”
                    for field_idx, field in enumerate(fields):
                        self.logger.info(f"    {field} å­—æ®µå¯¹æ¯”:")
                        self.logger.info(f"      è¯„æµ‹æ•°æ®å‰5å¤©: {recent_data[:5, field_idx].tolist()}")
                        self.logger.info(f"      å†å²æ•°æ®å‰5å¤©: {historical_data[:5, field_idx].tolist()}")
                        self.logger.info(f"      è¯„æµ‹æ•°æ®å5å¤©: {recent_data[-5:, field_idx].tolist()}")
                        self.logger.info(f"      å†å²æ•°æ®å5å¤©: {historical_data[-5:, field_idx].tolist()}")
                        
                        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                        recent_mean = np.mean(recent_data[:, field_idx])
                        historical_mean = np.mean(historical_data[:, field_idx])
                        recent_std = np.std(recent_data[:, field_idx])
                        historical_std = np.std(historical_data[:, field_idx])
                        
                        self.logger.info(f"      è¯„æµ‹æ•°æ®ç»Ÿè®¡ - å‡å€¼: {recent_mean:.4f}, æ ‡å‡†å·®: {recent_std:.4f}")
                        self.logger.info(f"      å†å²æ•°æ®ç»Ÿè®¡ - å‡å€¼: {historical_mean:.4f}, æ ‡å‡†å·®: {historical_std:.4f}")
            
            self.logger.info("-" * 60)
        
        self.logger.info("=" * 80)
    
    def _log_first_evaluation_debug_info(self, batch_avg_correlations_filtered, batch_high_corr_mask, 
                                        period_info_list, evaluation_dates, current_batch, historical_tensor, batch_start_idx, is_multi_stock):
        """
        ä¸ºç¬¬ä¸€ä¸ªè¯„æµ‹æ—¥æœŸæ‰“å°è¯¦ç»†çš„debugä¿¡æ¯
        
        Args:
            batch_avg_correlations_filtered: è¿‡æ»¤åçš„å¹³å‡ç›¸å…³ç³»æ•° [num_stocks, batch_size, num_historical_periods] æˆ– [batch_size, num_historical_periods]
            batch_high_corr_mask: é«˜ç›¸å…³æ€§æ©ç  [num_stocks, batch_size, num_historical_periods] æˆ– [batch_size, num_historical_periods]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            current_batch: å½“å‰æ‰¹æ¬¡çš„è¯„æµ‹æ•°æ® [num_stocks, batch_size, window_size, 5] æˆ– [batch_size, window_size, 5]
            historical_tensor: å†å²æ•°æ®å¼ é‡ [num_historical_periods, window_size, 5]
            batch_start_idx: å½“å‰æ‰¹æ¬¡çš„èµ·å§‹ç´¢å¼•
            is_multi_stock: æ˜¯å¦ä¸ºå¤šè‚¡ç¥¨æ¨¡å¼
        """
        # è·å–ç¬¬ä¸€ä¸ªè¯„æµ‹æ—¥æœŸçš„ä¿¡æ¯
        first_eval_date = evaluation_dates[batch_start_idx]
        
        if is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šå–ç¬¬ä¸€ä¸ªè‚¡ç¥¨çš„ç¬¬ä¸€ä¸ªè¯„æµ‹æ—¥æœŸ
            first_eval_correlations = batch_avg_correlations_filtered[0, 0]  # [num_historical_periods]
            first_eval_high_corr_mask = batch_high_corr_mask[0, 0]  # [num_historical_periods]
            first_eval_data = current_batch[0, 0]  # [window_size, 5]
        else:
            # å•è‚¡ç¥¨æ¨¡å¼
            first_eval_correlations = batch_avg_correlations_filtered[0]  # [num_historical_periods]
            first_eval_high_corr_mask = batch_high_corr_mask[0]  # [num_historical_periods]
            first_eval_data = current_batch[0]  # [window_size, 5]
        
        # è½¬æ¢ä¸ºCPU numpyæ•°ç»„ä»¥ä¾¿å¤„ç†
        first_eval_correlations_np = first_eval_correlations.cpu().numpy()
        first_eval_high_corr_mask_np = first_eval_high_corr_mask.cpu().numpy()
        first_eval_data_np = first_eval_data.cpu().numpy()
        
        # æ‰¾åˆ°æ‰€æœ‰è¶…è¿‡é˜ˆå€¼çš„å¯¹æ¯”æ—¥æœŸ
        high_corr_indices = np.where(first_eval_high_corr_mask_np)[0]
        
        self.logger.info("ğŸ”" + "=" * 80)
        self.logger.info(f"ğŸ” DEBUGæ¨¡å¼ - ç¬¬ä¸€ä¸ªè¯„æµ‹æ—¥æœŸè¯¦ç»†ä¿¡æ¯")
        self.logger.info("ğŸ”" + "=" * 80)
        self.logger.info(f"ğŸ” è¯„æµ‹æ—¥æœŸ: {first_eval_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"ğŸ” è¯„æµ‹æ•°æ®çª—å£: {first_eval_date - pd.Timedelta(days=self.window_size-1)} åˆ° {first_eval_date}")
        self.logger.info(f"ğŸ” è¶…è¿‡é˜ˆå€¼çš„å¯¹æ¯”æœŸé—´æ•°é‡: {len(high_corr_indices)}")
        
        if len(high_corr_indices) > 0:
            self.logger.info("ğŸ” è¶…è¿‡é˜ˆå€¼çš„å¯¹æ¯”æ—¥æœŸå’Œç›¸å…³ç³»æ•°:")
            
            # æŒ‰ç›¸å…³ç³»æ•°é™åºæ’åˆ—
            sorted_indices = high_corr_indices[np.argsort(-first_eval_correlations_np[high_corr_indices])]
            
            for rank, hist_idx in enumerate(sorted_indices[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                period_info = period_info_list[hist_idx]
                correlation = first_eval_correlations_np[hist_idx]
                
                self.logger.info(f"ğŸ”   #{rank} å†å²æœŸé—´ {hist_idx}: {period_info['start_date']} åˆ° {period_info['end_date']}")
                self.logger.info(f"ğŸ”       æ¥æºè‚¡ç¥¨: {period_info['stock_code']}")
                self.logger.info(f"ğŸ”       å¹³å‡ç›¸å…³ç³»æ•°: {correlation:.6f}")
                
                # è·å–å¯¹åº”çš„å†å²æ•°æ®
                historical_data_np = historical_tensor[hist_idx].cpu().numpy()  # [window_size, 5]
                
                # æ‰“å°æºæ•°æ®åˆ—çš„è¯¦ç»†å¯¹æ¯”
                fields = ['open', 'high', 'low', 'close', 'volume']
                self.logger.info(f"ğŸ”       æºæ•°æ®åˆ—å¯¹æ¯” (å‰3å¤©å’Œå3å¤©):")
                
                for field_idx, field in enumerate(fields):
                    eval_field_data = first_eval_data_np[:, field_idx]
                    hist_field_data = historical_data_np[:, field_idx]
                    
                    # è®¡ç®—ç›¸å…³ç³»æ•°
                    field_correlation = np.corrcoef(eval_field_data, hist_field_data)[0, 1]
                    
                    self.logger.info(f"ğŸ”         {field} (ç›¸å…³ç³»æ•°: {field_correlation:.6f}):")
                    self.logger.info(f"ğŸ”           è¯„æµ‹æ•°æ®å‰3å¤©: {eval_field_data[:3].tolist()}")
                    self.logger.info(f"ğŸ”           å†å²æ•°æ®å‰3å¤©: {hist_field_data[:3].tolist()}")
                    self.logger.info(f"ğŸ”           è¯„æµ‹æ•°æ®å3å¤©: {eval_field_data[-3:].tolist()}")
                    self.logger.info(f"ğŸ”           å†å²æ•°æ®å3å¤©: {hist_field_data[-3:].tolist()}")
                
                self.logger.info("ğŸ”" + "-" * 60)
            
            if len(high_corr_indices) > 10:
                self.logger.info(f"ğŸ”   ... è¿˜æœ‰ {len(high_corr_indices) - 10} ä¸ªè¶…è¿‡é˜ˆå€¼çš„æœŸé—´")
        else:
            self.logger.info("ğŸ” æ²¡æœ‰æ‰¾åˆ°è¶…è¿‡é˜ˆå€¼çš„å¯¹æ¯”æœŸé—´")
        
        # æ‰“å°è¯„æµ‹æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        self.logger.info("ğŸ” è¯„æµ‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        fields = ['open', 'high', 'low', 'close', 'volume']
        for field_idx, field in enumerate(fields):
            field_data = first_eval_data_np[:, field_idx]
            self.logger.info(f"ğŸ”   {field}: å‡å€¼={np.mean(field_data):.4f}, æ ‡å‡†å·®={np.std(field_data):.4f}, æœ€å°å€¼={np.min(field_data):.4f}, æœ€å¤§å€¼={np.max(field_data):.4f}")
        
        self.logger.info("ğŸ”" + "=" * 80)
    
    def calculate_future_performance_stats(self, data, high_correlation_periods):
        """
        è®¡ç®—é«˜ç›¸å…³æ€§æœŸé—´çš„æœªæ¥äº¤æ˜“æ—¥è¡¨ç°ç»Ÿè®¡
        
        Args:
            data: å®Œæ•´çš„è‚¡ç¥¨æ•°æ®
            high_correlation_periods: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
            
        Returns:
            dict: ç»Ÿè®¡ç»“æœ
        """
        if not high_correlation_periods:
            return None
        
        stats = {
            'total_periods': len(high_correlation_periods),
            'next_day_gap_up': 0,  # ä¸‹1ä¸ªäº¤æ˜“æ—¥é«˜å¼€
            'next_1_day_up': 0,    # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_3_day_up': 0,    # ä¸‹3ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_5_day_up': 0,    # ä¸‹5ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_10_day_up': 0,   # ä¸‹10ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'valid_periods': {
                'next_day': 0,
                'next_3_day': 0,
                'next_5_day': 0,
                'next_10_day': 0
            }
        }
        
        for i, period in enumerate(high_correlation_periods, 1):
            end_date = period['end_date']
            start_date = period['start_date']
            avg_correlation = period['avg_correlation']
            source_stock_code = period['stock_code']
            
            # æ ¹æ®æ¥æºè‚¡ç¥¨ä»£ç è·å–æ­£ç¡®çš„æ•°æ®æº
            if source_stock_code == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                source_data = self.loaded_stocks_data.get(source_stock_code)
                if source_data is None:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {source_stock_code} çš„æ•°æ®ï¼Œè·³è¿‡æœŸé—´ #{i}")
                    continue
            
            # æ‰¾åˆ°è¯¥æœŸé—´ç»“æŸåçš„æ•°æ®ä½ç½®
            try:
                end_idx = source_data.index.get_loc(end_date)
            except KeyError:
                if self.debug:
                    self.logger.warning(f"åœ¨è‚¡ç¥¨ {source_stock_code} æ•°æ®ä¸­æ‰¾ä¸åˆ°æ—¥æœŸ {end_date}ï¼Œè·³è¿‡æœŸé—´ #{i}")
                continue
            
            # è·å–æœŸé—´æœ€åä¸€å¤©çš„æ”¶ç›˜ä»·
            period_close = source_data.iloc[end_idx]['close']
            
            # æ£€æŸ¥ä¸‹1ä¸ªäº¤æ˜“æ—¥
            if end_idx + 1 < len(source_data):
                next_day_data = source_data.iloc[end_idx + 1]
                next_day_open = next_day_data['open']
                next_day_close = next_day_data['close']
                
                stats['valid_periods']['next_day'] += 1
                
                # é«˜å¼€åˆ¤æ–­
                if next_day_open > period_close:
                    stats['next_day_gap_up'] += 1
                
                # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨åˆ¤æ–­
                if next_day_close > period_close:
                    stats['next_1_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹3ä¸ªäº¤æ˜“æ—¥
            if end_idx + 3 < len(source_data):
                day_3_close = source_data.iloc[end_idx + 3]['close']
                stats['valid_periods']['next_3_day'] += 1
                
                if day_3_close > period_close:
                    stats['next_3_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹5ä¸ªäº¤æ˜“æ—¥
            if end_idx + 5 < len(source_data):
                day_5_close = source_data.iloc[end_idx + 5]['close']
                stats['valid_periods']['next_5_day'] += 1
                
                if day_5_close > period_close:
                    stats['next_5_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹10ä¸ªäº¤æ˜“æ—¥
            if end_idx + 10 < len(source_data):
                day_10_close = source_data.iloc[end_idx + 10]['close']
                stats['valid_periods']['next_10_day'] += 1
                
                if day_10_close > period_close:
                    stats['next_10_day_up'] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        stats['ratios'] = {}
        if stats['valid_periods']['next_day'] > 0:
            stats['ratios']['next_day_gap_up'] = stats['next_day_gap_up'] / stats['valid_periods']['next_day']
            stats['ratios']['next_1_day_up'] = stats['next_1_day_up'] / stats['valid_periods']['next_day']
        
        if stats['valid_periods']['next_3_day'] > 0:
            stats['ratios']['next_3_day_up'] = stats['next_3_day_up'] / stats['valid_periods']['next_3_day']
        
        if stats['valid_periods']['next_5_day'] > 0:
            stats['ratios']['next_5_day_up'] = stats['next_5_day_up'] / stats['valid_periods']['next_5_day']
        
        if stats['valid_periods']['next_10_day'] > 0:
            stats['ratios']['next_10_day_up'] = stats['next_10_day_up'] / stats['valid_periods']['next_10_day']
        
        return stats
    

    
    def analyze_batch(self, backtest_date=None, evaluation_days=None, window_size=None, 
                     threshold=None, comparison_mode=None, comparison_stocks=None, debug=None):
        """
        æ‰¹é‡åˆ†æä¸»å‡½æ•°
        
        Args:
            backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            window_size: çª—å£å¤§å°
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            debug: è°ƒè¯•æ¨¡å¼
            
        Returns:
            dict: æ‰¹é‡åˆ†æç»“æœ
        """
        self.start_timer('total_batch_analysis')
        
        # æ›´æ–°å‚æ•°
        if backtest_date is not None:
            self.backtest_date = pd.to_datetime(backtest_date)
        if evaluation_days is not None:
            self.evaluation_days = evaluation_days
        if window_size is not None:
            self.window_size = window_size
        if threshold is not None:
            self.threshold = threshold
        if comparison_mode is not None:
            self.comparison_mode = comparison_mode
        if comparison_stocks is not None:
            self.comparison_stocks = comparison_stocks
        if debug is not None:
            self.debug = debug
        
        self.logger.info("=" * 80)
        self.logger.info(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ")
        if self.is_multi_stock:
            self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_codes} (å¤šè‚¡ç¥¨æ¨¡å¼ï¼Œå…±{len(self.stock_codes)}åª)")
        else:
            self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"å›æµ‹ç»“æŸæ—¥æœŸ: {self.backtest_date}")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {self.evaluation_days}")
        self.logger.info(f"æ¯æ‰¹æ¬¡å¤„ç†æ•°é‡: {self.evaluation_batch_size}")
        self.logger.info(f"çª—å£å¤§å°: {self.window_size}")
        self.logger.info(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {self.threshold}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {self.comparison_mode}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}")
        
        # å¤šè‚¡ç¥¨æ¨¡å¼æ€»è®¡ç®—é‡ä¿¡æ¯ï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
        if self.is_multi_stock:
            total_computation_units = len(self.stock_codes) * self.evaluation_days
            self.logger.info(f"ğŸ“Š å¤šè‚¡ç¥¨æ¨¡å¼æ€»è®¡ç®—é‡: {len(self.stock_codes)} è‚¡ç¥¨ Ã— {self.evaluation_days} è¯„æµ‹æ—¥æœŸ = {total_computation_units} è®¡ç®—å•å…ƒ")
        
        self.logger.info("=" * 80)
        
        # åˆå§‹GPUæ˜¾å­˜ç›‘æ§
        self.monitor_gpu_memory("åˆ†æå¼€å§‹")
        
        # ğŸ“š ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½ - å¼€å§‹
        self.logger.info("ğŸ“š [é˜¶æ®µ1/4] æ•°æ®åŠ è½½ - å¼€å§‹")
        # å…ˆåŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆç›®æ ‡è‚¡ç¥¨å’Œå¯¹æ¯”è‚¡ç¥¨ï¼‰
        self.data = self.load_data()
        if self.data is None:
            self.logger.error("æ•°æ®åŠ è½½å¤±è´¥")
            return None
        self.logger.info("ğŸ“š [é˜¶æ®µ1/4] æ•°æ®åŠ è½½ - å®Œæˆ")
        
        # ğŸ”„ ç¬¬2é˜¶æ®µï¼šå†å²æ•°æ®å¤„ç† - å¼€å§‹
        self.logger.info("ğŸ”„ [é˜¶æ®µ2/4] å†å²æ•°æ®å¤„ç† - å¼€å§‹")
        # æ”¶é›†å†å²æœŸé—´æ•°æ®
        historical_periods_data = self._collect_historical_periods_data()
        
        if not historical_periods_data:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return None
        self.logger.info("ğŸ”„ [é˜¶æ®µ2/4] å†å²æ•°æ®å¤„ç† - å®Œæˆ")
        
        evaluation_dates = self.prepare_evaluation_dates(self.backtest_date)
        
        if not evaluation_dates:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ—¥æœŸ")
            return None
        
        # å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®
        batch_recent_data, valid_dates, stock_codes = self.prepare_batch_evaluation_data(evaluation_dates)
        
        if batch_recent_data is None:
            self.logger.error("æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å¤±è´¥")
            return None
        
        # ç›‘æ§æ•°æ®å‡†å¤‡åçš„GPUæ˜¾å­˜
        self.monitor_gpu_memory("æ•°æ®å‡†å¤‡å®Œæˆ")
        
        # ğŸ’¾ åŸºäºå®é™…å†å²æœŸé—´æ•°æ®é‡è¿›è¡ŒGPUå†…å­˜é¢„ä¼°
        self.logger.info("ğŸ’¾ åŸºäºå®é™…æ•°æ®é‡è¿›è¡ŒGPUå†…å­˜é¢„ä¼°...")
        estimation_result = self.estimate_memory_requirement(
            evaluation_days=self.evaluation_days,
            num_historical_periods=len(historical_periods_data),
            window_size=self.window_size
        )
        estimated_memory = estimation_result['total_estimated_gb']
        self.logger.info(f"ğŸ“Š å®é™…å†å²æœŸé—´æ•°æ®é‡: {len(historical_periods_data):,}")
        self.logger.info(f"ğŸ’¾ é¢„ä¼°GPUå†…å­˜ä½¿ç”¨é‡: {estimated_memory:.2f} GB (åŸºäºå®é™…{len(historical_periods_data):,}ä¸ªå†å²æœŸé—´)")
        self.logger.info("=" * 60)
        
        # ğŸ”„ æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ‰¹å¤„ç†
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šæŒ‰è®¡ç®—å•å…ƒï¼ˆè‚¡ç¥¨æ•° Ã— è¯„æµ‹æ—¥æœŸæ•°ï¼‰åˆ†æ‰¹
            total_computation_units = len(self.stock_codes) * len(valid_dates)
            total_batches = (total_computation_units + self.evaluation_batch_size - 1) // self.evaluation_batch_size
            
            self.logger.info(f"ğŸ“Š æ€»è®¡ç®—å•å…ƒ: {total_computation_units} ({len(self.stock_codes)} åªè‚¡ç¥¨ Ã— {len(valid_dates)} ä¸ªè¯„æµ‹æ—¥æœŸ)")
            self.logger.info(f"ğŸ“¦ æ¯æ‰¹å¤„ç†æœ€å¤§è®¡ç®—å•å…ƒæ•°: {self.evaluation_batch_size}")
            
            if total_batches > 1:
                self.logger.info(f"ğŸ”„ å¤šè‚¡ç¥¨åˆ†æ‰¹å¤„ç†ç­–ç•¥: å°† {total_computation_units} ä¸ªè®¡ç®—å•å…ƒåˆ†æˆ {total_batches} æ‰¹å¤„ç†")
                computation_units_per_batch = min(self.evaluation_batch_size, total_computation_units)
                memory_save_percent = ((total_computation_units - computation_units_per_batch) / total_computation_units) * 100
                self.logger.info(f"ğŸ’¾ é¢„è®¡GPUå†…å­˜èŠ‚çœ: {memory_save_percent:.1f}%")
                return self._process_evaluation_batches(valid_dates, batch_recent_data, historical_periods_data)
            else:
                self.logger.info(f"ğŸ”„ å¤šè‚¡ç¥¨å•æ‰¹å¤„ç†æ¨¡å¼: {total_computation_units} ä¸ªè®¡ç®—å•å…ƒä¸€æ¬¡æ€§å¤„ç†")
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
            total_batches = (len(valid_dates) + self.evaluation_batch_size - 1) // self.evaluation_batch_size
            if total_batches > 1:
                self.logger.info(f"ğŸ”„ å•è‚¡ç¥¨åˆ†æ‰¹å¤„ç†ç­–ç•¥: å°† {len(valid_dates)} ä¸ªè¯„æµ‹æ—¥æœŸåˆ†æˆ {total_batches} æ‰¹å¤„ç†")
                self.logger.info(f"ğŸ“¦ æ¯æ‰¹å¤„ç†: æœ€å¤š {self.evaluation_batch_size} ä¸ªè¯„æµ‹æ—¥æœŸ")
                memory_save_percent = ((len(valid_dates) - self.evaluation_batch_size) / len(valid_dates)) * 100
                self.logger.info(f"ğŸ’¾ é¢„è®¡GPUå†…å­˜èŠ‚çœ: {memory_save_percent:.1f}%")
                return self._process_evaluation_batches(valid_dates, batch_recent_data, historical_periods_data)
            else:
                self.logger.info("ğŸ”„ å•æ‰¹å¤„ç†æ¨¡å¼: æ‰€æœ‰è¯„æµ‹æ—¥æœŸä¸€æ¬¡æ€§å¤„ç†")
        
        # ğŸš€ ç¬¬3é˜¶æ®µï¼šGPUè®¡ç®—ä¸ç»“æœå¤„ç† - å¼€å§‹
        self.logger.info("ğŸš€ [é˜¶æ®µ3/4] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å¼€å§‹")
        self.monitor_gpu_memory("GPUè®¡ç®—å¼€å§‹")
        batch_correlations = self.calculate_batch_gpu_correlation_optimized(batch_recent_data, historical_periods_data, valid_dates)
        self.monitor_gpu_memory("GPUè®¡ç®—å®Œæˆ")
        self.logger.info("ğŸš€ [é˜¶æ®µ3/4] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å®Œæˆ")
        
        if not batch_correlations:
            self.logger.error("æ‰¹é‡ç›¸å…³æ€§è®¡ç®—å¤±è´¥")
            return None
        
        # ğŸ“Š ç¬¬4é˜¶æ®µï¼šæœ€ç»ˆå¤„ç† - å¼€å§‹
        self.logger.info("ğŸ“Š [é˜¶æ®µ4/4] æœ€ç»ˆå¤„ç† - å¼€å§‹")
        
        # ç›´æ¥ä½¿ç”¨é˜¶æ®µ4-5çš„æ•´åˆç»“æœï¼ˆå·²åŒ…å«ä¿å­˜å’Œæœ€ç»ˆç»“æœæ„å»ºï¼‰
        final_result = batch_correlations
        
        self.end_timer('total_batch_analysis')
        
        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        self._log_performance_summary()
        
        # æœ€ç»ˆGPUæ˜¾å­˜ç›‘æ§
        self.monitor_gpu_memory("åˆ†æå®Œæˆ")
        self.logger.info("ğŸ“Š [é˜¶æ®µ4/4] æœ€ç»ˆå¤„ç† - å®Œæˆ")
        
        # è¾“å‡ºåˆ†ææ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("æ‰¹é‡åˆ†æç»“æœæ€»ç»“:")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {final_result['batch_results']['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {final_result['batch_results']['summary']['avg_high_correlations_per_day']:.2f}")
        self.logger.info(f"æœ€å¤§æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {final_result['batch_results']['summary']['max_high_correlations_per_day']}")
        if final_result['batch_results']['summary']['overall_avg_correlation'] > 0:
            self.logger.info(f"æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°: {final_result['batch_results']['summary']['overall_avg_correlation']:.4f}")
        
        # æŸ¥æ‰¾å¹¶æ‰“å°ç›¸å…³ç³»æ•°æœ€å¤§çš„æ¡ç›®
        max_correlation = 0
        max_correlation_item = None
        max_eval_date = None
        max_stock_code = None
        
        detailed_results = final_result['batch_results']['detailed_results']
        
        if final_result.get('is_multi_stock', False):
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šéå†æ¯ä¸ªè‚¡ç¥¨çš„ç»“æœ
            for stock_code, stock_results in detailed_results.items():
                for result in stock_results:
                    for period in result['high_correlation_periods']:
                        if period['avg_correlation'] > max_correlation:
                            max_correlation = period['avg_correlation']
                            max_correlation_item = period
                            max_eval_date = result['evaluation_date']
                            max_stock_code = stock_code
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            for result in detailed_results:
                for period in result['high_correlation_periods']:
                    if period['avg_correlation'] > max_correlation:
                        max_correlation = period['avg_correlation']
                        max_correlation_item = period
                        max_eval_date = result['evaluation_date']
        
        if max_correlation_item:
            self.logger.info("=" * 40)
            self.logger.info("ç›¸å…³ç³»æ•°æœ€å¤§çš„æ¡ç›®:")
            if max_stock_code:
                self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {max_stock_code}")
            self.logger.info(f"è¯„æµ‹æ—¥æœŸ: {max_eval_date.strftime('%Y-%m-%d')}")
            self.logger.info(f"å†å²æœŸé—´: {max_correlation_item['start_date'].strftime('%Y-%m-%d')} åˆ° {max_correlation_item['end_date'].strftime('%Y-%m-%d')}")
            self.logger.info(f"ç›¸å…³ç³»æ•°: {max_correlation_item['avg_correlation']:.6f}")
            self.logger.info(f"æ¥æºè‚¡ç¥¨: {max_correlation_item['stock_code']}")
            self.logger.info(f"æ•°æ®æ¥æº: {max_correlation_item['source']}")
            self.logger.info("=" * 40)
        
        self.logger.info("=" * 80)
        
        return final_result
    
    def _collect_historical_periods_data(self):
        """æ”¶é›†å†å²æœŸé—´æ•°æ®ï¼ˆåˆå¹¶äº†å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½é€»è¾‘ï¼‰"""
        self.start_timer('historical_data_collection')
        
        historical_periods_data = []
        
        # æ£€æŸ¥self_onlyæ¨¡å¼çš„ç‰¹æ®Šæƒ…å†µ
        if self.comparison_mode == 'self_only':
            self.logger.info("ğŸ“ˆ ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”æ¨¡å¼")
            # åœ¨self_onlyæ¨¡å¼ä¸‹ï¼Œæ”¶é›†ç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
            self_historical_data = self._collect_self_historical_data()
            historical_periods_data.extend(self_historical_data)
            self.logger.info(f"æ”¶é›†åˆ° {len(historical_periods_data)} ä¸ªå†å²æœŸé—´æ•°æ®")
            self.end_timer('historical_data_collection')
            return historical_periods_data
        
        # å¯¹æ¯”è‚¡ç¥¨æ•°æ®å·²ç»åœ¨load_dataä¸­åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½
        
        # æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®
        # æ ¹æ®è‚¡ç¥¨æ•°é‡å†³å®šæ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        if len(self.loaded_stocks_data) >= 10 and self.num_processes > 1:
            comparison_historical_data = self._collect_comparison_historical_data_multiprocess()
        else:
            comparison_historical_data = self._collect_comparison_historical_data()
        historical_periods_data.extend(comparison_historical_data)
        
        self.logger.info(f"æ”¶é›†åˆ° {len(historical_periods_data)} ä¸ªå†å²æœŸé—´æ•°æ®")
        self.end_timer('historical_data_collection')
        return historical_periods_data
    


    
    def _collect_comparison_historical_data(self):
        """æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå·²ä¼˜åŒ–ï¼šç›´æ¥ç­›é€‰å’Œé¢„å¤„ç†ï¼‰"""
        historical_data = []
        total_valid_periods = 0
        total_invalid_periods = 0
        processed_stocks = 0
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        for stock_code, stock_data in self.loaded_stocks_data.items():
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸è¿›è¡Œæ—¥æœŸæˆªæ–­
            available_data = stock_data
            
            if len(available_data) < self.window_size:
                if self.debug:
                    self.logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®é•¿åº¦ {len(available_data)} å°äºçª—å£å¤§å° {self.window_size}ï¼Œè·³è¿‡")
                continue
            
            stock_valid_periods = 0
            stock_invalid_periods = 0
            
            # ç”Ÿæˆè¯¥è‚¡ç¥¨çš„å†å²æœŸé—´å¹¶ç›´æ¥è¿›è¡Œç­›é€‰å’Œé¢„å¤„ç†
            for i in range(len(available_data) - self.window_size + 1):
                period_data = available_data.iloc[i:i + self.window_size]
                
                # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
                if len(period_data) == self.window_size:
                    start_date = period_data.index[0]
                    end_date = period_data.index[-1]
                    
                    # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                    historical_values = period_data[fields].values
                    
                    # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                    historical_data.append((historical_values, start_date, end_date, stock_code))
                    stock_valid_periods += 1
                    total_valid_periods += 1
                else:
                    stock_invalid_periods += 1
                    total_invalid_periods += 1
            
            processed_stocks += 1
            
            # æ¯å¤„ç†100åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            if processed_stocks % 100 == 0:
                self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨æ•°æ®æ”¶é›†è¿›åº¦: {processed_stocks}/{len(self.loaded_stocks_data)} åªè‚¡ç¥¨")
        
        self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®æ”¶é›†å®Œæˆ: å¤„ç†è‚¡ç¥¨={processed_stocks}, æœ‰æ•ˆæœŸé—´={total_valid_periods}, æ— æ•ˆæœŸé—´={total_invalid_periods}")
        return historical_data
    
    def _collect_comparison_historical_data_multiprocess(self):
        """æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
        if not self.loaded_stocks_data:
            return []
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # å‡†å¤‡å¤šè¿›ç¨‹ä»»åŠ¡å‚æ•°
        tasks = []
        for stock_code, stock_data in self.loaded_stocks_data.items():
            tasks.append((stock_code, stock_data, self.window_size, fields, self.debug))
        
        self.logger.info(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†: {len(tasks)} åªè‚¡ç¥¨ï¼Œ{self.num_processes} ä¸ªè¿›ç¨‹")
        
        historical_data = []
        total_valid_periods = 0
        total_invalid_periods = 0
        processed_stocks = 0
        
        try:
            # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†ä»»åŠ¡
            with mp.Pool(processes=self.num_processes) as pool:
                # åˆ†æ‰¹å¤„ç†ä»¥æ˜¾ç¤ºè¿›åº¦
                batch_size = max(1, len(tasks) // 10)  # åˆ†æˆ10æ‰¹æ˜¾ç¤ºè¿›åº¦
                
                for i in range(0, len(tasks), batch_size):
                    batch_tasks = tasks[i:i + batch_size]
                    batch_results = pool.map(_process_stock_historical_data_worker, batch_tasks)
                    
                    # å¤„ç†æ‰¹æ¬¡ç»“æœ
                    for stock_code, stock_historical_data, stats in batch_results:
                        if 'error' in stats:
                            if self.debug:
                                self.logger.warning(f"è‚¡ç¥¨ {stock_code} å¤„ç†å‡ºé”™: {stats['error']}")
                            continue
                        
                        if stats.get('skipped', False):
                            if self.debug:
                                self.logger.debug(f"è‚¡ç¥¨ {stock_code} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                            continue
                        
                        # æ·»åŠ åˆ°æ€»ç»“æœä¸­
                        historical_data.extend(stock_historical_data)
                        total_valid_periods += stats['valid_periods']
                        total_invalid_periods += stats['invalid_periods']
                        processed_stocks += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = min(i + batch_size, len(tasks))
                    self.logger.info(f"ğŸ“Š å¤šè¿›ç¨‹å¤„ç†è¿›åº¦: {progress}/{len(tasks)} åªè‚¡ç¥¨ ({progress/len(tasks)*100:.1f}%)")
        
        except Exception as e:
            self.logger.error(f"å¤šè¿›ç¨‹å¤„ç†å‡ºé”™ï¼Œå›é€€åˆ°å•è¿›ç¨‹æ¨¡å¼: {str(e)}")
            return self._collect_comparison_historical_data(earliest_eval_date)
        
        self.logger.info(f"âœ… å¤šè¿›ç¨‹å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®æ”¶é›†å®Œæˆ: å¤„ç†è‚¡ç¥¨={processed_stocks}, æœ‰æ•ˆæœŸé—´={total_valid_periods}, æ— æ•ˆæœŸé—´={total_invalid_periods}")
        return historical_data
    
    def _collect_self_historical_data(self):
        """æ”¶é›†ç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®ï¼ˆç”¨äºself_onlyæ¨¡å¼ï¼‰"""
        historical_data = []
        
        if self.data is None or self.data.empty:
            self.logger.warning(f"ç›®æ ‡è‚¡ç¥¨ {self.stock_code} æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ”¶é›†å†å²æ•°æ®")
            return historical_data
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # ä½¿ç”¨ç›®æ ‡è‚¡ç¥¨çš„æ‰€æœ‰å¯ç”¨æ•°æ®
        available_data = self.data
        
        if len(available_data) < self.window_size:
            self.logger.warning(f"ç›®æ ‡è‚¡ç¥¨ {self.stock_code} æ•°æ®é•¿åº¦ {len(available_data)} å°äºçª—å£å¤§å° {self.window_size}")
            return historical_data
        
        valid_periods = 0
        invalid_periods = 0
        
        # ç”Ÿæˆç›®æ ‡è‚¡ç¥¨çš„å†å²æœŸé—´æ•°æ®
        for i in range(len(available_data) - self.window_size + 1):
            period_data = available_data.iloc[i:i + self.window_size]
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ­£ç¡®
            if len(period_data) == self.window_size:
                start_date = period_data.index[0]
                end_date = period_data.index[-1]
                
                # ç›´æ¥æå–å¹¶é¢„å¤„ç†æ•°æ®
                historical_values = period_data[fields].values
                
                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
                historical_data.append((historical_values, start_date, end_date, self.stock_code))
                valid_periods += 1
            else:
                invalid_periods += 1
        
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨ {self.stock_code} å†å²æ•°æ®æ”¶é›†å®Œæˆ: æœ‰æ•ˆæœŸé—´={valid_periods}, æ— æ•ˆæœŸé—´={invalid_periods}")
        return historical_data
    

    

    

    

    

    
    def monitor_gpu_memory(self, stage_name):
        """ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == 'cuda':
            # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            current_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            current_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            
            # æ›´æ–°å³°å€¼è®°å½•
            self.gpu_memory_stats['peak_allocated'] = max(
                self.gpu_memory_stats['peak_allocated'], current_allocated
            )
            self.gpu_memory_stats['peak_reserved'] = max(
                self.gpu_memory_stats['peak_reserved'], current_reserved
            )
            
            # æ›´æ–°å½“å‰å€¼
            self.gpu_memory_stats['current_allocated'] = current_allocated
            self.gpu_memory_stats['current_reserved'] = current_reserved
            
            # è®°å½•æ—¥å¿—
            self.logger.info(f"ğŸ” GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]:")
            self.logger.info(f"   å½“å‰å·²åˆ†é…: {current_allocated:.2f}GB")
            self.logger.info(f"   å½“å‰å·²ä¿ç•™: {current_reserved:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ç‡
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            usage_rate = current_allocated / total_memory
            
            if usage_rate > 0.8:
                self.logger.warning(f"âš ï¸ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {usage_rate*100:.1f}%")
            elif usage_rate > 0.9:
                self.logger.error(f"âŒ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_rate*100:.1f}%ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º")
        else:
            self.logger.info(f"ğŸ” CPUæ¨¡å¼ï¼Œè·³è¿‡GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]")
    
    def estimate_memory_requirement(self, evaluation_days, num_historical_periods, window_size, num_fields=5):
        """
        ç²¾ç¡®ä¼°ç®—GPUæ˜¾å­˜éœ€æ±‚ï¼ˆGBï¼‰
        åŸºäºå®é™…å†…å­˜ä½¿ç”¨æ¨¡å¼å’ŒPyTorchå†…å­˜æ± æœºåˆ¶
        æ”¯æŒå¤šè‚¡ç¥¨æ¨¡å¼çš„å†…å­˜ä¼°ç®—
        
        Args:
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            num_historical_periods: å†å²æœŸé—´æ•°é‡
            window_size: çª—å£å¤§å°
            num_fields: å­—æ®µæ•°é‡ï¼ˆé»˜è®¤5ï¼šå¼€é«˜ä½æ”¶é‡ï¼‰
            
        Returns:
            dict: åŒ…å«è¯¦ç»†å†…å­˜ä¼°ç®—çš„å­—å…¸
        """
        bytes_per_float32 = 4
        
        # è·å–è‚¡ç¥¨æ•°é‡ï¼ˆå¤šè‚¡ç¥¨æ¨¡å¼ä¸‹éœ€è¦è€ƒè™‘ï¼‰
        num_stocks = len(self.stock_codes) if self.is_multi_stock else 1
        
        # 1. åŸºç¡€æ•°æ®å¼ é‡
        # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, window_size, num_fields]
        # å•è‚¡ç¥¨æ¨¡å¼: [evaluation_days, window_size, num_fields]
        batch_recent_data_bytes = num_stocks * evaluation_days * window_size * num_fields * bytes_per_float32
        
        # å†å²æ•°æ®å¼ é‡: [num_historical_periods, window_size, num_fields]
        historical_tensor_bytes = num_historical_periods * window_size * num_fields * bytes_per_float32
        
        # 2. ç›¸å…³ç³»æ•°è®¡ç®—ä¸­é—´å¼ é‡ï¼ˆè¿™æ˜¯å†…å­˜å³°å€¼çš„ä¸»è¦æ¥æºï¼‰
        # åœ¨_compute_correlation_matrixä¸­çš„å¹¿æ’­è®¡ç®—
        
        # å¤šè‚¡ç¥¨æ¨¡å¼ä¸‹çš„å¼ é‡å½¢çŠ¶ï¼š
        # recent_expanded: [num_stocks, batch_size, 1, window_size, num_fields]
        # historical_expanded: [1, num_historical_periods, window_size, num_fields]
        # å¹¿æ’­åçš„å®é™…å†…å­˜å ç”¨: [num_stocks, batch_size, num_historical_periods, window_size, num_fields]
        
        # ä½¿ç”¨å®é™…çš„GPUåˆ†ç»„æ‰¹å¤„ç†å¤§å°ï¼Œè€Œä¸æ˜¯self.batch_size
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šæ‰¹å¤„ç†å¤§å°åŸºäºè®¡ç®—å•å…ƒæ•°é‡
            total_computation_units = num_stocks * evaluation_days
            batch_size = min(self.evaluation_batch_size, total_computation_units)
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šæ‰¹å¤„ç†å¤§å°åŸºäºè¯„æµ‹æ—¥æœŸæ•°é‡
            batch_size = min(self.evaluation_batch_size, evaluation_days)
        
        # å¹¿æ’­å¼ é‡ï¼ˆæœ€å¤§å†…å­˜æ¶ˆè€—ç‚¹ï¼‰- è€ƒè™‘å¤šè‚¡ç¥¨æ¨¡å¼
        broadcast_tensor_bytes = num_stocks * batch_size * num_historical_periods * window_size * num_fields * bytes_per_float32
        
        # ä¸­å¿ƒåŒ–å¼ é‡ï¼ˆ2ä¸ªï¼‰
        centered_tensors_bytes = 2 * broadcast_tensor_bytes
        
        # åæ–¹å·®ã€æ ‡å‡†å·®ã€ç›¸å…³ç³»æ•°å¼ é‡ - è€ƒè™‘å¤šè‚¡ç¥¨æ¨¡å¼
        covariance_bytes = num_stocks * batch_size * num_historical_periods * num_fields * bytes_per_float32
        std_tensors_bytes = 2 * num_stocks * batch_size * num_historical_periods * num_fields * bytes_per_float32
        correlation_bytes = num_stocks * batch_size * num_historical_periods * num_fields * bytes_per_float32
        
        # 3. GPUç«¯ç»“æœå­˜å‚¨å¼ é‡
        # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, num_historical_periods]
        # å•è‚¡ç¥¨æ¨¡å¼: [evaluation_days, num_historical_periods]
        avg_correlations_bytes = num_stocks * evaluation_days * num_historical_periods * bytes_per_float32
        
        # é«˜ç›¸å…³æ©ç : [num_stocks, evaluation_days, num_historical_periods] (bool = 1 byte)
        high_corr_mask_bytes = num_stocks * evaluation_days * num_historical_periods * 1
        
        # 4. å…³é”®ä¿®æ­£ï¼šGPUè®¡ç®—è¿‡ç¨‹ä¸­çš„çœŸå®å†…å­˜å³°å€¼
        # åœ¨_compute_correlation_matrixä¸­ï¼Œå¹¿æ’­æ“ä½œä¼šåˆ›å»ºå·¨å¤§çš„ä¸­é—´å¼ é‡ï¼š
        # - recent_expanded.unsqueeze(1): [batch_size, 1, window_size, 5]
        # - historical_expanded.unsqueeze(0): [1, num_historical_periods, window_size, 5]  
        # - å¹¿æ’­è®¡ç®—æ—¶ï¼ŒPyTorchä¼šåˆ›å»ºå®Œæ•´çš„ [batch_size, num_historical_periods, window_size, 5] å¼ é‡
        
        # çœŸå®çš„å¹¿æ’­å†…å­˜æ¶ˆè€—ï¼ˆè¿™æ˜¯è¢«ä¸¥é‡ä½ä¼°çš„éƒ¨åˆ†ï¼‰- è€ƒè™‘å¤šè‚¡ç¥¨æ¨¡å¼
        full_broadcast_tensor_bytes = num_stocks * batch_size * num_historical_periods * window_size * num_fields * bytes_per_float32
        
        # GPUè®¡ç®—å³°å€¼æ—¶åŒæ—¶å­˜åœ¨çš„å¼ é‡ï¼š
        # 1. åŸå§‹æ•°æ®
        # 2. recent_expanded (å¹¿æ’­åçš„å®Œæ•´å¤§å°)
        # 3. historical_expanded (å¹¿æ’­åçš„å®Œæ•´å¤§å°)
        # 4. recent_centered (å®Œæ•´å¤§å°)
        # 5. historical_centered (å®Œæ•´å¤§å°)
        # 6. å„ç§ä¸­é—´è®¡ç®—ç»“æœ
        
        # å®é™…å†…å­˜å³°å€¼ä¸»è¦æ¥æºï¼š
        # 1. å†å²æ•°æ®å¼ é‡ï¼ˆæŒç»­å­˜åœ¨ï¼‰
        # 2. å¹¿æ’­è®¡ç®—æ—¶çš„ä¸´æ—¶å¼ é‡ï¼ˆå³°å€¼æ—¶åˆ»ï¼‰
        # 3. å°‘é‡ä¸­é—´ç»“æœå¼ é‡
        peak_allocated_bytes = (
            historical_tensor_bytes +           # åŸå§‹å†å²æ•°æ®ï¼ˆæŒç»­å­˜åœ¨ï¼‰
            full_broadcast_tensor_bytes +       # ä¸»è¦çš„å¹¿æ’­å¼ é‡å³°å€¼
            covariance_bytes +                  # åæ–¹å·®å¼ é‡
            std_tensors_bytes +                 # æ ‡å‡†å·®å¼ é‡
            correlation_bytes                   # ç›¸å…³ç³»æ•°å¼ é‡
        )
        
        # 5. PyTorchå†…å­˜æ± é¢„ç•™ï¼ˆåŸºäºå®é™…è§‚å¯Ÿä¿®æ­£ï¼‰
        # å®é™…è§‚å¯Ÿï¼šé¢„ä¼°åˆ†é…29.2GBï¼Œå®é™…å³°å€¼27GBï¼Œçº¦0.9å€
        # è¯´æ˜æˆ‘ä»¬çš„åŸºç¡€è®¡ç®—ç•¥æœ‰è¿‡åº¦ä¼°ç®—ï¼ŒPyTorchå®é™…ä½¿ç”¨æ›´é«˜æ•ˆ
        pytorch_memory_pool_multiplier = 0.9  # åŸºäºå®é™…è§‚å¯Ÿçš„ç²¾ç¡®è°ƒæ•´
        
        peak_allocated_gb = peak_allocated_bytes / (1024**3)
        estimated_reserved_gb = peak_allocated_gb * pytorch_memory_pool_multiplier
        
        # 6. æ„å»ºè¯¦ç»†ä¼°ç®—ç»“æœ
        estimation_details = {
            'evaluation_days': evaluation_days,
            'num_historical_periods': num_historical_periods,
            'window_size': window_size,
            'batch_size': batch_size,
            
            # åŸºç¡€å¼ é‡å¤§å°ï¼ˆGBï¼‰
            'batch_recent_data_gb': batch_recent_data_bytes / (1024**3),
            'historical_tensor_gb': historical_tensor_bytes / (1024**3),
            'broadcast_tensor_gb': broadcast_tensor_bytes / (1024**3),
            'intermediate_tensors_gb': (centered_tensors_bytes + covariance_bytes + std_tensors_bytes + correlation_bytes) / (1024**3),
            
            # å†…å­˜å³°å€¼ä¼°ç®—
            'peak_allocated_gb': peak_allocated_gb,
            'estimated_reserved_gb': estimated_reserved_gb,
            'total_estimated_gb': estimated_reserved_gb,  # ä¸»è¦å…³æ³¨ä¿ç•™å†…å­˜
            
            # å†…å­˜æ± ä¿¡æ¯
            'pytorch_pool_multiplier': pytorch_memory_pool_multiplier,
            
            # å…³é”®è®¡ç®—å‚æ•°
            'critical_tensor_size': f"[{batch_size}, {num_historical_periods}, {window_size}, {num_fields}]",
            'critical_tensor_gb': broadcast_tensor_bytes / (1024**3)
        }
        
        # è®°å½•è¯¦ç»†çš„å†…å­˜ä¼°ç®—æ—¥å¿—
        self.logger.info(f"ğŸ§® GPUå†…å­˜éœ€æ±‚ç²¾ç¡®ä¼°ç®—:")
        self.logger.info(f"   ğŸ“Š è¾“å…¥å‚æ•°:")
        if self.is_multi_stock:
            self.logger.info(f"      è‚¡ç¥¨æ•°é‡: {num_stocks}")
            self.logger.info(f"      è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days} (æ¯è‚¡ç¥¨)")
            self.logger.info(f"      æ€»è®¡ç®—å•å…ƒ: {num_stocks * evaluation_days}")
        else:
            self.logger.info(f"      è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"      å†å²æœŸé—´æ•°: {num_historical_periods:,}")
        self.logger.info(f"      çª—å£å¤§å°: {window_size}")
        self.logger.info(f"      æ‰¹å¤„ç†å¤§å°: {batch_size}")
        self.logger.info(f"   ğŸ“¦ å…³é”®å¼ é‡å¤§å°:")
        self.logger.info(f"      æ‰¹é‡è¯„æµ‹æ•°æ®: {estimation_details['batch_recent_data_gb']:.3f}GB")
        self.logger.info(f"      å†å²æ•°æ®å¼ é‡: {estimation_details['historical_tensor_gb']:.3f}GB")
        self.logger.info(f"      å…³é”®å¹¿æ’­å¼ é‡: {estimation_details['critical_tensor_gb']:.3f}GB {estimation_details['critical_tensor_size']}")
        self.logger.info(f"      ä¸­é—´è®¡ç®—å¼ é‡: {estimation_details['intermediate_tensors_gb']:.3f}GB")
        self.logger.info(f"   ğŸ’¾ å†…å­˜å³°å€¼é¢„ä¼°:")
        self.logger.info(f"      é¢„ä¼°åˆ†é…å³°å€¼: {peak_allocated_gb:.2f}GB")
        self.logger.info(f"      é¢„ä¼°ä¿ç•™å³°å€¼: {estimated_reserved_gb:.2f}GB (PyTorchå†…å­˜æ±  x{pytorch_memory_pool_multiplier:.1f})")
        self.logger.info(f"   ğŸ¯ æ€»å†…å­˜éœ€æ±‚é¢„ä¼°: {estimated_reserved_gb:.2f}GB")
        
        return estimation_details
    
    def check_gpu_memory_limit(self, required_memory_gb):
        """æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if self.device.type != 'cuda':
            return True  # CPUæ¨¡å¼ä¸å—æ˜¾å­˜é™åˆ¶
        
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        available_memory = total_memory * self.gpu_memory_limit
        
        self.logger.info(f"ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
        self.logger.info(f"   æ€»æ˜¾å­˜: {total_memory:.2f}GB")
        self.logger.info(f"   å¯ç”¨æ˜¾å­˜: {available_memory:.2f}GB (é™åˆ¶: {self.gpu_memory_limit*100:.0f}%)")
        self.logger.info(f"   éœ€æ±‚æ˜¾å­˜: {required_memory_gb:.2f}GB")
        
        if required_memory_gb <= available_memory:
            self.logger.info(f"âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ç›´æ¥å¤„ç†")
            return True
        else:
            self.logger.warning(f"âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†")
            return False
    
    def _get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for timer_name, timer_records in self.performance_timers.items():
            if timer_records:
                # å¤„ç†æ–°çš„æ•°æ®ç»“æ„
                if isinstance(timer_records[0], dict):
                    elapsed_times = [record['elapsed_time'] for record in timer_records]
                    stats[timer_name] = {
                        'total_time': sum(elapsed_times),
                        'avg_time': sum(elapsed_times) / len(elapsed_times),
                        'max_time': max(elapsed_times),
                        'min_time': min(elapsed_times),
                        'count': len(elapsed_times),
                        'parent': timer_records[0]['parent'],
                        'timestamp': timer_records[0]['timestamp']
                    }
                else:
                    # å…¼å®¹æ—§çš„æ•°æ®ç»“æ„
                    stats[timer_name] = {
                        'total_time': sum(timer_records),
                        'avg_time': sum(timer_records) / len(timer_records),
                        'max_time': max(timer_records),
                        'min_time': min(timer_records),
                        'count': len(timer_records),
                        'parent': None,
                        'timestamp': time.time()
                    }
        
        # æ·»åŠ GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            stats['gpu_memory'] = self.gpu_memory_stats.copy()
        
        return stats
    
    def _process_evaluation_batches(self, valid_dates, batch_recent_data, historical_periods_data):
        """
        åˆ†æ‰¹å¤„ç†è¯„æµ‹æ—¥æœŸï¼Œé¿å…GPUå†…å­˜æº¢å‡º
        
        Args:
            valid_dates: æœ‰æ•ˆçš„è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            batch_recent_data: æ‰¹é‡æœ€è¿‘æ•°æ®
            historical_periods_data: å†å²æœŸé—´æ•°æ®
            
        Returns:
            dict: åˆå¹¶åçš„åˆ†æç»“æœ
        """
        self.logger.info("ğŸ”„ å¼€å§‹åˆ†æ‰¹å¤„ç†è¯„æµ‹æ—¥æœŸ...")
        
        # åˆå§‹åŒ–åˆå¹¶ç»“æœ
        merged_results = {
            'evaluation_days': len(valid_dates),
            'batch_results': {
                'detailed_results': [],
                'summary': {
                    'total_high_correlations': 0,
                    'avg_high_correlations_per_day': 0.0,
                    'max_high_correlations_per_day': 0,
                    'overall_avg_correlation': 0.0
                }
            }
        }
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡ï¼ˆè€ƒè™‘å¤šè‚¡ç¥¨æ¨¡å¼ï¼‰
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šæŒ‰è®¡ç®—å•å…ƒï¼ˆè‚¡ç¥¨æ•° Ã— è¯„æµ‹æ—¥æœŸæ•°ï¼‰åˆ†æ‰¹
            total_computation_units = len(self.stock_codes) * len(valid_dates)
            
            # ç›´æ¥æŒ‰ç…§è®¡ç®—å•å…ƒæ•°é‡åˆ†æ‰¹ï¼Œç¡®ä¿æ¯æ‰¹ä¸è¶…è¿‡evaluation_batch_sizeä¸ªè®¡ç®—å•å…ƒ
            total_batches = (total_computation_units + self.evaluation_batch_size - 1) // self.evaluation_batch_size
        else:
            # å•è‚¡ç¥¨æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
            total_batches = (len(valid_dates) + self.evaluation_batch_size - 1) // self.evaluation_batch_size
        
        # åˆ†æ‰¹å¤„ç†
        if self.is_multi_stock:
            # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šæŒ‰è®¡ç®—å•å…ƒåˆ†æ‰¹å¤„ç†
            # åˆ›å»ºæ‰€æœ‰è®¡ç®—å•å…ƒçš„åˆ—è¡¨ï¼š[(stock_idx, stock_code, date_idx, date)]
            all_computation_units = []
            for stock_idx, stock_code in enumerate(self.stock_codes):
                for date_idx, date in enumerate(valid_dates):
                    all_computation_units.append((stock_idx, stock_code, date_idx, date))
            
            # æŒ‰æ‰¹æ¬¡å¤„ç†è®¡ç®—å•å…ƒ
            for batch_idx in range(total_batches):
                start_unit = batch_idx * self.evaluation_batch_size
                end_unit = min(start_unit + self.evaluation_batch_size, total_computation_units)
                current_batch_units = end_unit - start_unit
                
                self.logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹: {current_batch_units} ä¸ªè®¡ç®—å•å…ƒ")
                
                # è·å–å½“å‰æ‰¹æ¬¡çš„è®¡ç®—å•å…ƒ
                batch_units = all_computation_units[start_unit:end_unit]
                
                # æŒ‰è‚¡ç¥¨åˆ†ç»„å½“å‰æ‰¹æ¬¡çš„è®¡ç®—å•å…ƒ
                stock_date_groups = {}
                for stock_idx, stock_code, date_idx, date in batch_units:
                    if stock_code not in stock_date_groups:
                        stock_date_groups[stock_code] = {'stock_idx': stock_idx, 'dates': []}
                    stock_date_groups[stock_code]['dates'].append((date_idx, date))
                
                # å‡†å¤‡æ‰¹æ¬¡æ•°æ®ï¼šæ”¶é›†æ‰€æœ‰è‚¡ç¥¨çš„ç›¸å…³æ—¥æœŸæ•°æ®
                batch_stock_indices = []
                batch_date_indices = []
                batch_dates_list = []
                
                for stock_code, group_info in stock_date_groups.items():
                    stock_idx = group_info['stock_idx']
                    batch_dates_info = group_info['dates']
                    
                    for date_idx, date in batch_dates_info:
                        batch_stock_indices.append(stock_idx)
                        batch_date_indices.append(date_idx)
                        batch_dates_list.append(date)
                
                # æå–æ‰¹æ¬¡æ•°æ®ï¼š[batch_size, window_size, 5]
                # batch_recent_data: [num_stocks, evaluation_days, window_size, 5]
                batch_data_list = []
                for stock_idx, date_idx in zip(batch_stock_indices, batch_date_indices):
                    batch_data_list.append(batch_recent_data[stock_idx, date_idx, :, :])
                
                # å †å æˆæ‰¹æ¬¡å¼ é‡
                batch_tensor = torch.stack(batch_data_list, dim=0)  # [batch_size, window_size, 5]
                
                # ç›‘æ§GPUå†…å­˜
                self.monitor_gpu_memory(f"æ‰¹æ¬¡ {batch_idx + 1} GPUè®¡ç®—å¼€å§‹")
                
                # ğŸš€ ä¸€æ¬¡æ€§GPUè®¡ç®—æ•´ä¸ªæ‰¹æ¬¡
                self.logger.info(f"ğŸš€ æ‰¹æ¬¡ {batch_idx + 1} GPUè®¡ç®— - å¼€å§‹")
                self.logger.info(f"ğŸ“¦ å¤„ç† {len(set(batch_stock_indices))} åªè‚¡ç¥¨ï¼Œ{current_batch_units} ä¸ªè®¡ç®—å•å…ƒ")
                
                # è¾“å‡ºè¯¦ç»†çš„è®¡ç®—å•å…ƒä¿¡æ¯
                self.logger.info("ğŸ“‹ è®¡ç®—å•å…ƒè¯¦ç»†ä¿¡æ¯:")
                for i, (stock_idx, date_idx, date) in enumerate(zip(batch_stock_indices, batch_date_indices, batch_dates_list)):
                    stock_code = self.stock_codes[stock_idx] if self.is_multi_stock else self.stock_code
                    self.logger.info(f"   å•å…ƒ {i+1}: è‚¡ç¥¨ä»£ç ={stock_code}, è¯„æµ‹æ—¥æœŸ={date}")
                
                # å¼€å§‹æ‰¹æ¬¡çº§åˆ«çš„GPUè®¡æ—¶
                self.start_timer('gpu_step1_data_preparation')
                self.start_timer('gpu_step2_tensor_creation') 
                self.start_timer('gpu_step3_integrated_correlation_processing')
                
                # è°ƒç”¨ä¸å¸¦è®¡æ—¶å™¨çš„GPUè®¡ç®—å‡½æ•°
                batch_correlations = self._calculate_batch_gpu_correlation_no_timer(
                    batch_tensor.unsqueeze(0), historical_periods_data, batch_dates_list
                )
                
                # ç»“æŸæ‰¹æ¬¡çº§åˆ«çš„GPUè®¡æ—¶
                self.end_timer('gpu_step3_integrated_correlation_processing')
                self.end_timer('gpu_step2_tensor_creation')
                self.end_timer('gpu_step1_data_preparation')
                
                self.monitor_gpu_memory(f"æ‰¹æ¬¡ {batch_idx + 1} GPUè®¡ç®—å®Œæˆ")
                self.logger.info(f"ğŸš€ æ‰¹æ¬¡ {batch_idx + 1} GPUè®¡ç®— - å®Œæˆ")
                
                # åˆå¹¶æ‰¹æ¬¡ç»“æœ
                if batch_correlations:
                    merged_results['batch_results']['detailed_results'].extend(
                        batch_correlations['batch_results']['detailed_results']
                    )
                    
                    # ç´¯åŠ ç»Ÿè®¡æ•°æ®
                    batch_summary = batch_correlations['batch_results']['summary']
                    merged_results['batch_results']['summary']['total_high_correlations'] += batch_summary['total_high_correlations']
                    merged_results['batch_results']['summary']['max_high_correlations_per_day'] = max(
                        merged_results['batch_results']['summary']['max_high_correlations_per_day'],
                        batch_summary['max_high_correlations_per_day']
                    )
                
                # æ¸…ç†GPUç¼“å­˜
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                    gc.collect()
                
                self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å®Œæˆï¼Œå·²å¤„ç† {current_batch_units} ä¸ªè®¡ç®—å•å…ƒ")
            
            self.logger.info(f"ğŸ”„ åˆ†æ‰¹å¤„ç†å®Œæˆï¼")
        else:
            # å•è‚¡ç¥¨æ¨¡å¼çš„åŸæœ‰é€»è¾‘
            for batch_idx in range(total_batches):
                # å•è‚¡ç¥¨æ¨¡å¼ï¼šæŒ‰æ—¥æœŸåˆ†æ‰¹
                start_idx = batch_idx * self.evaluation_batch_size
                end_idx = min(start_idx + self.evaluation_batch_size, len(valid_dates))
                
                batch_dates = valid_dates[start_idx:end_idx]
                batch_size = len(batch_dates)
                
                self.logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹: {batch_size} ä¸ªè¯„æµ‹æ—¥æœŸ")
                self.logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {batch_dates[0]} åˆ° {batch_dates[-1]}")
                
                # æå–å½“å‰æ‰¹æ¬¡çš„æ•°æ® (batch_recent_data æ˜¯ PyTorch å¼ é‡)
                # ä½¿ç”¨ä¼ å‚åˆ¤æ–­æ¨¡å¼ï¼Œä¸ä¾èµ–çŸ©é˜µå½¢çŠ¶
                if self.is_multi_stock:
                    # å¤šè‚¡ç¥¨æ¨¡å¼: [num_stocks, evaluation_days, window_size, 5]
                    batch_recent_subset = batch_recent_data[:, start_idx:end_idx]
                else:
                    # å•è‚¡ç¥¨æ¨¡å¼: å¯èƒ½æ˜¯ [1, evaluation_days, window_size, 5] æˆ– [evaluation_days, window_size, 5]
                    if len(batch_recent_data.shape) == 4:
                        # å·²è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ [1, evaluation_days, window_size, 5]
                        batch_recent_subset = batch_recent_data[:, start_idx:end_idx]
                    else:
                        # åŸå§‹å•è‚¡ç¥¨æ ¼å¼ [evaluation_days, window_size, 5]
                        batch_recent_subset = batch_recent_data[start_idx:end_idx]
                
                # ç›‘æ§GPUå†…å­˜
                self.monitor_gpu_memory(f"æ‰¹æ¬¡ {batch_idx + 1} å¼€å§‹")
                
                # ğŸš€ GPUè®¡ç®—å½“å‰æ‰¹æ¬¡
                self.logger.info(f"ğŸš€ [æ‰¹æ¬¡ {batch_idx + 1}] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å¼€å§‹")
                
                # è¾“å‡ºè¯¦ç»†çš„è®¡ç®—å•å…ƒä¿¡æ¯ï¼ˆå•è‚¡ç¥¨æ¨¡å¼ï¼‰
                self.logger.info("ğŸ“‹ è®¡ç®—å•å…ƒè¯¦ç»†ä¿¡æ¯:")
                for i, date in enumerate(batch_dates):
                    self.logger.info(f"   å•å…ƒ {i+1}: è‚¡ç¥¨ä»£ç ={self.stock_code}, è¯„æµ‹æ—¥æœŸ={date}")
                
                batch_correlations = self.calculate_batch_gpu_correlation_optimized(
                    batch_recent_subset, historical_periods_data, batch_dates
                )
                self.monitor_gpu_memory(f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")
                self.logger.info(f"ğŸš€ [æ‰¹æ¬¡ {batch_idx + 1}] GPUè®¡ç®—ä¸ç»“æœå¤„ç† - å®Œæˆ")
                
                if not batch_correlations:
                    self.logger.error(f"æ‰¹æ¬¡ {batch_idx + 1} è®¡ç®—å¤±è´¥")
                    continue
                
                # åˆå¹¶ç»“æœ
                merged_results['batch_results']['detailed_results'].extend(
                    batch_correlations['batch_results']['detailed_results']
                )
                
                # ç´¯åŠ ç»Ÿè®¡æ•°æ®
                batch_summary = batch_correlations['batch_results']['summary']
                merged_results['batch_results']['summary']['total_high_correlations'] += batch_summary['total_high_correlations']
                merged_results['batch_results']['summary']['max_high_correlations_per_day'] = max(
                    merged_results['batch_results']['summary']['max_high_correlations_per_day'],
                    batch_summary['max_high_correlations_per_day']
                )
                
                # æ¸…ç†GPUç¼“å­˜
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                    gc.collect()
                
                self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å®Œæˆï¼Œç´¯è®¡é«˜ç›¸å…³æ€§æœŸé—´: {merged_results['batch_results']['summary']['total_high_correlations']}")
        
        # è®¡ç®—æœ€ç»ˆå¹³å‡å€¼
        total_days = len(valid_dates)
        if total_days > 0:
            merged_results['batch_results']['summary']['avg_high_correlations_per_day'] = (
                merged_results['batch_results']['summary']['total_high_correlations'] / total_days
            )
        
        # è®¡ç®—æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°
        if merged_results['batch_results']['detailed_results']:
            all_correlations = []
            for result in merged_results['batch_results']['detailed_results']:
                if 'high_correlations' in result:
                    for corr_data in result['high_correlations']:
                        if 'correlation' in corr_data:
                            all_correlations.append(corr_data['correlation'])
            
            if all_correlations:
                merged_results['batch_results']['summary']['overall_avg_correlation'] = np.mean(all_correlations)
        
        self.logger.info("ğŸ”„ åˆ†æ‰¹å¤„ç†å®Œæˆï¼")
        if self.is_multi_stock:
            total_computation_units = len(self.stock_codes) * total_days
            self.logger.info(f"ğŸ“Š æ€»è®¡å¤„ç†: {total_computation_units} ä¸ªè®¡ç®—å•å…ƒ ({len(self.stock_codes)} åªè‚¡ç¥¨ Ã— {total_days} ä¸ªè¯„æµ‹æ—¥æœŸ)ï¼Œåˆ† {total_batches} æ‰¹")
        else:
            self.logger.info(f"ğŸ“Š æ€»è®¡å¤„ç†: {total_days} ä¸ªè¯„æµ‹æ—¥æœŸï¼Œåˆ† {total_batches} æ‰¹")
        self.logger.info(f"ğŸ“ˆ æ€»é«˜ç›¸å…³æ€§æœŸé—´: {merged_results['batch_results']['summary']['total_high_correlations']}")
        
        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡ï¼ˆåˆ†æ‰¹å¤„ç†æ¨¡å¼ï¼‰
        self._log_performance_summary()
        
        return merged_results
    
    def _merge_batch_correlations(self, batch_correlations_list):
        """
        åˆå¹¶å¤šä¸ªè‚¡ç¥¨çš„æ‰¹æ¬¡ç›¸å…³æ€§ç»“æœ
        
        Args:
            batch_correlations_list: å¤šä¸ªè‚¡ç¥¨çš„æ‰¹æ¬¡ç»“æœåˆ—è¡¨
            
        Returns:
            dict: åˆå¹¶åçš„æ‰¹æ¬¡ç»“æœ
        """
        if not batch_correlations_list:
            return None
        
        # åˆå§‹åŒ–åˆå¹¶ç»“æœ
        merged_result = {
            'evaluation_days': 0,
            'batch_results': {
                'detailed_results': [],
                'summary': {
                    'total_high_correlations': 0,
                    'avg_high_correlations_per_day': 0.0,
                    'max_high_correlations_per_day': 0,
                    'overall_avg_correlation': 0.0
                }
            }
        }
        
        # åˆå¹¶æ‰€æœ‰è¯¦ç»†ç»“æœ
        for batch_result in batch_correlations_list:
            if batch_result and 'batch_results' in batch_result:
                # åˆå¹¶è¯¦ç»†ç»“æœ
                merged_result['batch_results']['detailed_results'].extend(
                    batch_result['batch_results']['detailed_results']
                )
                
                # ç´¯åŠ ç»Ÿè®¡æ•°æ®
                batch_summary = batch_result['batch_results']['summary']
                merged_result['batch_results']['summary']['total_high_correlations'] += batch_summary['total_high_correlations']
                merged_result['batch_results']['summary']['max_high_correlations_per_day'] = max(
                    merged_result['batch_results']['summary']['max_high_correlations_per_day'],
                    batch_summary['max_high_correlations_per_day']
                )
                
                # ç´¯åŠ è¯„æµ‹æ—¥æœŸæ•°
                merged_result['evaluation_days'] += batch_result.get('evaluation_days', 0)
        
        # è®¡ç®—å¹³å‡å€¼
        if merged_result['evaluation_days'] > 0:
            merged_result['batch_results']['summary']['avg_high_correlations_per_day'] = (
                merged_result['batch_results']['summary']['total_high_correlations'] / merged_result['evaluation_days']
            )
        
        # è®¡ç®—æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°
        if merged_result['batch_results']['detailed_results']:
            all_correlations = []
            for result in merged_result['batch_results']['detailed_results']:
                if 'high_correlations' in result:
                    for corr_data in result['high_correlations']:
                        if 'correlation' in corr_data:
                            all_correlations.append(corr_data['correlation'])
            
            if all_correlations:
                merged_result['batch_results']['summary']['overall_avg_correlation'] = np.mean(all_correlations)
        
        return merged_result
    
    def _log_performance_summary(self):
        """è¾“å‡ºåˆ†å±‚æ€§èƒ½æ€»ç»“"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š åˆ†å±‚æ€§èƒ½ç»Ÿè®¡æ€»ç»“ (æŒ‰æ‰§è¡Œé¡ºåº)")
        self.logger.info("=" * 80)
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = self._get_performance_stats()
        
        # å®šä¹‰æ­¥éª¤æ˜ å°„å’Œæ˜¾ç¤ºé¡ºåº - æ–°çš„4é˜¶æ®µåˆ’åˆ†
        step_mapping = {
            # ç¬¬1é˜¶æ®µï¼šå¤šè¿›ç¨‹å†å²æ•°æ®å¤„ç†ï¼ˆå«å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½ï¼‰
            'historical_data_collection': ('1-1', 'å†å²æ•°æ®æ”¶é›†ï¼ˆå«å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½ï¼‰'),
            
            # ç¬¬2é˜¶æ®µï¼šåˆå§‹åŒ–ä¸æ•°æ®å‡†å¤‡
            'target_stock_loading': ('2-1', 'ç›®æ ‡è‚¡ç¥¨æ•°æ®åŠ è½½'),
            'evaluation_dates_preparation': ('2-2', 'è¯„æµ‹æ—¥æœŸå‡†å¤‡'),
            'batch_data_preparation': ('2-3', 'æ‰¹é‡æ•°æ®å‡†å¤‡'),
            
            # ç¬¬3é˜¶æ®µï¼šGPUè®¡ç®—ä¸ç»“æœå¤„ç†ï¼ˆåˆå¹¶åŸ4-6é˜¶æ®µï¼‰
            'gpu_step1_data_preparation': ('3-1', 'å†å²æ•°æ®å‡†å¤‡å’Œç­›é€‰'),
            'gpu_step2_tensor_creation': ('3-2', 'åˆ›å»ºGPUå†å²æ•°æ®å¼ é‡'),
            'gpu_step3_correlation_calculation': ('3-3', 'æ‰¹é‡ç›¸å…³ç³»æ•°è®¡ç®—'),
            'gpu_step3_integrated_correlation_processing': ('3-4', 'é›†æˆç›¸å…³æ€§å¤„ç†'),
            'gpu_step4_batch_merging': ('3-5', 'åˆå¹¶æ‰¹æ¬¡ç»“æœ'),
            'gpu_step5_result_processing': ('3-6', 'å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ'),
            'integrated_result_processing': ('3-7', 'é›†æˆç»“æœå¤„ç†'),
            
            # æ€»ä½“ç»Ÿè®¡
            'total_batch_analysis': ('æ€»è®¡', 'å®Œæ•´æ‰¹é‡åˆ†æ')
        }
        
        # æŒ‰æ­¥éª¤é¡ºåºæ˜¾ç¤º - æ–°çš„4é˜¶æ®µåˆ’åˆ†
        current_stage = 0
        stage_names = {
            1: "ğŸ“š ç¬¬1é˜¶æ®µï¼šå¤šè¿›ç¨‹å†å²æ•°æ®å¤„ç†",
            2: "ğŸ”„ ç¬¬2é˜¶æ®µï¼šåˆå§‹åŒ–ä¸æ•°æ®å‡†å¤‡",
            3: "ğŸš€ ç¬¬3é˜¶æ®µï¼šGPUè®¡ç®—ä¸ç»“æœå¤„ç†"
        }
        
        for timer_name, (step_id, step_name) in step_mapping.items():
            if timer_name in stats:
                stat = stats[timer_name]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºæ–°çš„é˜¶æ®µæ ‡é¢˜
                if step_id != 'æ€»è®¡':
                    stage_num = int(step_id.split('-')[0])
                    if stage_num != current_stage:
                        if current_stage > 0:
                            self.logger.info("")  # ç©ºè¡Œåˆ†éš”
                        self.logger.info(stage_names[stage_num])
                        current_stage = stage_num
                
                # æ˜¾ç¤ºæ­¥éª¤ç»Ÿè®¡
                if step_id == 'æ€»è®¡':
                    self.logger.info("")
                    self.logger.info("=" * 40)
                    self.logger.info(f"ğŸ“ˆ {step_id} - {step_name}:")
                else:
                    self.logger.info(f"  {step_id} {step_name}:")
                
                self.logger.info(f"      æ€»è€—æ—¶: {stat['total_time']:.3f}ç§’")
                self.logger.info(f"      å¹³å‡è€—æ—¶: {stat['avg_time']:.3f}ç§’") 
                self.logger.info(f"      æ‰§è¡Œæ¬¡æ•°: {stat['count']}")
                
                # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆç›¸å¯¹äºæ€»æ—¶é—´ï¼‰
                if 'total_batch_analysis' in stats:
                    total_time = stats['total_batch_analysis']['total_time']
                    percentage = (stat['total_time'] / total_time) * 100
                    self.logger.info(f"      å æ¯”: {percentage:.1f}%")
        
        # æ˜¾ç¤ºå…¶ä»–æœªæ˜ å°„çš„è®¡æ—¶å™¨
        unmapped_timers = set(stats.keys()) - set(step_mapping.keys()) - {'gpu_memory'}
        if unmapped_timers:
            self.logger.info("")
            self.logger.info("ğŸ”§ å…¶ä»–è®¡æ—¶å™¨:")
            for timer_name in sorted(unmapped_timers):
                stat = stats[timer_name]
                self.logger.info(f"  {timer_name}: æ€»è€—æ—¶={stat['total_time']:.3f}ç§’, "
                               f"å¹³å‡={stat['avg_time']:.3f}ç§’, æ¬¡æ•°={stat['count']}")
        
        # GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            self.logger.info("")
            self.logger.info("ğŸ’¾ GPUæ˜¾å­˜ç»Ÿè®¡:")
            self.logger.info(f"  å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"  å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²åˆ†é…: {self.gpu_memory_stats['current_allocated']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²ä¿ç•™: {self.gpu_memory_stats['current_reserved']:.2f}GB")
        
        self.logger.info("=" * 80)
    
    def save_batch_results_to_csv(self, result):
        """ä¿å­˜æ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶ - åŸºäºè¯„æµ‹å•å…ƒåˆ—è¡¨ç¡®ä¿æ•°æ®å®Œå…¨å¯¹åº”"""
        self.logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜æ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶...")
        
        # è®°å½•è¾“å…¥å‚æ•°çš„è¯¦ç»†ä¿¡æ¯
        self.logger.info(f"ğŸ’¾ è¾“å…¥å‚æ•°ç±»å‹: {type(result)}")
        self.logger.info(f"ğŸ’¾ è¾“å…¥å‚æ•°é”®: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
        
        # è¯¦ç»†æ‰“å°å…³é”®ä¼ å…¥å‚æ•°
        self.logger.info("ğŸ’¾ ========== å…³é”®ä¼ å…¥å‚æ•°è¯¦æƒ… ==========")
        
        # 1. evaluation_dates - è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        evaluation_dates = result.get('evaluation_dates', [])
        self.logger.info(f"ğŸ’¾ evaluation_dates (è¯„æµ‹æ—¥æœŸåˆ—è¡¨):")
        self.logger.info(f"ğŸ’¾   - ç±»å‹: {type(evaluation_dates)}")
        self.logger.info(f"ğŸ’¾   - é•¿åº¦: {len(evaluation_dates) if evaluation_dates else 0}")
        if evaluation_dates:
            self.logger.info(f"ğŸ’¾   - å†…å®¹: {evaluation_dates}")
        else:
            self.logger.info(f"ğŸ’¾   - å†…å®¹: ç©ºåˆ—è¡¨")
        
        # 2. batch_results - æ‰¹é‡åˆ†æç»“æœ
        batch_results = result.get('batch_results', {})
        self.logger.info(f"ğŸ’¾ batch_results (æ‰¹é‡åˆ†æç»“æœ):")
        self.logger.info(f"ğŸ’¾   - ç±»å‹: {type(batch_results)}")
        if isinstance(batch_results, dict):
            self.logger.info(f"ğŸ’¾   - é”®åˆ—è¡¨: {list(batch_results.keys())}")
            
            # æ‰“å°æ¯ä¸ªä¸»è¦é”®çš„è¯¦ç»†ä¿¡æ¯
            for key in batch_results.keys():
                value = batch_results[key]
                self.logger.info(f"ğŸ’¾   - {key}: {type(value)}")
                
                if key == 'summary' and isinstance(value, dict):
                    self.logger.info(f"ğŸ’¾     summaryå†…å®¹: {value}")
                elif key == 'evaluation_days':
                    self.logger.info(f"ğŸ’¾     evaluation_dayså€¼: {value}")
                elif key == 'num_historical_periods':
                    self.logger.info(f"ğŸ’¾     num_historical_periodså€¼: {value}")
                elif key == 'high_correlation_counts' and hasattr(value, '__len__'):
                    self.logger.info(f"ğŸ’¾     high_correlation_countsé•¿åº¦: {len(value)}")
                    if hasattr(value, 'shape'):
                        self.logger.info(f"ğŸ’¾     high_correlation_countså½¢çŠ¶: {value.shape}")
                elif key == 'avg_correlations' and hasattr(value, '__len__'):
                    self.logger.info(f"ğŸ’¾     avg_correlationsé•¿åº¦: {len(value)}")
                    if hasattr(value, 'shape'):
                        self.logger.info(f"ğŸ’¾     avg_correlationså½¢çŠ¶: {value.shape}")
                elif key == 'period_info' and isinstance(value, list):
                    self.logger.info(f"ğŸ’¾     period_infoåˆ—è¡¨é•¿åº¦: {len(value)}")
                    if len(value) > 0:
                        self.logger.info(f"ğŸ’¾     period_infoç¬¬ä¸€ä¸ªå…ƒç´ : {value[0]}")
            
            # æ‰“å°detailed_resultsçš„è¯¦ç»†ä¿¡æ¯
            detailed_results = batch_results.get('detailed_results', {})
            self.logger.info(f"ğŸ’¾   - detailed_resultsç±»å‹: {type(detailed_results)}")
            if isinstance(detailed_results, dict):
                self.logger.info(f"ğŸ’¾   - detailed_resultsåŒ…å«è‚¡ç¥¨: {list(detailed_results.keys())}")
                for stock_code, stock_data in detailed_results.items():
                    self.logger.info(f"ğŸ’¾   - è‚¡ç¥¨{stock_code}æ•°æ®ç±»å‹: {type(stock_data)}, é•¿åº¦: {len(stock_data) if hasattr(stock_data, '__len__') else 'N/A'}")
                    
                    # æ‰“å°æ¯ä¸ªè‚¡ç¥¨çš„è¯¦ç»†æ•°æ®ç»“æ„
                    if isinstance(stock_data, list) and len(stock_data) > 0:
                        self.logger.info(f"ğŸ’¾     è‚¡ç¥¨{stock_code}ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(stock_data[0])}")
                        if isinstance(stock_data[0], dict):
                            self.logger.info(f"ğŸ’¾     è‚¡ç¥¨{stock_code}ç¬¬ä¸€ä¸ªå…ƒç´ é”®: {list(stock_data[0].keys())}")
                            # æ‰“å°ç¬¬ä¸€ä¸ªå…ƒç´ çš„è¯¦ç»†å†…å®¹
                            first_item = stock_data[0]
                            for item_key, item_value in first_item.items():
                                if isinstance(item_value, (int, float, str, bool)):
                                    self.logger.info(f"ğŸ’¾       {item_key}: {item_value}")
                                else:
                                    self.logger.info(f"ğŸ’¾       {item_key}: {type(item_value)} (é•¿åº¦: {len(item_value) if hasattr(item_value, '__len__') else 'N/A'})")
            elif isinstance(detailed_results, list):
                self.logger.info(f"ğŸ’¾   - detailed_resultsåˆ—è¡¨é•¿åº¦: {len(detailed_results)}")
        else:
            self.logger.info(f"ğŸ’¾   - å†…å®¹: {batch_results}")
        
        # 3. is_multi_stock - æ˜¯å¦ä¸ºå¤šè‚¡ç¥¨æ¨¡å¼çš„æ ‡å¿—
        is_multi_stock = result.get('is_multi_stock', False)
        self.logger.info(f"ğŸ’¾ is_multi_stock (å¤šè‚¡ç¥¨æ¨¡å¼æ ‡å¿—):")
        self.logger.info(f"ğŸ’¾   - ç±»å‹: {type(is_multi_stock)}")
        self.logger.info(f"ğŸ’¾   - å€¼: {is_multi_stock}")
        
        # 4. å…¶ä»–é‡è¦å‚æ•°
        self.logger.info(f"ğŸ’¾ å…¶ä»–é‡è¦å‚æ•°:")
        other_params = ['stock_codes', 'backtest_date', 'evaluation_days', 'window_size', 'threshold', 'performance_stats']
        for param in other_params:
            if param in result:
                value = result[param]
                self.logger.info(f"ğŸ’¾   - {param}: {type(value)} = {value}")
                
                # å¯¹performance_statsè¿›è¡Œè¯¦ç»†å±•ç¤º
                if param == 'performance_stats' and isinstance(value, dict):
                    for perf_key, perf_value in value.items():
                        self.logger.info(f"ğŸ’¾     {perf_key}: {perf_value}")
        
        self.logger.info("ğŸ’¾ ========================================")
        
        try:
            
            # è®°å½•å…³é”®å‚æ•°ä¿¡æ¯
            self.logger.info(f"ğŸ’¾ è¯„æµ‹æ¨¡å¼: {'å¤šè‚¡ç¥¨æ¨¡å¼' if is_multi_stock else 'å•è‚¡ç¥¨æ¨¡å¼'}")
            
            # è®°å½•ç›®æ ‡CSVæ–‡ä»¶ä¿¡æ¯
            self.logger.info(f"ğŸ’¾ ç›®æ ‡CSVæ–‡ä»¶: {self.csv_results_file}")
            self.logger.info(f"ğŸ’¾ CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(self.csv_results_file)}")
            
            # è¯»å–ç°æœ‰CSVæ–‡ä»¶
            self.logger.info("ğŸ’¾ å¼€å§‹è¯»å–ç°æœ‰CSVæ–‡ä»¶...")
            if os.path.exists(self.csv_results_file):
                try:
                    df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
                    self.logger.info(f"ğŸ’¾ æˆåŠŸè¯»å–ç°æœ‰CSVæ–‡ä»¶ï¼Œç°æœ‰è®°å½•æ•°: {len(df)}")
                    if len(df) > 0:
                        self.logger.info(f"ğŸ’¾ ç°æœ‰CSVåˆ—å: {list(df.columns)}")
                        # æ˜¾ç¤ºç°æœ‰æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡
                        unique_stocks = df['ä»£ç '].nunique() if 'ä»£ç ' in df.columns else 0
                        unique_dates = df['è¯„æµ‹æ—¥æœŸ'].nunique() if 'è¯„æµ‹æ—¥æœŸ' in df.columns else 0
                        self.logger.info(f"ğŸ’¾ ç°æœ‰æ•°æ®ç»Ÿè®¡: {unique_stocks} ä¸ªè‚¡ç¥¨, {unique_dates} ä¸ªè¯„æµ‹æ—¥æœŸ")
                except Exception as e:
                    self.logger.error(f"ğŸ’¾ è¯»å–ç°æœ‰CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                    df = pd.DataFrame()
                    self.logger.info("ğŸ’¾ åˆ›å»ºç©ºDataFrameä½œä¸ºå¤‡ç”¨")
            else:
                df = pd.DataFrame()
                self.logger.info("ğŸ’¾ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºDataFrame")
            
            # æ„å»ºè¯„æµ‹å•å…ƒåˆ—è¡¨ - ä½¿ç”¨å’Œæ‰¹æ¬¡å¤„ç†æ—¶ç›¸åŒçš„é€»è¾‘
            evaluation_units = []
            self.logger.info("ğŸ’¾ å¼€å§‹æ„å»ºè¯„æµ‹å•å…ƒåˆ—è¡¨...")
            
            # è·å–è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            evaluation_dates = result.get('evaluation_dates', [])
            self.logger.info(f"ğŸ’¾ è¯„æµ‹æ—¥æœŸæ•°é‡: {len(evaluation_dates)}")
            
            if is_multi_stock:
                # å¤šè‚¡ç¥¨æ¨¡å¼ï¼šä½¿ç”¨å’Œæ‰¹æ¬¡å¤„ç†æ—¶ç›¸åŒçš„é€»è¾‘
                detailed_results = batch_results['detailed_results']
                self.logger.info(f"ğŸ’¾ å¤šè‚¡ç¥¨æ¨¡å¼ - è¯¦ç»†ç»“æœåŒ…å«è‚¡ç¥¨: {list(detailed_results.keys()) if isinstance(detailed_results, dict) else 'N/A'}")
                
                if isinstance(detailed_results, dict):
                    # æŒ‰ç…§è‚¡ç¥¨ä»£ç å’Œè¯„æµ‹æ—¥æœŸçš„ç»„åˆæ¥æ„å»ºè®¡ç®—å•å…ƒ
                    for stock_code, stock_daily_results in detailed_results.items():
                        self.logger.info(f"ğŸ’¾ å¤„ç†è‚¡ç¥¨ {stock_code}ï¼Œæ—¥ç»“æœæ•°é‡: {len(stock_daily_results) if isinstance(stock_daily_results, list) else 'N/A'}")
                        
                        if isinstance(stock_daily_results, list):
                            for daily_result in stock_daily_results:
                                evaluation_date = daily_result.get('evaluation_date')
                                
                                if evaluation_date:
                                    evaluation_unit = {
                                        'stock_code': str(stock_code),  # ç›´æ¥ä½¿ç”¨å¤–å±‚çš„stock_code
                                        'evaluation_date': evaluation_date,
                                        'daily_result': daily_result,
                                        'window_size': result['window_size'],
                                        'threshold': result['threshold']
                                    }
                                    evaluation_units.append(evaluation_unit)
                                else:
                                    self.logger.warning(f"ğŸ’¾ è‚¡ç¥¨ {stock_code} çš„æŸä¸ªæ—¥ç»“æœç¼ºå°‘evaluation_dateå­—æ®µ")
                        else:
                            self.logger.warning(f"ğŸ’¾ è‚¡ç¥¨ {stock_code} çš„æ—¥ç»“æœä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(stock_daily_results)}")
                else:
                    self.logger.error(f"ğŸ’¾ å¤šè‚¡ç¥¨æ¨¡å¼ä¸‹detailed_resultsä¸æ˜¯å­—å…¸æ ¼å¼: {type(detailed_results)}")
            else:
                # å•è‚¡ç¥¨æ¨¡å¼ï¼šä»detailed_resultsåˆ—è¡¨ä¸­æå–è¯„æµ‹å•å…ƒ
                detailed_results_list = batch_results['detailed_results']
                stock_code = result.get('stock_code', self.stock_code)
                self.logger.info(f"ğŸ’¾ å•è‚¡ç¥¨æ¨¡å¼ - ç›®æ ‡è‚¡ç¥¨: {stock_code}ï¼Œæ—¥ç»“æœæ•°é‡: {len(detailed_results_list) if isinstance(detailed_results_list, list) else 'N/A'}")
                
                if isinstance(detailed_results_list, list):
                    for daily_result in detailed_results_list:
                        evaluation_date = daily_result.get('evaluation_date')
                        
                        if evaluation_date:
                            evaluation_unit = {
                                'stock_code': str(stock_code),  # ä½¿ç”¨ç»Ÿä¸€çš„stock_code
                                'evaluation_date': evaluation_date,
                                'daily_result': daily_result,
                                'window_size': result['window_size'],
                                'threshold': result['threshold']
                            }
                            evaluation_units.append(evaluation_unit)
                        else:
                            self.logger.warning(f"ğŸ’¾ æŸä¸ªæ—¥ç»“æœç¼ºå°‘evaluation_dateå­—æ®µ")
                else:
                    self.logger.error(f"ğŸ’¾ å•è‚¡ç¥¨æ¨¡å¼ä¸‹detailed_resultsä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(detailed_results_list)}")
            
            # è®°å½•å½“å‰æ‰¹æ¬¡çš„è®¡ç®—å•å…ƒåˆ—è¡¨
            self.logger.info(f"ğŸ’¾ å½“å‰æ‰¹æ¬¡è®¡ç®—å•å…ƒåˆ—è¡¨ (å…± {len(evaluation_units)} ä¸ª):")
            if evaluation_units:
                for i, unit in enumerate(evaluation_units):
                    self.logger.info(f"   å•å…ƒ {i+1}: {unit['stock_code']} - {unit['evaluation_date'].strftime('%Y-%m-%d')}")
            
            # åŸºäºè¯„æµ‹å•å…ƒåˆ—è¡¨ç”ŸæˆCSVæ•°æ®è¡Œ
            new_rows = []
            self.logger.info("ğŸ’¾ å¼€å§‹åŸºäºè¯„æµ‹å•å…ƒç”ŸæˆCSVæ•°æ®è¡Œ...")
            
            for unit_idx, unit in enumerate(evaluation_units):
                stock_code = unit['stock_code']
                evaluation_date = unit['evaluation_date']
                daily_result = unit['daily_result']
                
                # æå–é¢„æµ‹ç»Ÿè®¡ä¿¡æ¯
                prediction_stats = daily_result.get('prediction_stats', {})
                
                # è®¡ç®—å¯¹æ¯”è‚¡ç¥¨æ•°é‡
                comparison_stock_count = len(self.comparison_stocks)
                
                # å‡†å¤‡å•è¡Œæ•°æ®
                row_data = {
                    'ä»£ç ': stock_code,
                    'window_size': unit['window_size'],
                    'é˜ˆå€¼': unit['threshold'],
                    'è¯„æµ‹æ—¥æœŸ': evaluation_date.strftime('%Y-%m-%d'),
                    'å¯¹æ¯”è‚¡ç¥¨æ•°é‡': comparison_stock_count,
                    'ç›¸å…³æ•°é‡': daily_result.get('daily_high_count', 0),
                    'ä¸‹1æ—¥é«˜å¼€': f"{prediction_stats.get('ratios', {}).get('next_day_gap_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹1æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_1_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹3æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_3_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹5æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_5_day_up', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹10æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('ratios', {}).get('next_10_day_up', 0):.2%}" if prediction_stats else 'N/A'
                }
                new_rows.append(row_data)
                
                # è®°å½•æ¯è¡Œæ•°æ®çš„è¯¦ç»†å†…å®¹ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹æˆ–å‰å‡ è¡Œï¼‰
                if self.debug or unit_idx < 3:
                    self.logger.info(f"ğŸ’¾ æ–°å¢æ•°æ®è¡Œ {unit_idx+1}: {row_data}")
            
            # è®°å½•æ•°æ®å‡†å¤‡å®Œæˆçš„ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(f"ğŸ’¾ CSVæ•°æ®å‡†å¤‡å®Œæˆï¼Œå…±ç”Ÿæˆ {len(new_rows)} è¡Œæ–°æ•°æ®")
            
            # æ·»åŠ æ‰€æœ‰æ–°è¡Œ
            if new_rows:
                self.logger.info("ğŸ’¾ å¼€å§‹åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰CSVæ•°æ®...")
                self.logger.info(f"ğŸ’¾ åˆå¹¶å‰ç°æœ‰æ•°æ®è¡Œæ•°: {len(df)}")
                self.logger.info(f"ğŸ’¾ å¾…åˆå¹¶æ–°æ•°æ®è¡Œæ•°: {len(new_rows)}")
                
                new_df = pd.DataFrame(new_rows)
                self.logger.info(f"ğŸ’¾ æ–°DataFrameåˆ›å»ºæˆåŠŸï¼Œåˆ—å: {list(new_df.columns)}")
                
                # åˆå¹¶æ•°æ®
                original_row_count = len(df)
                df = pd.concat([df, new_df], ignore_index=True)
                self.logger.info(f"ğŸ’¾ æ•°æ®åˆå¹¶å®Œæˆï¼Œåˆå¹¶åæ€»è¡Œæ•°: {len(df)} (å¢åŠ äº† {len(df) - original_row_count} è¡Œ)")
                
                # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df['ä»£ç '] = df['ä»£ç '].astype(str)
                self.logger.info("ğŸ’¾ ä»£ç åˆ—ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²å®Œæˆ")
                
                # æŒ‰è¯„æµ‹æ—¥æœŸé™åºæ’åˆ—ï¼ˆæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
                self.logger.info("ğŸ’¾ å¼€å§‹æŒ‰è¯„æµ‹æ—¥æœŸæ’åº...")
                df['è¯„æµ‹æ—¥æœŸ_æ’åº'] = pd.to_datetime(df['è¯„æµ‹æ—¥æœŸ'])
                df = df.sort_values('è¯„æµ‹æ—¥æœŸ_æ’åº', ascending=False)
                df = df.drop('è¯„æµ‹æ—¥æœŸ_æ’åº', axis=1)  # åˆ é™¤ä¸´æ—¶æ’åºåˆ—
                df = df.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
                self.logger.info("ğŸ’¾ æ•°æ®æ’åºå®Œæˆï¼ˆæŒ‰è¯„æµ‹æ—¥æœŸé™åºï¼‰")
                
                # ä¿å­˜CSVæ–‡ä»¶
                self.logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜CSVæ–‡ä»¶...")
                df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
                
                # ä¿å­˜åéªŒè¯
                self.logger.info("âœ… CSVæ–‡ä»¶ä¿å­˜å®Œæˆï¼Œå¼€å§‹éªŒè¯...")
                try:
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(self.csv_results_file):
                        # è·å–æ–‡ä»¶å¤§å°
                        file_size = os.path.getsize(self.csv_results_file)
                        file_size_mb = file_size / (1024 * 1024)
                        self.logger.info(f"âœ… CSVæ–‡ä»¶éªŒè¯ - æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size_mb:.2f} MB)")
                        
                        # é‡æ–°è¯»å–æ–‡ä»¶éªŒè¯è¡Œæ•°
                        verification_df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
                        actual_rows = len(verification_df)
                        self.logger.info(f"âœ… CSVæ–‡ä»¶éªŒè¯ - å®é™…è¡Œæ•°: {actual_rows}")
                        self.logger.info(f"âœ… CSVæ–‡ä»¶éªŒè¯ - åˆ—æ•°: {len(verification_df.columns)}")
                        
                        # éªŒè¯æ•°æ®ç»Ÿè®¡
                        if actual_rows > 0:
                            unique_stocks = verification_df['ä»£ç '].nunique() if 'ä»£ç ' in verification_df.columns else 0
                            unique_dates = verification_df['è¯„æµ‹æ—¥æœŸ'].nunique() if 'è¯„æµ‹æ—¥æœŸ' in verification_df.columns else 0
                            self.logger.info(f"âœ… CSVæ–‡ä»¶éªŒè¯ - åŒ…å« {unique_stocks} ä¸ªè‚¡ç¥¨, {unique_dates} ä¸ªè¯„æµ‹æ—¥æœŸ")
                            
                            # æ˜¾ç¤ºæœ€æ–°çš„å‡ æ¡è®°å½•ï¼ˆå‰3è¡Œï¼‰
                            if self.debug and actual_rows > 0:
                                self.logger.info("âœ… CSVæ–‡ä»¶éªŒè¯ - æœ€æ–°3æ¡è®°å½•:")
                                for i, row in verification_df.head(3).iterrows():
                                    self.logger.info(f"âœ…   è¡Œ{i+1}: {dict(row)}")
                        
                        self.logger.info(f"âœ… æ‰¹é‡ç»“æœå·²æˆåŠŸä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
                        self.logger.info(f"âœ… æœ¬æ¬¡æ–°å¢ {len(new_rows)} æ¡é€æ—¥è¯„æµ‹è®°å½•ï¼Œæ–‡ä»¶æ€»è®¡ {actual_rows} æ¡è®°å½•")
                    else:
                        self.logger.error("âŒ CSVæ–‡ä»¶ä¿å­˜åéªŒè¯å¤±è´¥ï¼šæ–‡ä»¶ä¸å­˜åœ¨")
                except Exception as verify_error:
                    self.logger.error(f"âŒ CSVæ–‡ä»¶ä¿å­˜åéªŒè¯æ—¶å‡ºé”™: {str(verify_error)}")
                    self.logger.info(f"âœ… æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
                    self.logger.info(f"âœ… å…±ä¿å­˜ {len(new_rows)} æ¡é€æ—¥è¯„æµ‹è®°å½•")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœéœ€è¦ä¿å­˜")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")


def analyze_pearson_correlation_gpu_batch(stock_code, backtest_date=None, evaluation_days=1, 
                                         window_size=15, threshold=0.85, comparison_mode='default', 
                                         comparison_stocks=None, debug=False, csv_filename=None, 
                                         use_gpu=True, batch_size=1000, earliest_date='2020-01-01',
                                         num_processes=None, evaluation_batch_size=20):
    """
    GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æçš„ä¾¿æ·å‡½æ•°
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
        evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
        window_size: çª—å£å¤§å°
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
        comparison_mode: å¯¹æ¯”æ¨¡å¼
        comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        debug: è°ƒè¯•æ¨¡å¼
        csv_filename: CSVæ–‡ä»¶å
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        batch_size: æ‰¹å¤„ç†å¤§å°
        earliest_date: æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (æ ¼å¼: YYYY-MM-DDï¼Œé»˜è®¤: 2020-01-01)
        evaluation_batch_size: æ¯æ‰¹æ¬¡å¤„ç†çš„è¯„æµ‹æ—¥æœŸæ•°é‡
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    if backtest_date is None:
        backtest_date = datetime.now().strftime('%Y-%m-%d')
    
    if csv_filename is None:
        csv_filename = 'batch_evaluation_results.csv'
    
    analyzer = GPUBatchPearsonAnalyzer(
        stock_code=stock_code,
        window_size=window_size,
        threshold=threshold,
        evaluation_days=evaluation_days,
        debug=debug,
        comparison_stocks=comparison_stocks,
        comparison_mode=comparison_mode,
        backtest_date=backtest_date,
        csv_filename=csv_filename,
        use_gpu=use_gpu,
        batch_size=batch_size,
        earliest_date=earliest_date,
        num_processes=num_processes,
        evaluation_batch_size=evaluation_batch_size
    )
    
    result = analyzer.analyze_batch()
    
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ')
    parser.add_argument('--stock_code', required=True, help='è‚¡ç¥¨ä»£ç æˆ–æ¨¡å¼åç§°ã€‚æ”¯æŒ: 1)å•ä¸ªè‚¡ç¥¨ä»£ç (000001) 2)å¤šä¸ªé€—å·åˆ†éš”(000001,000002) 3)é¢„å®šä¹‰æ¨¡å¼(top10/industry/all)')
    parser.add_argument('--backtest_date', type=str, help='å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--evaluation_days', type=int, default=1, help='è¯„æµ‹æ—¥æœŸæ•°é‡ (é»˜è®¤: 1)')
    parser.add_argument('--window_size', type=int, default=15, help='åˆ†æçª—å£å¤§å° (é»˜è®¤: 15)')
    parser.add_argument('--threshold', type=float, default=0.85, help='ç›¸å…³ç³»æ•°é˜ˆå€¼ (é»˜è®¤: 0.85)')
    parser.add_argument('--comparison_mode', type=str, default='top10', 
                       choices=['top10', 'industry', 'custom', 'self_only', 'all'],
                       help='å¯¹æ¯”æ¨¡å¼: top10(å¸‚å€¼å‰10), industry(è¡Œä¸šè‚¡ç¥¨), custom(è‡ªå®šä¹‰), self_only(ä»…è‡ªèº«å†å²), all(å…¨éƒ¨Aè‚¡) (é»˜è®¤: top10)')
    parser.add_argument('--comparison_stocks', nargs='*', 
                       help='è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš” (ä»…åœ¨comparison_mode=customæ—¶æœ‰æ•ˆ)')
    parser.add_argument('--debug', action='store_true', help='å¼€å¯è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--csv_filename', type=str, default='evaluation_results.csv', help='CSVç»“æœæ–‡ä»¶å (é»˜è®¤: evaluation_results.csv)')
    parser.add_argument('--no_gpu', action='store_true', help='ç¦ç”¨GPUåŠ é€Ÿ (é»˜è®¤å¯ç”¨GPU)')
    parser.add_argument('--batch_size', type=int, default=1000, 
                       help='GPUæ‰¹å¤„ç†å¤§å° - æ§åˆ¶å•æ¬¡GPUè®¡ç®—çš„æ•°æ®é‡ï¼Œå½±å“å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚'
                            'æ¨èå€¼ï¼šRTX 3060(8GB)=500-1000, RTX 3080(10GB)=1000-2000, RTX 4090(24GB)=2000-5000 (é»˜è®¤: 1000)')
    parser.add_argument('--earliest_date', type=str, default='2022-01-01', 
                       help='æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (YYYY-MM-DD)ï¼Œæ—©äºæ­¤æ—¥æœŸçš„æ•°æ®å°†è¢«è¿‡æ»¤æ‰ (é»˜è®¤: 2022-01-01)')
    parser.add_argument('--num_processes', type=int, default=None,
                       help='å¤šè¿›ç¨‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°-1ï¼‰')
    parser.add_argument('--evaluation_batch_size', type=int, default=20,
                        help='æ¯æ‰¹æ¬¡å¤„ç†çš„è®¡ç®—å•å…ƒæ•°é‡ï¼Œç”¨äºæ§åˆ¶GPUå†…å­˜ä½¿ç”¨ã€‚'
                             'å•è‚¡ç¥¨æ¨¡å¼: ç›´æ¥è¡¨ç¤ºè¯„æµ‹æ—¥æœŸæ•°é‡ (å¦‚evaluation_days=100, batch_size=20, åˆ†5æ‰¹å¤„ç†)ã€‚'
                             'å¤šè‚¡ç¥¨æ¨¡å¼: è¡¨ç¤ºæ€»è®¡ç®—å•å…ƒæ•° (å¦‚100è‚¡ç¥¨Ã—15è¯„æµ‹æ—¥æœŸ=1500å•å…ƒ, batch_size=20, åˆ†75æ‰¹å¤„ç†) (é»˜è®¤: 20)')

    args = parser.parse_args()
    
    # è§£æè‚¡ç¥¨ä»£ç ï¼Œæ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªè‚¡ç¥¨æˆ–æ¨¡å¼åç§°
    input_value = args.stock_code.strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„å®šä¹‰çš„æ¨¡å¼åç§°
    predefined_modes = ['top10', 'industry', 'all']
    if input_value in predefined_modes:
        # ä½¿ç”¨æ¨¡å¼è·å–è‚¡ç¥¨åˆ—è¡¨
        from stock_config import get_comparison_stocks, get_all_stocks_list
        if input_value == 'all':
            stock_codes = get_all_stocks_list()
        else:
            stock_codes = get_comparison_stocks(input_value)
        print(f"ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼ '{input_value}'ï¼Œè·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨")
    else:
        # ä¼ ç»Ÿçš„è‚¡ç¥¨ä»£ç è§£æï¼Œæ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªè‚¡ç¥¨
        stock_codes = [code.strip() for code in input_value.split(',')]
    
    print(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹åˆ†æï¼Œè‚¡ç¥¨ä»£ç : {stock_codes}")
    print(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {args.evaluation_days}")
    print(f"çª—å£å¤§å°: {args.window_size}")
    print(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {args.threshold}")
    
    # ä½¿ç”¨çœŸæ­£çš„å¤šè‚¡ç¥¨æ‰¹é‡å¤„ç†
    print(f"\nå¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨: {stock_codes}")
    result = analyze_pearson_correlation_gpu_batch(
        stock_code=','.join(stock_codes),  # ä¼ é€’é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç 
        backtest_date=args.backtest_date,
        evaluation_days=args.evaluation_days,
        window_size=args.window_size,
        threshold=args.threshold,
        comparison_mode=args.comparison_mode,
        comparison_stocks=args.comparison_stocks,
        debug=args.debug,
        csv_filename=args.csv_filename,
        use_gpu=not args.no_gpu,
        batch_size=args.batch_size,
        earliest_date=args.earliest_date,
        num_processes=args.num_processes,
        evaluation_batch_size=args.evaluation_batch_size
    )
    
    # è¾“å‡ºæ€»ä½“ç»“æœ
    if result:
        print(f"\næ‰€æœ‰è‚¡ç¥¨åˆ†æå®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(stock_codes)} ä¸ªè‚¡ç¥¨")
        print(f"è¯„æµ‹äº† {result['evaluation_days']} ä¸ªæ—¥æœŸ")
        print(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {result['batch_results']['summary']['total_high_correlations']}")
        print(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {result['batch_results']['summary']['avg_high_correlations_per_day']:.2f}")
        print(f"æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°: {result['batch_results']['summary']['overall_avg_correlation']:.4f}")
        
        # å¦‚æœæ˜¯å¤šè‚¡ç¥¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
        if len(stock_codes) > 1 and 'stock_summary' in result['batch_results']:
            print("\nå„è‚¡ç¥¨ç»Ÿè®¡ä¿¡æ¯:")
            for stock_code, stats in result['batch_results']['stock_summary'].items():
                print(f"  {stock_code}: é«˜ç›¸å…³æœŸé—´={stats['high_correlations']}, å¹³å‡ç›¸å…³æ€§={stats['avg_correlation']:.4f}")
    else:
        print("æ‰€æœ‰è‚¡ç¥¨åˆ†æå¤±è´¥")