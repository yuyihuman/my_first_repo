"""
è‚¡ç¥¨æ•°æ®Pearsonç›¸å…³ç³»æ•°åˆ†æè„šæœ¬ - GPUåŠ é€Ÿç‰ˆæœ¬

è¯¥è„šæœ¬ä½¿ç”¨PyTorch GPUåŠ é€Ÿæ¥æ‰¹é‡è®¡ç®—Pearsonç›¸å…³ç³»æ•°ï¼Œå¤§å¹…æå‡æ€§èƒ½ã€‚
é€šè¿‡çŸ©é˜µè¿ç®—æ›¿ä»£é€ä¸€æ¯”è¾ƒï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å¤§é‡æ•°æ®çš„ç›¸å…³æ€§åˆ†æã€‚

åŠŸèƒ½ï¼š
1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ä¼ å…¥è‚¡ç¥¨ä»£ç 
2. åŠ è½½è‚¡ç¥¨å†å²æ•°æ®
3. ä½¿ç”¨GPUæ‰¹é‡è®¡ç®—å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æˆäº¤é‡çš„Pearsonç›¸å…³ç³»æ•°
4. æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äºé˜ˆå€¼çš„æ•°æ®
5. å°†ç»“æœè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
python pearson_analyzer_gpu.py --stock_code 000001

ä½œè€…ï¼šStock Backtest System
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
GPUä¼˜åŒ–ç‰ˆæœ¬ï¼š2024å¹´
"""

import argparse
import logging
import os
from datetime import datetime
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from scipy.stats import pearsonr
from data_loader import StockDataLoader
import matplotlib.pyplot as plt
import mplfinance as mpf
from stock_config import get_comparison_stocks
import time
import threading
from collections import defaultdict
import warnings

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)


class GPUPearsonAnalyzer:
    def __init__(self, stock_code, log_dir='logs', window_size=15, threshold=0.9, debug=False, 
                 comparison_stocks=None, comparison_mode='default', backtest_date=None, 
                 csv_filename='evaluation_results.csv', use_gpu=True, batch_size=1000):
        """
        åˆå§‹åŒ–GPUåŠ é€Ÿçš„Pearsonç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            log_dir: æ—¥å¿—ç›®å½•
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥æ•°é‡ï¼‰
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            debug: æ˜¯å¦å¼€å¯debugæ¨¡å¼ï¼ˆå½±å“æ€§èƒ½ï¼‰
            comparison_stocks: è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            comparison_mode: å¯¹æ¯”æ¨¡å¼ ('default', 'top10', 'banks', 'tech', 'new_energy', 'healthcare', 'consumer', 'self_only')
            backtest_date: å›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä»è¯¥æ—¥æœŸå¾€å‰æ•°è·å–æ•°æ®æ®µè¿›è¡Œåˆ†æ
            csv_filename: CSVç»“æœæ–‡ä»¶å (é»˜è®¤: evaluation_results.csv)
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ (é»˜è®¤: True)
            batch_size: GPUæ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1000)
        """
        self.stock_code = stock_code
        
        # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
        script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
        self.log_dir = os.path.join(script_dir, 'logs')
        self.csv_results_file = os.path.join(script_dir, csv_filename)
        
        self.window_size = window_size
        self.threshold = threshold
        self.debug = debug
        self.comparison_mode = comparison_mode
        self.backtest_date = backtest_date
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.data_loader = None
        self.logger = None
        
        # GPUè®¾å¤‡è®¾ç½®
        self.device = self._setup_device()
        
        # è®¾ç½®å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        if comparison_stocks:
            self.comparison_stocks = comparison_stocks
        elif comparison_mode == 'self_only':
            self.comparison_stocks = [stock_code]  # åªå¯¹æ¯”è‡ªå·±çš„å†å²æ•°æ®
        else:
            self.comparison_stocks = get_comparison_stocks(comparison_mode)
            # ç¡®ä¿ç›®æ ‡è‚¡ç¥¨ä¸åœ¨å¯¹æ¯”åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
            if stock_code in self.comparison_stocks:
                self.comparison_stocks.remove(stock_code)
        
        # å­˜å‚¨å·²åŠ è½½çš„è‚¡ç¥¨æ•°æ®
        self.loaded_stocks_data = {}
        
        # æ€§èƒ½è®¡æ—¶å™¨
        self.performance_timers = defaultdict(list)
        self.current_timers = {}
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # è®¾ç½®CSVæ–‡ä»¶
        self._setup_csv_file()
        
        self.logger.info(f"åˆå§‹åŒ–GPUåŠ é€ŸPearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {stock_code}")
        self.logger.info(f"çª—å£å¤§å°: {window_size}, é˜ˆå€¼: {threshold}, Debugæ¨¡å¼: {debug}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {comparison_mode}, å¯¹æ¯”è‚¡ç¥¨æ•°é‡: {len(self.comparison_stocks)}")
        if self.debug:
            self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨: {self.comparison_stocks[:10]}{'...' if len(self.comparison_stocks) > 10 else ''}")
    
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰"""
        if self.use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if self.debug:
                print(f"ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name} ({gpu_memory:.1f}GB)")
            return device
        else:
            if self.use_gpu:
                print("è­¦å‘Šï¼šCUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")
            else:
                print("ä½¿ç”¨CPUè®¡ç®—")
            return torch.device('cpu')
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ›å»ºå­æ–‡ä»¶å¤¹
        stock_log_dir = os.path.join(self.log_dir, self.stock_code)
        os.makedirs(stock_log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        thread_id = threading.get_ident()
        log_filename = f"pearson_analysis_gpu_{self.stock_code}_{timestamp}_thread_{thread_id}.log"
        log_path = os.path.join(stock_log_dir, log_filename)
        
        # åˆ›å»ºlogger
        self.logger = logging.getLogger(f'GPUPearsonAnalyzer_{self.stock_code}')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶handler
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºformatter
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ handleråˆ°logger
        self.logger.addHandler(file_handler)
        
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_path}")
    
    def _setup_csv_file(self):
        """è®¾ç½®CSVæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(self.csv_results_file):
            if self.debug:
                self.logger.info(f"ğŸ†• Debug: CSVç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {self.csv_results_file}")
            
            # åˆ›å»ºCSVæ–‡ä»¶çš„è¡¨å¤´
            header = ['ä»£ç ', 'window_size', 'é˜ˆå€¼', 'è¯„æµ‹æ—¥æœŸ', 'å¯¹æ¯”è‚¡ç¥¨æ•°é‡', 'ç›¸å…³æ•°é‡', 'ä¸‹1æ—¥é«˜å¼€', 'ä¸‹1æ—¥ä¸Šæ¶¨', 'ä¸‹3æ—¥ä¸Šæ¶¨', 'ä¸‹5æ—¥ä¸Šæ¶¨', 'ä¸‹10æ—¥ä¸Šæ¶¨']
            df = pd.DataFrame(columns=header)
            # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"ğŸ†• Debug: CSVæ–‡ä»¶åˆ›å»ºå®Œæˆï¼Œè¡¨å¤´: {header}")
                self.logger.info(f"ğŸ†• Debug: åˆå§‹æ–‡ä»¶å¤§å°: {file_size} bytes")
        else:
            if self.debug:
                file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"âœ… Debug: CSVç»“æœæ–‡ä»¶å·²å­˜åœ¨: {self.csv_results_file}")
                self.logger.info(f"âœ… Debug: ç°æœ‰æ–‡ä»¶å¤§å°: {file_size} bytes")
    
    def save_evaluation_result(self, evaluation_date, stats, correlation_count=0):
        """
        ä¿å­˜å•æ¬¡è¯„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
        
        Args:
            evaluation_date: è¯„æµ‹æ—¥æœŸï¼ˆè¯„æµ‹åºåˆ—çš„æœ€åä¸€å¤©ï¼‰
            stats: ç»Ÿè®¡ç»“æœå­—å…¸
            correlation_count: ç›¸å…³æ•°é‡ï¼ˆé«˜ç›¸å…³æ€§æœŸé—´çš„æ•°é‡ï¼‰
        """
        try:
            # è®¡ç®—å¯¹æ¯”è‚¡ç¥¨æ•°é‡
            # åœ¨self_onlyæ¨¡å¼ä¸‹ï¼Œåªå¯¹æ¯”è‡ªèº«å†å²æ•°æ®ï¼Œä¸éœ€è¦é¢å¤–åŠ 1
            # åœ¨å…¶ä»–æ¨¡å¼ä¸‹ï¼Œéœ€è¦åŠ ä¸Šç›®æ ‡è‚¡ç¥¨è‡ªèº«
            if self.comparison_mode == 'self_only':
                comparison_stock_count = len(self.comparison_stocks)
            else:
                comparison_stock_count = len(self.comparison_stocks) + 1
            
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
            result_data = {
                'ä»£ç ': str(self.stock_code),  # ç¡®ä¿è‚¡ç¥¨ä»£ç ä¸ºå­—ç¬¦ä¸²
                'window_size': self.window_size,
                'é˜ˆå€¼': self.threshold,
                'è¯„æµ‹æ—¥æœŸ': evaluation_date.strftime('%Y-%m-%d'),
                'å¯¹æ¯”è‚¡ç¥¨æ•°é‡': comparison_stock_count,
                'ç›¸å…³æ•°é‡': correlation_count,
                'ä¸‹1æ—¥é«˜å¼€': f"{stats['ratios']['next_day_gap_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹1æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_1_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹3æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_3_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹5æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_5_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹10æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_10_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A'
            }
            
            # è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶ï¼ŒæŒ‡å®šä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            if os.path.exists(self.csv_results_file):
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: å¼€å§‹è¯»å–ç°æœ‰CSVæ–‡ä»¶: {self.csv_results_file}")
                    file_size = os.path.getsize(self.csv_results_file)
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶å¤§å°: {file_size} bytes")
                
                df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
                
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶è¯»å–å®Œæˆï¼Œå…± {len(df)} è¡Œæ•°æ®")
                    if not df.empty:
                        self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
                        self.logger.info(f"ğŸ“– Debug: æœ€åä¸€æ¡è®°å½•çš„è‚¡ç¥¨ä»£ç : {df.iloc[-1]['ä»£ç '] if 'ä»£ç ' in df.columns else 'N/A'}")
            else:
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„DataFrame: {self.csv_results_file}")
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„DataFrame
                df = pd.DataFrame()
            
            # æ·»åŠ æ–°çš„ç»“æœè¡Œ
            new_row = pd.DataFrame([result_data])
            df = pd.concat([df, new_row], ignore_index=True)
            
            # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            
            if self.debug:
                self.logger.info(f"ğŸ’¾ Debug: å‡†å¤‡ä¿å­˜CSVæ–‡ä»¶ï¼Œå½“å‰DataFrameå…± {len(df)} è¡Œæ•°æ®")
                self.logger.info(f"ğŸ’¾ Debug: æ–°å¢æ•°æ®: {result_data}")
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                # éªŒè¯ä¿å­˜åçš„æ–‡ä»¶
                saved_file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"ğŸ’¾ Debug: CSVæ–‡ä»¶ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {saved_file_size} bytes")
            
            self.logger.info(f"è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
            self.logger.info(f"ä¿å­˜çš„ç»“æœ: {result_data}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def start_timer(self, timer_name):
        """å¼€å§‹è®¡æ—¶"""
        self.current_timers[timer_name] = time.time()
        if self.debug:
            self.logger.info(f"â±ï¸ å¼€å§‹è®¡æ—¶: {timer_name}")
    
    def end_timer(self, timer_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if timer_name in self.current_timers:
            elapsed_time = time.time() - self.current_timers[timer_name]
            self.performance_timers[timer_name].append(elapsed_time)
            del self.current_timers[timer_name]
            if self.debug:
                self.logger.info(f"â±ï¸ ç»“æŸè®¡æ—¶: {timer_name} - è€—æ—¶: {elapsed_time:.3f}ç§’")
            return elapsed_time
        return 0
    
    def get_timer_stats(self, timer_name):
        """è·å–è®¡æ—¶å™¨ç»Ÿè®¡ä¿¡æ¯"""
        times = self.performance_timers[timer_name]
        if not times:
            return None
        return {
            'count': len(times),
            'total': sum(times),
            'average': sum(times) / len(times),
            'min': min(times),
            'max': max(times)
        }
    
    def log_performance_summary(self):
        """è¾“å‡ºæ€§èƒ½ç»Ÿè®¡è¡¨"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ GPUåŠ é€Ÿæ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        self.logger.info("=" * 80)
        
        # è®¡ç®—æ€»è€—æ—¶
        total_analysis_time = sum(self.performance_timers.get('total_analysis', [0]))
        
        # åˆ›å»ºç»Ÿè®¡è¡¨
        stats_table = []
        stats_table.append(f"{'é˜¶æ®µ':<25} {'æ¬¡æ•°':<8} {'æ€»è€—æ—¶(ç§’)':<12} {'å¹³å‡è€—æ—¶(ç§’)':<15} {'æœ€å°è€—æ—¶(ç§’)':<15} {'æœ€å¤§è€—æ—¶(ç§’)':<15}")
        stats_table.append("-" * 90)
        
        # å®šä¹‰å…³é”®é˜¶æ®µçš„æ˜¾ç¤ºé¡ºåºå’Œä¸­æ–‡åç§°
        stage_names = {
            'total_analysis': 'æ€»åˆ†ææ—¶é—´',
            'data_loading': 'æ•°æ®åŠ è½½',
            'target_stock_loading': 'ç›®æ ‡è‚¡ç¥¨æ•°æ®åŠ è½½',
            'comparison_stocks_loading': 'å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½',
            'gpu_data_preparation': 'GPUæ•°æ®å‡†å¤‡',
            'gpu_correlation_calculation': 'GPUç›¸å…³æ€§è®¡ç®—',
            'self_analysis': 'è‡ªèº«å†å²æ•°æ®åˆ†æ',
            'comparison_analysis': 'è·¨è‚¡ç¥¨å¯¹æ¯”åˆ†æ',
            'plotting': 'Kçº¿å›¾ç»˜åˆ¶',
            'stats_calculation': 'ç»Ÿè®¡è®¡ç®—'
        }
        
        for timer_name, display_name in stage_names.items():
            stats = self.get_timer_stats(timer_name)
            if stats:
                stats_table.append(
                    f"{display_name:<25} {stats['count']:<8} {stats['total']:<12.3f} "
                    f"{stats['average']:<15.3f} {stats['min']:<15.3f} {stats['max']:<15.3f}"
                )
        
        # è¾“å‡ºç»Ÿè®¡è¡¨
        for line in stats_table:
            self.logger.info(line)
        
        # è¾“å‡ºæ€§èƒ½åˆ†æ
        self.logger.info("-" * 90)
        if total_analysis_time > 0:
            data_loading_time = sum(self.performance_timers.get('data_loading', [0]))
            gpu_prep_time = sum(self.performance_timers.get('gpu_data_preparation', [0]))
            gpu_calc_time = sum(self.performance_timers.get('gpu_correlation_calculation', [0]))
            analysis_time = sum(self.performance_timers.get('self_analysis', [0])) + sum(self.performance_timers.get('comparison_analysis', [0]))
            plotting_time = sum(self.performance_timers.get('plotting', [0]))
            
            self.logger.info(f"ğŸ“Š GPUåŠ é€Ÿæ€§èƒ½åˆ†æ:")
            self.logger.info(f"   æ•°æ®åŠ è½½å æ¯”: {(data_loading_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   GPUæ•°æ®å‡†å¤‡å æ¯”: {(gpu_prep_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   GPUè®¡ç®—å æ¯”: {(gpu_calc_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   åˆ†æå¤„ç†å æ¯”: {(analysis_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   å›¾è¡¨ç»˜åˆ¶å æ¯”: {(plotting_time/total_analysis_time)*100:.1f}%")
        
        self.logger.info("=" * 80)

    def load_data(self):
        """åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®"""
        self.start_timer('target_stock_loading')
        self.logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨")
        self.data_loader = StockDataLoader()
        
        self.logger.info(f"å¼€å§‹åŠ è½½ç›®æ ‡è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
        data = self.data_loader.load_stock_data(self.stock_code)
        
        if data is None or data.empty:
            self.logger.error(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
            self.end_timer('target_stock_loading')
            return None
        
        # æ•°æ®è¿‡æ»¤ï¼šç¡®ä¿ä»·æ ¼ä¸ºæ­£æ•°ï¼Œæˆäº¤é‡å¤§äº0
        self.data = self._filter_data(data, self.stock_code)
        self.end_timer('target_stock_loading')
        
        # åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        self._load_comparison_stocks_data()
        
        return self.data
    
    def _filter_data(self, data, stock_code):
        """
        è¿‡æ»¤è‚¡ç¥¨æ•°æ®ï¼Œç¡®ä¿æ•°æ®è´¨é‡
        
        Args:
            data: åŸå§‹è‚¡ç¥¨æ•°æ®
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            è¿‡æ»¤åçš„æ•°æ®
        """
        if data is None or data.empty:
            return data
            
        original_count = len(data)
        data = data[
            (data['open'] > 0) & 
            (data['high'] > 0) & 
            (data['low'] > 0) & 
            (data['close'] > 0) & 
            (data['volume'] > 0)
        ]
        filtered_count = len(data)
        removed_count = original_count - filtered_count
        
        if removed_count > 0:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®è¿‡æ»¤å®Œæˆï¼Œç§»é™¤ {removed_count} æ¡å¼‚å¸¸æ•°æ®")
        
        if not data.empty:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        
        return data
    
    def _load_comparison_stocks_data(self):
        """åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®"""
        if self.comparison_mode == 'self_only':
            self.logger.info("ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”æ¨¡å¼ï¼Œè·³è¿‡å…¶ä»–è‚¡ç¥¨æ•°æ®åŠ è½½")
            return
        
        self.start_timer('comparison_stocks_loading')
        self.logger.info(f"å¼€å§‹åŠ è½½ {len(self.comparison_stocks)} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        successful_loads = 0
        
        for stock_code in self.comparison_stocks:
            try:
                if self.debug:
                    self.logger.info(f"æ­£åœ¨åŠ è½½å¯¹æ¯”è‚¡ç¥¨: {stock_code}")
                
                data = self.data_loader.load_stock_data(stock_code)
                if data is not None and not data.empty:
                    # è¿‡æ»¤æ•°æ®
                    filtered_data = self._filter_data(data, stock_code)
                    if not filtered_data.empty:
                        self.loaded_stocks_data[stock_code] = filtered_data
                        successful_loads += 1
                    else:
                        if self.debug:
                            self.logger.warning(f"è‚¡ç¥¨ {stock_code} è¿‡æ»¤åæ•°æ®ä¸ºç©º")
                else:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"æˆåŠŸåŠ è½½ {successful_loads} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        if successful_loads == 0:
            self.logger.warning("æœªèƒ½åŠ è½½ä»»ä½•å¯¹æ¯”è‚¡ç¥¨æ•°æ®ï¼Œå°†ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”")
        self.end_timer('comparison_stocks_loading')
    
    def prepare_gpu_data(self, recent_data, historical_periods_data):
        """
        å‡†å¤‡GPUè®¡ç®—æ‰€éœ€çš„æ•°æ®æ ¼å¼
        
        Args:
            recent_data: æœ€è¿‘æœŸé—´çš„æ•°æ® (DataFrame)
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨ [(data, start_date, end_date, stock_code), ...]
            
        Returns:
            tuple: (recent_tensor, historical_tensor, period_info_list)
        """
        self.start_timer('gpu_data_preparation')
        
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        try:
            # å‡†å¤‡æœ€è¿‘æ•°æ®
            recent_values = recent_data[fields].values.astype(np.float32)
            recent_tensor = torch.from_numpy(recent_values).to(self.device)
            
            # å‡†å¤‡å†å²æ•°æ®
            historical_data_list = []
            period_info_list = []
            
            for hist_data, start_date, end_date, stock_code in historical_periods_data:
                hist_values = hist_data[fields].values.astype(np.float32)
                if hist_values.shape[0] == self.window_size:  # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
                    historical_data_list.append(hist_values)
                    period_info_list.append({
                        'start_date': start_date,
                        'end_date': end_date,
                        'stock_code': stock_code
                    })
            
            if not historical_data_list:
                self.end_timer('gpu_data_preparation')
                return None, None, []
            
            # è½¬æ¢ä¸ºå¼ é‡ [num_periods, window_size, num_features]
            historical_array = np.stack(historical_data_list, axis=0)
            historical_tensor = torch.from_numpy(historical_array).to(self.device)
            
            self.logger.info(f"GPUæ•°æ®å‡†å¤‡å®Œæˆ: æœ€è¿‘æ•°æ® {recent_tensor.shape}, å†å²æ•°æ® {historical_tensor.shape}")
            
            self.end_timer('gpu_data_preparation')
            return recent_tensor, historical_tensor, period_info_list
            
        except Exception as e:
            self.logger.error(f"GPUæ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            self.end_timer('gpu_data_preparation')
            return None, None, []
    
    def calculate_pearson_correlation_gpu_batch(self, recent_tensor, historical_tensor):
        """
        ä½¿ç”¨GPUæ‰¹é‡è®¡ç®—Pearsonç›¸å…³ç³»æ•°
        
        Args:
            recent_tensor: æœ€è¿‘æ•°æ®å¼ é‡ [window_size, num_features]
            historical_tensor: å†å²æ•°æ®å¼ é‡ [num_periods, window_size, num_features]
            
        Returns:
            torch.Tensor: ç›¸å…³ç³»æ•°çŸ©é˜µ [num_periods, num_features]
        """
        self.start_timer('gpu_correlation_calculation')
        
        try:
            # æ‰©å±•recent_tensorä»¥åŒ¹é…historical_tensorçš„ç»´åº¦
            # recent_tensor: [window_size, num_features] -> [1, window_size, num_features]
            recent_expanded = recent_tensor.unsqueeze(0)
            
            # è®¡ç®—å‡å€¼
            recent_mean = torch.mean(recent_expanded, dim=1, keepdim=True)  # [1, 1, num_features]
            historical_mean = torch.mean(historical_tensor, dim=1, keepdim=True)  # [num_periods, 1, num_features]
            
            # ä¸­å¿ƒåŒ–æ•°æ®
            recent_centered = recent_expanded - recent_mean  # [1, window_size, num_features]
            historical_centered = historical_tensor - historical_mean  # [num_periods, window_size, num_features]
            
            # è®¡ç®—åæ–¹å·®
            # ä½¿ç”¨å¹¿æ’­æœºåˆ¶è®¡ç®—æ‰€æœ‰æœŸé—´çš„åæ–¹å·®
            covariance = torch.sum(recent_centered * historical_centered, dim=1)  # [num_periods, num_features]
            
            # è®¡ç®—æ ‡å‡†å·®
            recent_std = torch.sqrt(torch.sum(recent_centered ** 2, dim=1))  # [1, num_features]
            historical_std = torch.sqrt(torch.sum(historical_centered ** 2, dim=1))  # [num_periods, num_features]
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            denominator = recent_std * historical_std
            
            # é¿å…é™¤é›¶é”™è¯¯
            correlation = torch.where(
                denominator > 1e-8,
                covariance / denominator,
                torch.zeros_like(covariance)
            )
            
            # å¤„ç†NaNå’ŒInfå€¼
            correlation = torch.where(
                torch.isfinite(correlation),
                correlation,
                torch.zeros_like(correlation)
            )
            
            self.end_timer('gpu_correlation_calculation')
            return correlation
            
        except Exception as e:
            self.logger.error(f"GPUç›¸å…³ç³»æ•°è®¡ç®—å¤±è´¥: {str(e)}")
            self.end_timer('gpu_correlation_calculation')
            return None
    
    def process_gpu_correlation_results(self, correlation_tensor, period_info_list, threshold):
        """
        å¤„ç†GPUè®¡ç®—çš„ç›¸å…³ç³»æ•°ç»“æœ
        
        Args:
            correlation_tensor: ç›¸å…³ç³»æ•°å¼ é‡ [num_periods, num_features]
            period_info_list: æœŸé—´ä¿¡æ¯åˆ—è¡¨
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            
        Returns:
            list: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
        """
        fields = ['open', 'high', 'low', 'close', 'volume']
        high_correlation_periods = []
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¤„ç†
        correlation_array = correlation_tensor.cpu().numpy()
        
        # è®¡ç®—æ¯ä¸ªæœŸé—´çš„å¹³å‡ç›¸å…³ç³»æ•°
        avg_correlations = np.mean(correlation_array, axis=1)
        
        # æ‰¾å‡ºè¶…è¿‡é˜ˆå€¼çš„æœŸé—´
        high_corr_indices = np.where(avg_correlations >= threshold)[0]
        
        for idx in high_corr_indices:
            period_info = period_info_list[idx]
            avg_correlation = avg_correlations[idx]
            
            # æ„å»ºå„å­—æ®µçš„ç›¸å…³ç³»æ•°å­—å…¸
            correlations = {}
            for i, field in enumerate(fields):
                correlations[field] = {
                    'correlation': float(correlation_array[idx, i]),
                    'p_value': np.nan  # GPUç‰ˆæœ¬æš‚ä¸è®¡ç®—på€¼ä»¥æé«˜æ€§èƒ½
                }
            
            high_correlation_periods.append({
                'start_date': period_info['start_date'],
                'end_date': period_info['end_date'],
                'avg_correlation': float(avg_correlation),
                'correlations': correlations,
                'stock_code': period_info['stock_code'],
                'source': 'self' if period_info['stock_code'] == self.stock_code else 'comparison'
            })
        
        return high_correlation_periods
    
    def calculate_pearson_correlation_gpu(self, recent_data, historical_periods_data):
        """
        GPUåŠ é€Ÿçš„Pearsonç›¸å…³ç³»æ•°è®¡ç®—ä¸»å‡½æ•°
        
        Args:
            recent_data: æœ€è¿‘æœŸé—´çš„æ•°æ®
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            
        Returns:
            list: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
        """
        if not historical_periods_data:
            return []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…GPUå†…å­˜ä¸è¶³
        all_high_correlation_periods = []
        total_periods = len(historical_periods_data)
        
        for batch_start in range(0, total_periods, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total_periods)
            batch_data = historical_periods_data[batch_start:batch_end]
            
            if self.debug:
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start//self.batch_size + 1}/{(total_periods-1)//self.batch_size + 1}: "
                               f"æœŸé—´ {batch_start+1}-{batch_end}/{total_periods}")
            
            # å‡†å¤‡GPUæ•°æ®
            recent_tensor, historical_tensor, period_info_list = self.prepare_gpu_data(recent_data, batch_data)
            
            if recent_tensor is None or historical_tensor is None:
                continue
            
            # GPUæ‰¹é‡è®¡ç®—ç›¸å…³ç³»æ•°
            correlation_tensor = self.calculate_pearson_correlation_gpu_batch(recent_tensor, historical_tensor)
            
            if correlation_tensor is None:
                continue
            
            # å¤„ç†ç»“æœ
            batch_high_correlations = self.process_gpu_correlation_results(
                correlation_tensor, period_info_list, self.threshold
            )
            
            all_high_correlation_periods.extend(batch_high_correlations)
            
            # æ¸…ç†GPUå†…å­˜
            del recent_tensor, historical_tensor, correlation_tensor
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return all_high_correlation_periods
    
    def calculate_pearson_correlation(self, recent_data, historical_data):
        """
        å…¼å®¹æ€§å‡½æ•°ï¼šå•ä¸ªå†å²æœŸé—´çš„ç›¸å…³ç³»æ•°è®¡ç®—
        ä¸ºäº†ä¿æŒä¸åŸç‰ˆæœ¬çš„æ¥å£å…¼å®¹æ€§
        """
        # æ„å»ºå†å²æœŸé—´æ•°æ®åˆ—è¡¨
        historical_periods_data = [(
            historical_data,
            historical_data.index[0],
            historical_data.index[-1],
            self.stock_code
        )]
        
        # ä½¿ç”¨GPUæ‰¹é‡è®¡ç®—
        high_correlation_periods = self.calculate_pearson_correlation_gpu(recent_data, historical_periods_data)
        
        if high_correlation_periods:
            period = high_correlation_periods[0]
            return period['avg_correlation'], period['correlations']
        else:
            # å¦‚æœæ²¡æœ‰è¶…è¿‡é˜ˆå€¼ï¼Œä»ç„¶è¿”å›è®¡ç®—ç»“æœ
            recent_tensor, historical_tensor, period_info_list = self.prepare_gpu_data(recent_data, historical_periods_data)
            if recent_tensor is not None and historical_tensor is not None:
                correlation_tensor = self.calculate_pearson_correlation_gpu_batch(recent_tensor, historical_tensor)
                if correlation_tensor is not None:
                    correlation_array = correlation_tensor.cpu().numpy()
                    avg_correlation = float(np.mean(correlation_array[0]))
                    
                    fields = ['open', 'high', 'low', 'close', 'volume']
                    correlations = {}
                    for i, field in enumerate(fields):
                        correlations[field] = {
                            'correlation': float(correlation_array[0, i]),
                            'p_value': np.nan
                        }
                    
                    return avg_correlation, correlations
        
        # å›é€€åˆ°CPUè®¡ç®—
        return self.calculate_pearson_correlation_fallback(recent_data, historical_data)
    
    def calculate_pearson_correlation_fallback(self, recent_data, historical_data):
        """
        CPUå›é€€è®¡ç®—æ–¹æ³•
        """
        fields = ['open', 'high', 'low', 'close', 'volume']
        correlations = {}
        
        for field in fields:
            try:
                corr_coef, p_value = pearsonr(recent_data[field], historical_data[field])
                correlations[field] = {'correlation': corr_coef, 'p_value': p_value}
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"è®¡ç®— {field} ç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}")
                correlations[field] = {'correlation': np.nan, 'p_value': np.nan}
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°ï¼ˆå¿½ç•¥NaNå€¼ï¼‰
        valid_correlations = [corr['correlation'] for corr in correlations.values() 
                            if not np.isnan(corr['correlation'])]
        avg_correlation = np.mean(valid_correlations) if valid_correlations else 0
        
        return avg_correlation, correlations
    
    def analyze_self_historical_data(self, recent_data, backtest_date):
        """
        åˆ†æç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
        
        Args:
            recent_data: æœ€è¿‘æœŸé—´çš„æ•°æ®
            backtest_date: å›æµ‹æ—¥æœŸ
            
        Returns:
            list: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
        """
        self.start_timer('self_historical_analysis')
        self.logger.info(f"å¼€å§‹åˆ†æè‚¡ç¥¨ {self.stock_code} è‡ªèº«å†å²æ•°æ®")
        
        # è·å–å›æµ‹æ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®
        historical_data = self.data[self.data.index < backtest_date]
        
        if len(historical_data) < self.window_size:
            self.logger.warning(f"å†å²æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.window_size} æ¡è®°å½•")
            self.end_timer('self_historical_analysis')
            return []
        
        # å‡†å¤‡å†å²æœŸé—´æ•°æ®
        historical_periods_data = []
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å†å²çª—å£
        for i in range(len(historical_data) - self.window_size + 1):
            start_idx = i
            end_idx = i + self.window_size
            
            hist_period_data = historical_data.iloc[start_idx:end_idx]
            start_date = hist_period_data.index[0]
            end_date = hist_period_data.index[-1]
            
            historical_periods_data.append((
                hist_period_data,
                start_date,
                end_date,
                self.stock_code
            ))
        
        self.logger.info(f"å‡†å¤‡åˆ†æ {len(historical_periods_data)} ä¸ªå†å²æœŸé—´")
        
        # ä½¿ç”¨GPUæ‰¹é‡è®¡ç®—
        high_correlation_periods = self.calculate_pearson_correlation_gpu(recent_data, historical_periods_data)
        
        # æŒ‰ç›¸å…³ç³»æ•°æ’åº
        high_correlation_periods.sort(key=lambda x: x['avg_correlation'], reverse=True)
        
        self.logger.info(f"è‡ªèº«å†å²æ•°æ®åˆ†æå®Œæˆï¼Œå‘ç° {len(high_correlation_periods)} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´")
        
        if high_correlation_periods:
            avg_correlation = np.mean([p['avg_correlation'] for p in high_correlation_periods])
            self.logger.info(f"è‡ªèº«å†å²æ•°æ®å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
        
        self.end_timer('self_historical_analysis')
        return high_correlation_periods
    
    def analyze_comparison_stocks(self, recent_data, backtest_date):
        """
        åˆ†æå¯¹æ¯”è‚¡ç¥¨æ•°æ®
        
        Args:
            recent_data: æœ€è¿‘æœŸé—´çš„æ•°æ®
            backtest_date: å›æµ‹æ—¥æœŸ
            
        Returns:
            list: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
        """
        if self.comparison_mode == 'self_only':
            self.logger.info("è·³è¿‡å¯¹æ¯”è‚¡ç¥¨åˆ†æï¼ˆä»…ä½¿ç”¨è‡ªèº«å†å²æ•°æ®æ¨¡å¼ï¼‰")
            return []
        
        if not self.loaded_stocks_data:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„å¯¹æ¯”è‚¡ç¥¨æ•°æ®")
            return []
        
        self.start_timer('comparison_stocks_analysis')
        self.logger.info(f"å¼€å§‹åˆ†æ {len(self.loaded_stocks_data)} åªå¯¹æ¯”è‚¡ç¥¨")
        
        all_high_correlation_periods = []
        stock_analysis_count = 0
        
        for stock_code, stock_data in self.loaded_stocks_data.items():
            if self.debug:
                self.logger.info(f"æ­£åœ¨åˆ†æå¯¹æ¯”è‚¡ç¥¨: {stock_code}")
            
            # è·å–å›æµ‹æ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®
            historical_data = stock_data[stock_data.index < backtest_date]
            
            if len(historical_data) < self.window_size:
                if self.debug:
                    self.logger.warning(f"è‚¡ç¥¨ {stock_code} å†å²æ•°æ®ä¸è¶³")
                continue
            
            # å‡†å¤‡å†å²æœŸé—´æ•°æ®
            historical_periods_data = []
            
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å†å²çª—å£
            for i in range(len(historical_data) - self.window_size + 1):
                start_idx = i
                end_idx = i + self.window_size
                
                hist_period_data = historical_data.iloc[start_idx:end_idx]
                start_date = hist_period_data.index[0]
                end_date = hist_period_data.index[-1]
                
                historical_periods_data.append((
                    hist_period_data,
                    start_date,
                    end_date,
                    stock_code
                ))
            
            if self.debug:
                self.logger.info(f"è‚¡ç¥¨ {stock_code} å‡†å¤‡åˆ†æ {len(historical_periods_data)} ä¸ªå†å²æœŸé—´")
            
            # ä½¿ç”¨GPUæ‰¹é‡è®¡ç®—
            stock_high_correlations = self.calculate_pearson_correlation_gpu(recent_data, historical_periods_data)
            
            if stock_high_correlations:
                all_high_correlation_periods.extend(stock_high_correlations)
                if self.debug:
                    avg_corr = np.mean([p['avg_correlation'] for p in stock_high_correlations])
                    self.logger.info(f"è‚¡ç¥¨ {stock_code} å‘ç° {len(stock_high_correlations)} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´ï¼Œ"
                                   f"å¹³å‡ç›¸å…³ç³»æ•°: {avg_corr:.4f}")
            
            stock_analysis_count += 1
        
        # æŒ‰ç›¸å…³ç³»æ•°æ’åº
        all_high_correlation_periods.sort(key=lambda x: x['avg_correlation'], reverse=True)
        
        self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨åˆ†æå®Œæˆï¼Œå…±åˆ†æ {stock_analysis_count} åªè‚¡ç¥¨ï¼Œ"
                        f"å‘ç° {len(all_high_correlation_periods)} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´")
        
        if all_high_correlation_periods:
            avg_correlation = np.mean([p['avg_correlation'] for p in all_high_correlation_periods])
            self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
        
        self.end_timer('comparison_stocks_analysis')
        return all_high_correlation_periods
    
    def analyze(self, backtest_date=None, window_size=None, threshold=None, comparison_mode=None, 
                comparison_stocks=None, debug=None):
        """
        ä¸»åˆ†æå‡½æ•° - ä¿æŒä¸åŸç‰ˆæœ¬ç›¸åŒçš„æ¥å£
        
        Args:
            backtest_date: å›æµ‹æ—¥æœŸ
            window_size: çª—å£å¤§å°
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            debug: è°ƒè¯•æ¨¡å¼
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        self.start_timer('total_analysis')
        
        # æ›´æ–°å‚æ•°
        if backtest_date is not None:
            self.backtest_date = pd.to_datetime(backtest_date)
        if window_size is not None:
            self.window_size = window_size
        if threshold is not None:
            self.threshold = threshold
        if comparison_mode is not None:
            self.comparison_mode = comparison_mode
        if comparison_stocks is not None:
            self.comparison_stocks = comparison_stocks
        if debug is not None:
            self.debug = debug
        
        self.logger.info("=" * 80)
        self.logger.info(f"å¼€å§‹GPUåŠ é€ŸPearsonç›¸å…³æ€§åˆ†æ")
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"å›æµ‹æ—¥æœŸ: {self.backtest_date}")
        self.logger.info(f"çª—å£å¤§å°: {self.window_size}")
        self.logger.info(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {self.threshold}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {self.comparison_mode}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}")
        self.logger.info(f"æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        self.logger.info("=" * 80)
        
        # åŠ è½½æ•°æ®
        if self.data is None:
            if self.load_data() is None:
                self.logger.error("æ•°æ®åŠ è½½å¤±è´¥")
                return None
        
        # å‡†å¤‡æœ€è¿‘æœŸé—´æ•°æ®
        recent_data = self.data[self.data.index < self.backtest_date].tail(self.window_size)
        
        if len(recent_data) < self.window_size:
            self.logger.error(f"æœ€è¿‘æœŸé—´æ•°æ®ä¸è¶³ï¼Œéœ€è¦ {self.window_size} æ¡è®°å½•ï¼Œå®é™…åªæœ‰ {len(recent_data)} æ¡")
            return None
        
        self.logger.info(f"æœ€è¿‘æœŸé—´æ•°æ®: {recent_data.index[0]} åˆ° {recent_data.index[-1]}")
        
        # åˆ†æè‡ªèº«å†å²æ•°æ®
        self_high_correlations = self.analyze_self_historical_data(recent_data, self.backtest_date)
        
        # åˆ†æå¯¹æ¯”è‚¡ç¥¨æ•°æ®
        comparison_high_correlations = self.analyze_comparison_stocks(recent_data, self.backtest_date)
        
        # åˆå¹¶ç»“æœ
        all_high_correlations = self_high_correlations + comparison_high_correlations
        all_high_correlations.sort(key=lambda x: x['avg_correlation'], reverse=True)
        
        # ç”Ÿæˆåˆ†æç»“æœ
        result = {
            'stock_code': self.stock_code,
            'backtest_date': self.backtest_date,
            'window_size': self.window_size,
            'threshold': self.threshold,
            'recent_period': {
                'start_date': recent_data.index[0],
                'end_date': recent_data.index[-1]
            },
            'self_high_correlations': self_high_correlations,
            'comparison_high_correlations': comparison_high_correlations,
            'all_high_correlations': all_high_correlations,
            'summary': {
                'total_high_correlations': len(all_high_correlations),
                'self_high_correlations_count': len(self_high_correlations),
                'comparison_high_correlations_count': len(comparison_high_correlations),
                'avg_correlation': np.mean([p['avg_correlation'] for p in all_high_correlations]) if all_high_correlations else 0
            }
        }
        
        self.end_timer('total_analysis')
        
        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        self.print_performance_summary()
        
        # è¾“å‡ºåˆ†ææ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("åˆ†æç»“æœæ€»ç»“:")
        self.logger.info(f"æ€»è®¡å‘ç° {result['summary']['total_high_correlations']} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´")
        self.logger.info(f"  - è‡ªèº«å†å²æ•°æ®: {result['summary']['self_high_correlations_count']} ä¸ª")
        self.logger.info(f"  - å¯¹æ¯”è‚¡ç¥¨æ•°æ®: {result['summary']['comparison_high_correlations_count']} ä¸ª")
        if result['summary']['avg_correlation'] > 0:
            self.logger.info(f"å¹³å‡ç›¸å…³ç³»æ•°: {result['summary']['avg_correlation']:.4f}")
        self.logger.info("=" * 80)
        
        return result
    
    def save_results_to_csv(self, result, output_file=None):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°CSVæ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if result is None or not result['all_high_correlations']:
            self.logger.info("æ²¡æœ‰é«˜ç›¸å…³æ€§æœŸé—´æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"pearson_analysis_gpu_{self.stock_code}_{timestamp}.csv"
        
        try:
            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            for period in result['all_high_correlations']:
                row = {
                    'stock_code': period['stock_code'],
                    'source': period['source'],
                    'start_date': period['start_date'],
                    'end_date': period['end_date'],
                    'avg_correlation': period['avg_correlation']
                }
                
                # æ·»åŠ å„å­—æ®µçš„ç›¸å…³ç³»æ•°
                for field, corr_data in period['correlations'].items():
                    row[f'{field}_correlation'] = corr_data['correlation']
                    row[f'{field}_p_value'] = corr_data['p_value']
                
                csv_data.append(row)
            
            # ä¿å­˜åˆ°CSV
            df = pd.DataFrame(csv_data)
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            self.logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            self.logger.info(f"å…±ä¿å­˜ {len(csv_data)} æ¡é«˜ç›¸å…³æ€§æœŸé—´è®°å½•")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {str(e)}")

# å…¼å®¹æ€§å‡½æ•°ï¼šä¿æŒä¸åŸç‰ˆæœ¬ç›¸åŒçš„è°ƒç”¨æ¥å£
def analyze_pearson_correlation_gpu(stock_code, backtest_date, window_size=30, threshold=0.7,
                                   comparison_mode='mixed', comparison_stocks=None, debug=False,
                                   output_csv=True, batch_size=1000):
    """
    GPUåŠ é€Ÿçš„Pearsonç›¸å…³æ€§åˆ†æå‡½æ•° - ä¿æŒä¸åŸç‰ˆæœ¬ç›¸åŒçš„æ¥å£
    
    Args:
        stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç 
        backtest_date: å›æµ‹æ—¥æœŸ
        window_size: çª—å£å¤§å°
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
        comparison_mode: å¯¹æ¯”æ¨¡å¼
        comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        debug: è°ƒè¯•æ¨¡å¼
        output_csv: æ˜¯å¦è¾“å‡ºCSVæ–‡ä»¶
        batch_size: GPUæ‰¹å¤„ç†å¤§å°
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    analyzer = GPUPearsonAnalyzer(
        stock_code=stock_code,
        window_size=window_size,
        threshold=threshold,
        comparison_mode=comparison_mode,
        comparison_stocks=comparison_stocks,
        debug=debug,
        batch_size=batch_size
    )
    
    result = analyzer.analyze(backtest_date=backtest_date)
    
    if result and output_csv:
        analyzer.save_results_to_csv(result)
    
    return result

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    stock_code = "000001.SZ"
    backtest_date = "2024-01-01"
    
    result = analyze_pearson_correlation_gpu(
        stock_code=stock_code,
        backtest_date=backtest_date,
        window_size=30,
        threshold=0.7,
        comparison_mode='mixed',
        debug=True,
        batch_size=500
    )
    
    if result:
        print(f"åˆ†æå®Œæˆï¼Œå‘ç° {result['summary']['total_high_correlations']} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´")