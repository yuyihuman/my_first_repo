"""
è‚¡ç¥¨æ•°æ®Pearsonç›¸å…³ç³»æ•°åˆ†æè„šæœ¬ - GPUæ‰¹é‡è¯„æµ‹ç‰ˆæœ¬

è¯¥è„šæœ¬æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œé€šè¿‡ä¸‰ç»´çŸ©é˜µè¿ç®—å¤§å¹…æå‡GPUåˆ©ç”¨ç‡ã€‚
ç›¸æ¯”å•æ—¥è¯„æµ‹ç‰ˆæœ¬ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯„æµ‹æ—¥æœŸï¼Œå®ç°æ›´é«˜çš„å¹¶è¡Œè®¡ç®—æ•ˆç‡ã€‚

åŠŸèƒ½ï¼š
1. æ”¯æŒæ‰¹é‡è¯„æµ‹æ—¥æœŸå‚æ•°ï¼ˆevaluation_daysï¼‰
2. ä¸‰ç»´GPUçŸ©é˜µè¿ç®—ï¼š[è¯„æµ‹æ—¥æœŸæ•°, çª—å£å¤§å°, å­—æ®µæ•°]
3. æ‰¹é‡è®¡ç®—æ‰€æœ‰è¯„æµ‹æ—¥æœŸçš„Pearsonç›¸å…³ç³»æ•°
4. æ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œé¿å…GPUå†…å­˜æº¢å‡º
5. æ‰¹é‡ç»“æœç»Ÿè®¡å’ŒCSVå¯¼å‡º
6. GPUæ˜¾å­˜ç›‘æ§å’Œè‡ªé€‚åº”åˆ†ç»„å¤„ç†
7. çœŸæ­£çš„TopNæ¨¡å¼ï¼ˆNä¸ªè‚¡ç¥¨åŒæ—¶æœŸæ•°æ®çŸ©é˜µæ¯”è¾ƒï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
python pearson_analyzer_gpu_batch.py --stock_code 000001 --evaluation_days 100

ä½œè€…ï¼šStock Backtest System
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
GPUæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼š2024å¹´
TopNæ¨¡å¼å¢å¼ºç‰ˆæœ¬ï¼š2024å¹´
"""

import argparse
import logging
import os
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from scipy.stats import pearsonr
from data_loader import StockDataLoader
import matplotlib.pyplot as plt
import mplfinance as mpf
from stock_config import get_comparison_stocks
import time
import threading
from collections import defaultdict
import warnings
import gc

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)


class GPUBatchPearsonAnalyzer:
    def __init__(self, stock_code, log_dir='logs', window_size=15, threshold=0.9, 
                 evaluation_days=100, debug=False, comparison_stocks=None, 
                 comparison_mode='top10', backtest_date=None, 
                 csv_filename='evaluation_results.csv', use_gpu=True, 
                 batch_size=1000, gpu_memory_limit=0.8, topn_mode=True, 
                 max_comparison_stocks=10):
        """
        åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            log_dir: æ—¥å¿—ç›®å½•
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥æ•°é‡ï¼‰
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡ï¼ˆä»backtest_dateå¾€å‰æ•°çš„äº¤æ˜“æ—¥æ•°ï¼‰
            debug: æ˜¯å¦å¼€å¯debugæ¨¡å¼
            comparison_stocks: è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            backtest_date: å›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
            csv_filename: CSVç»“æœæ–‡ä»¶å
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            batch_size: GPUæ‰¹å¤„ç†å¤§å°
            gpu_memory_limit: GPUå†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆ0.0-1.0ï¼‰
            topn_mode: æ˜¯å¦å¯ç”¨TopNæ¨¡å¼ï¼ˆNä¸ªè‚¡ç¥¨åŒæ—¶æœŸæ•°æ®çŸ©é˜µæ¯”è¾ƒï¼‰
            max_comparison_stocks: TopNæ¨¡å¼ä¸‹æœ€å¤§å¯¹æ¯”è‚¡ç¥¨æ•°é‡
        """
        self.stock_code = stock_code
        
        # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
        script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
        self.log_dir = os.path.join(script_dir, 'logs')
        self.csv_results_file = os.path.join(script_dir, csv_filename)
        
        self.window_size = window_size
        self.threshold = threshold
        self.evaluation_days = evaluation_days  # æ–°å¢ï¼šè¯„æµ‹æ—¥æœŸæ•°é‡
        self.debug = debug
        self.comparison_mode = comparison_mode
        self.backtest_date = pd.to_datetime(backtest_date) if backtest_date else None
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.gpu_memory_limit = gpu_memory_limit
        self.topn_mode = topn_mode  # æ–°å¢ï¼šTopNæ¨¡å¼å¼€å…³
        self.max_comparison_stocks = max_comparison_stocks  # æ–°å¢ï¼šæœ€å¤§å¯¹æ¯”è‚¡ç¥¨æ•°é‡
        self.data_loader = None
        self.logger = None
        
        # GPUè®¾å¤‡è®¾ç½®
        self.device = self._setup_device()
        
        # GPUæ˜¾å­˜ç›‘æ§
        self.gpu_memory_stats = {
            'peak_allocated': 0,
            'peak_reserved': 0,
            'current_allocated': 0,
            'current_reserved': 0
        }
        
        # è®¾ç½®å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        if comparison_stocks:
            self.comparison_stocks = comparison_stocks
        elif comparison_mode == 'self_only':
            self.comparison_stocks = [stock_code]
        else:
            self.comparison_stocks = get_comparison_stocks(comparison_mode)
            if stock_code in self.comparison_stocks:
                self.comparison_stocks.remove(stock_code)
        
        # TopNæ¨¡å¼ä¸‹é™åˆ¶å¯¹æ¯”è‚¡ç¥¨æ•°é‡
        if self.topn_mode and len(self.comparison_stocks) > self.max_comparison_stocks:
            self.comparison_stocks = self.comparison_stocks[:self.max_comparison_stocks]
        
        # å­˜å‚¨å·²åŠ è½½çš„è‚¡ç¥¨æ•°æ®
        self.loaded_stocks_data = {}
        
        # æ€§èƒ½è®¡æ—¶å™¨
        self.performance_timers = defaultdict(list)
        self.current_timers = {}
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # è®¾ç½®CSVæ–‡ä»¶
        self._setup_csv_file()
        
        self.logger.info(f"åˆå§‹åŒ–GPUæ‰¹é‡è¯„æµ‹Pearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {stock_code}")
        self.logger.info(f"çª—å£å¤§å°: {window_size}, é˜ˆå€¼: {threshold}, è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        self.logger.info(f"GPUå†…å­˜é™åˆ¶: {gpu_memory_limit*100:.0f}%")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {comparison_mode}, å¯¹æ¯”è‚¡ç¥¨æ•°é‡: {len(self.comparison_stocks)}")
    
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰"""
        if self.use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if self.debug:
                print(f"ä½¿ç”¨GPUåŠ é€Ÿ: {gpu_name} ({gpu_memory:.1f}GB)")
            return device
        else:
            if self.use_gpu:
                print("è­¦å‘Šï¼šCUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUè®¡ç®—")
            else:
                print("ä½¿ç”¨CPUè®¡ç®—")
            return torch.device('cpu')
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        stock_log_dir = os.path.join(self.log_dir, self.stock_code)
        os.makedirs(stock_log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        thread_id = threading.get_ident()
        log_filename = f"batch_pearson_analysis_{self.stock_code}_{timestamp}_thread_{thread_id}.log"
        log_path = os.path.join(stock_log_dir, log_filename)
        
        self.logger = logging.getLogger(f'GPUBatchPearsonAnalyzer_{self.stock_code}')
        self.logger.setLevel(logging.INFO)
        
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        file_handler = logging.FileHandler(log_path, encoding='utf-8-sig')
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.info(f"æ‰¹é‡è¯„æµ‹æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_path}")
    
    def _setup_csv_file(self):
        """è®¾ç½®CSVæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(self.csv_results_file):
            # ä½¿ç”¨ä¸å•æ—¥è„šæœ¬ç›¸åŒçš„è¡¨å¤´æ ¼å¼
            header = ['ä»£ç ', 'window_size', 'é˜ˆå€¼', 'è¯„æµ‹æ—¥æœŸ', 'å¯¹æ¯”è‚¡ç¥¨æ•°é‡', 'ç›¸å…³æ•°é‡', 
                     'ä¸‹1æ—¥é«˜å¼€', 'ä¸‹1æ—¥ä¸Šæ¶¨', 'ä¸‹3æ—¥ä¸Šæ¶¨', 'ä¸‹5æ—¥ä¸Šæ¶¨', 'ä¸‹10æ—¥ä¸Šæ¶¨']
            df = pd.DataFrame(columns=header)
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                self.logger.info(f"ğŸ†• Debug: æ‰¹é‡è¯„æµ‹CSVæ–‡ä»¶åˆ›å»ºå®Œæˆ: {self.csv_results_file}")
    
    def start_timer(self, timer_name):
        """å¼€å§‹è®¡æ—¶"""
        self.current_timers[timer_name] = time.time()
        if self.debug:
            self.logger.info(f"â±ï¸ å¼€å§‹è®¡æ—¶: {timer_name}")
    
    def end_timer(self, timer_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if timer_name in self.current_timers:
            elapsed_time = time.time() - self.current_timers[timer_name]
            self.performance_timers[timer_name].append(elapsed_time)
            del self.current_timers[timer_name]
            if self.debug:
                self.logger.info(f"â±ï¸ ç»“æŸè®¡æ—¶: {timer_name} - è€—æ—¶: {elapsed_time:.3f}ç§’")
            return elapsed_time
        return 0
    
    def load_data(self):
        """åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®"""
        self.start_timer('target_stock_loading')
        self.logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨")
        self.data_loader = StockDataLoader()
        
        self.logger.info(f"å¼€å§‹åŠ è½½ç›®æ ‡è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
        data = self.data_loader.load_stock_data(self.stock_code)
        
        if data is None or data.empty:
            self.logger.error(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
            self.end_timer('target_stock_loading')
            return None
        
        self.data = self._filter_data(data, self.stock_code)
        self.end_timer('target_stock_loading')
        
        # åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        self._load_comparison_stocks_data()
        
        return self.data
    
    def _filter_data(self, data, stock_code):
        """è¿‡æ»¤è‚¡ç¥¨æ•°æ®ï¼Œç¡®ä¿æ•°æ®è´¨é‡"""
        if data is None or data.empty:
            return data
            
        original_count = len(data)
        data = data[
            (data['open'] > 0) & 
            (data['high'] > 0) & 
            (data['low'] > 0) & 
            (data['close'] > 0) & 
            (data['volume'] > 0)
        ]
        filtered_count = len(data)
        removed_count = original_count - filtered_count
        
        if removed_count > 0:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®è¿‡æ»¤å®Œæˆï¼Œç§»é™¤ {removed_count} æ¡å¼‚å¸¸æ•°æ®")
        
        if not data.empty:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        
        return data
    
    def _load_comparison_stocks_data(self):
        """åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®"""
        if self.comparison_mode == 'self_only':
            self.logger.info("ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”æ¨¡å¼ï¼Œè·³è¿‡å…¶ä»–è‚¡ç¥¨æ•°æ®åŠ è½½")
            return
        
        self.start_timer('comparison_stocks_loading')
        self.logger.info(f"å¼€å§‹åŠ è½½ {len(self.comparison_stocks)} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        successful_loads = 0
        
        for stock_code in self.comparison_stocks:
            try:
                if self.debug:
                    self.logger.info(f"æ­£åœ¨åŠ è½½å¯¹æ¯”è‚¡ç¥¨: {stock_code}")
                
                data = self.data_loader.load_stock_data(stock_code)
                if data is not None and not data.empty:
                    filtered_data = self._filter_data(data, stock_code)
                    if not filtered_data.empty:
                        self.loaded_stocks_data[stock_code] = filtered_data
                        successful_loads += 1
                    else:
                        if self.debug:
                            self.logger.warning(f"è‚¡ç¥¨ {stock_code} è¿‡æ»¤åæ•°æ®ä¸ºç©º")
                else:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"æˆåŠŸåŠ è½½ {successful_loads} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        self.end_timer('comparison_stocks_loading')
    
    def prepare_evaluation_dates(self, end_date):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        
        Args:
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            list: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
        """
        self.start_timer('evaluation_dates_preparation')
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„äº¤æ˜“æ—¥æœŸï¼ˆåŒ…å«end_dateå½“å¤©ï¼Œå¦‚æœæ•°æ®å¯ç”¨ï¼‰
        available_dates = self.data[self.data.index <= end_date].index
        
        if len(available_dates) < self.evaluation_days + self.window_size:
            self.logger.warning(f"å¯ç”¨æ•°æ®ä¸è¶³ï¼Œéœ€è¦ {self.evaluation_days + self.window_size} ä¸ªäº¤æ˜“æ—¥ï¼Œ"
                              f"å®é™…åªæœ‰ {len(available_dates)} ä¸ª")
            # è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡
            self.evaluation_days = max(1, len(available_dates) - self.window_size)
            self.logger.info(f"è°ƒæ•´è¯„æµ‹æ—¥æœŸæ•°é‡ä¸º: {self.evaluation_days}")
        
        # é€‰æ‹©æœ€è¿‘çš„evaluation_daysä¸ªäº¤æ˜“æ—¥ä½œä¸ºè¯„æµ‹æ—¥æœŸ
        evaluation_dates = available_dates[-self.evaluation_days:].tolist()
        
        self.logger.info(f"å‡†å¤‡äº† {len(evaluation_dates)} ä¸ªè¯„æµ‹æ—¥æœŸ")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸèŒƒå›´: {evaluation_dates[0]} åˆ° {evaluation_dates[-1]}")
        
        self.end_timer('evaluation_dates_preparation')
        return evaluation_dates
    
    def prepare_batch_evaluation_data(self, evaluation_dates):
        """
        å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®çŸ©é˜µ
        
        Args:
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            
        Returns:
            torch.Tensor: å½¢çŠ¶ä¸º [evaluation_days, window_size, 5] çš„è¯„æµ‹æ•°æ®å¼ é‡
        """
        self.start_timer('batch_data_preparation')
        
        fields = ['open', 'high', 'low', 'close', 'volume']
        batch_data_list = []
        valid_dates = []
        
        for eval_date in evaluation_dates:
            # è·å–è¯¥è¯„æµ‹æ—¥æœŸçš„çª—å£æ•°æ®
            recent_data = self.data[self.data.index < eval_date].tail(self.window_size)
            
            if len(recent_data) == self.window_size:
                # æå–å­—æ®µæ•°æ®
                data_values = recent_data[fields].values  # [window_size, 5]
                batch_data_list.append(data_values)
                valid_dates.append(eval_date)
            else:
                if self.debug:
                    self.logger.warning(f"è¯„æµ‹æ—¥æœŸ {eval_date} çš„æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
        
        if not batch_data_list:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ•°æ®")
            self.end_timer('batch_data_preparation')
            return None, []
        
        # è½¬æ¢ä¸ºå¼ é‡ [evaluation_days, window_size, 5]
        batch_data = np.stack(batch_data_list, axis=0)
        batch_tensor = torch.tensor(batch_data, dtype=torch.float32, device=self.device)
        
        self.logger.info(f"æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {batch_tensor.shape}")
        self.logger.info(f"æœ‰æ•ˆè¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
        
        self.end_timer('batch_data_preparation')
        return batch_tensor, valid_dates
    
    def calculate_batch_gpu_correlation(self, batch_recent_data, historical_periods_data):
        """
        æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—
        
        Args:
            batch_recent_data: æ‰¹é‡è¯„æµ‹æ•°æ® [evaluation_days, window_size, 5]
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            
        Returns:
            dict: æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        """
        self.start_timer('batch_gpu_correlation')
        
        if batch_recent_data is None or len(historical_periods_data) == 0:
            self.end_timer('batch_gpu_correlation')
            return {}
        
        evaluation_days, window_size, num_fields = batch_recent_data.shape
        num_historical_periods = len(historical_periods_data)
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}, å†å²æœŸé—´æ•°: {num_historical_periods}")
        
        # å‡†å¤‡å†å²æ•°æ®å¼ é‡ [num_historical_periods, window_size, 5]
        historical_data_list = []
        period_info_list = []
        
        for data, start_date, end_date, stock_code in historical_periods_data:
            if len(data) == window_size:
                fields = ['open', 'high', 'low', 'close', 'volume']
                historical_values = data[fields].values
                historical_data_list.append(historical_values)
                period_info_list.append({
                    'start_date': start_date,
                    'end_date': end_date,
                    'stock_code': stock_code
                })
        
        if not historical_data_list:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            self.end_timer('batch_gpu_correlation')
            return {}
        
        historical_tensor = torch.tensor(
            np.stack(historical_data_list, axis=0), 
            dtype=torch.float32, 
            device=self.device
        )  # [num_historical_periods, window_size, 5]
        
        # æ‰¹é‡è®¡ç®—ç›¸å…³ç³»æ•°
        # æ‰©å±•ç»´åº¦è¿›è¡Œæ‰¹é‡è®¡ç®—
        # batch_recent_data: [evaluation_days, window_size, 5]
        # historical_tensor: [num_historical_periods, window_size, 5]
        # ç›®æ ‡: [evaluation_days, num_historical_periods, 5]
        
        batch_correlations = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        batch_size = min(self.batch_size, evaluation_days)
        
        for i in range(0, evaluation_days, batch_size):
            end_idx = min(i + batch_size, evaluation_days)
            current_batch = batch_recent_data[i:end_idx]  # [batch_size, window_size, 5]
            
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç›¸å…³ç³»æ•°
            batch_corr = self._compute_correlation_matrix(current_batch, historical_tensor)
            batch_correlations.append(batch_corr)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_correlations = torch.cat(batch_correlations, dim=0)  # [evaluation_days, num_historical_periods, 5]
        
        self.logger.info(f"æ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {all_correlations.shape}")
        
        # å¤„ç†ç»“æœ
        results = self._process_batch_correlation_results(
            all_correlations, period_info_list, evaluation_days
        )
        
        self.end_timer('batch_gpu_correlation')
        return results
    
    def _compute_correlation_matrix(self, recent_batch, historical_tensor):
        """
        è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        
        Args:
            recent_batch: [batch_size, window_size, 5]
            historical_tensor: [num_historical_periods, window_size, 5]
            
        Returns:
            torch.Tensor: [batch_size, num_historical_periods, 5]
        """
        batch_size, window_size, num_fields = recent_batch.shape
        num_historical_periods = historical_tensor.shape[0]
        
        # æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­è®¡ç®—
        recent_expanded = recent_batch.unsqueeze(1)  # [batch_size, 1, window_size, 5]
        historical_expanded = historical_tensor.unsqueeze(0)  # [1, num_historical_periods, window_size, 5]
        
        # è®¡ç®—å‡å€¼
        recent_mean = recent_expanded.mean(dim=2, keepdim=True)  # [batch_size, 1, 1, 5]
        historical_mean = historical_expanded.mean(dim=2, keepdim=True)  # [1, num_historical_periods, 1, 5]
        
        # ä¸­å¿ƒåŒ–
        recent_centered = recent_expanded - recent_mean
        historical_centered = historical_expanded - historical_mean
        
        # è®¡ç®—åæ–¹å·®
        covariance = (recent_centered * historical_centered).sum(dim=2)  # [batch_size, num_historical_periods, 5]
        
        # è®¡ç®—æ ‡å‡†å·®
        recent_std = torch.sqrt((recent_centered ** 2).sum(dim=2))  # [batch_size, 1, 5]
        historical_std = torch.sqrt((historical_centered ** 2).sum(dim=2))  # [1, num_historical_periods, 5]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = covariance / (recent_std * historical_std + 1e-8)
        
        return correlation
    
    def _process_batch_correlation_results(self, correlations_tensor, period_info_list, evaluation_days):
        """
        å¤„ç†æ‰¹é‡ç›¸å…³æ€§è®¡ç®—ç»“æœ
        
        Args:
            correlations_tensor: [evaluation_days, num_historical_periods, 5]
            period_info_list: å†å²æœŸé—´ä¿¡æ¯åˆ—è¡¨
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            
        Returns:
            dict: å¤„ç†åçš„ç»“æœ
        """
        self.start_timer('batch_result_processing')
        
        correlations_np = correlations_tensor.cpu().numpy()
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•° [evaluation_days, num_historical_periods]
        avg_correlations = correlations_np.mean(axis=2)
        
        # è¿‡æ»¤æ‰ç›¸å…³æ€§ä¸º1.0çš„ç»“æœï¼ˆè‡ªç›¸å…³ï¼‰
        # è®¾ç½®å®¹å·®ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
        self_correlation_threshold = 0.9999
        self_correlation_mask = avg_correlations >= self_correlation_threshold
        
        # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è‡ªç›¸å…³æ•°é‡
        filtered_count = self_correlation_mask.sum()
        if filtered_count > 0:
            self.logger.info(f"è¿‡æ»¤æ‰ {filtered_count} ä¸ªè‡ªç›¸å…³ç»“æœï¼ˆç›¸å…³æ€§ >= {self_correlation_threshold}ï¼‰")
        
        # å°†è‡ªç›¸å…³çš„ä½ç½®è®¾ç½®ä¸º0ï¼Œä½¿å…¶ä¸ä¼šè¢«é€‰ä¸ºé«˜ç›¸å…³æ€§æœŸé—´
        avg_correlations_filtered = avg_correlations.copy()
        avg_correlations_filtered[self_correlation_mask] = 0.0
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§æœŸé—´ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°ï¼‰
        high_corr_mask = avg_correlations_filtered > self.threshold
        
        # ç»Ÿè®¡ç»“æœ
        results = {
            'evaluation_days': evaluation_days,
            'num_historical_periods': len(period_info_list),
            'high_correlation_counts': high_corr_mask.sum(axis=1).tolist(),  # æ¯ä¸ªè¯„æµ‹æ—¥æœŸçš„é«˜ç›¸å…³æ•°é‡
            'avg_correlations': avg_correlations_filtered.tolist(),  # ä½¿ç”¨è¿‡æ»¤åçš„ç›¸å…³ç³»æ•°
            'detailed_correlations': correlations_np.tolist(),
            'period_info': period_info_list,
            'summary': {
                'total_high_correlations': high_corr_mask.sum(),
                'avg_high_correlations_per_day': high_corr_mask.sum(axis=1).mean(),
                'max_high_correlations_per_day': high_corr_mask.sum(axis=1).max(),
                'overall_avg_correlation': avg_correlations_filtered[high_corr_mask].mean() if high_corr_mask.any() else 0,
                'filtered_self_correlations': int(filtered_count)  # æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
            }
        }
        
        self.logger.info(f"æ‰¹é‡ç»“æœå¤„ç†å®Œæˆ")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {results['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°: {results['summary']['avg_high_correlations_per_day']:.2f}")
        
        self.end_timer('batch_result_processing')
        return results
    
    def calculate_future_performance_stats(self, data, high_correlation_periods):
        """
        è®¡ç®—é«˜ç›¸å…³æ€§æœŸé—´çš„æœªæ¥äº¤æ˜“æ—¥è¡¨ç°ç»Ÿè®¡
        
        Args:
            data: å®Œæ•´çš„è‚¡ç¥¨æ•°æ®
            high_correlation_periods: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
            
        Returns:
            dict: ç»Ÿè®¡ç»“æœ
        """
        if not high_correlation_periods:
            return None
        
        stats = {
            'total_periods': len(high_correlation_periods),
            'next_day_gap_up': 0,  # ä¸‹1ä¸ªäº¤æ˜“æ—¥é«˜å¼€
            'next_1_day_up': 0,    # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_3_day_up': 0,    # ä¸‹3ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_5_day_up': 0,    # ä¸‹5ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_10_day_up': 0,   # ä¸‹10ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'valid_periods': {
                'next_day': 0,
                'next_3_day': 0,
                'next_5_day': 0,
                'next_10_day': 0
            }
        }
        
        for i, period in enumerate(high_correlation_periods, 1):
            end_date = period['end_date']
            start_date = period['start_date']
            avg_correlation = period['avg_correlation']
            source_stock_code = period['stock_code']
            
            # æ ¹æ®æ¥æºè‚¡ç¥¨ä»£ç è·å–æ­£ç¡®çš„æ•°æ®æº
            if source_stock_code == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                source_data = self.loaded_stocks_data.get(source_stock_code)
                if source_data is None:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {source_stock_code} çš„æ•°æ®ï¼Œè·³è¿‡æœŸé—´ #{i}")
                    continue
            
            # æ‰¾åˆ°è¯¥æœŸé—´ç»“æŸåçš„æ•°æ®ä½ç½®
            try:
                end_idx = source_data.index.get_loc(end_date)
            except KeyError:
                if self.debug:
                    self.logger.warning(f"åœ¨è‚¡ç¥¨ {source_stock_code} æ•°æ®ä¸­æ‰¾ä¸åˆ°æ—¥æœŸ {end_date}ï¼Œè·³è¿‡æœŸé—´ #{i}")
                continue
            
            # è·å–æœŸé—´æœ€åä¸€å¤©çš„æ”¶ç›˜ä»·
            period_close = source_data.iloc[end_idx]['close']
            
            # æ£€æŸ¥ä¸‹1ä¸ªäº¤æ˜“æ—¥
            if end_idx + 1 < len(source_data):
                next_day_data = source_data.iloc[end_idx + 1]
                next_day_open = next_day_data['open']
                next_day_close = next_day_data['close']
                
                stats['valid_periods']['next_day'] += 1
                
                # é«˜å¼€åˆ¤æ–­
                if next_day_open > period_close:
                    stats['next_day_gap_up'] += 1
                
                # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨åˆ¤æ–­
                if next_day_close > period_close:
                    stats['next_1_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹3ä¸ªäº¤æ˜“æ—¥
            if end_idx + 3 < len(source_data):
                day_3_close = source_data.iloc[end_idx + 3]['close']
                stats['valid_periods']['next_3_day'] += 1
                
                if day_3_close > period_close:
                    stats['next_3_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹5ä¸ªäº¤æ˜“æ—¥
            if end_idx + 5 < len(source_data):
                day_5_close = source_data.iloc[end_idx + 5]['close']
                stats['valid_periods']['next_5_day'] += 1
                
                if day_5_close > period_close:
                    stats['next_5_day_up'] += 1
            
            # æ£€æŸ¥ä¸‹10ä¸ªäº¤æ˜“æ—¥
            if end_idx + 10 < len(source_data):
                day_10_close = source_data.iloc[end_idx + 10]['close']
                stats['valid_periods']['next_10_day'] += 1
                
                if day_10_close > period_close:
                    stats['next_10_day_up'] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        stats['ratios'] = {}
        if stats['valid_periods']['next_day'] > 0:
            stats['ratios']['next_day_gap_up'] = stats['next_day_gap_up'] / stats['valid_periods']['next_day']
            stats['ratios']['next_1_day_up'] = stats['next_1_day_up'] / stats['valid_periods']['next_day']
        
        if stats['valid_periods']['next_3_day'] > 0:
            stats['ratios']['next_3_day_up'] = stats['next_3_day_up'] / stats['valid_periods']['next_3_day']
        
        if stats['valid_periods']['next_5_day'] > 0:
            stats['ratios']['next_5_day_up'] = stats['next_5_day_up'] / stats['valid_periods']['next_5_day']
        
        if stats['valid_periods']['next_10_day'] > 0:
            stats['ratios']['next_10_day_up'] = stats['next_10_day_up'] / stats['valid_periods']['next_10_day']
        
        return stats
    
    def process_batch_results(self, batch_correlations, evaluation_dates, historical_periods_data):
        """
        å¤„ç†æ‰¹é‡ç›¸å…³æ€§ç»“æœ
        
        Args:
            batch_correlations: æ‰¹é‡ç›¸å…³æ€§ç»“æœå­—å…¸
            evaluation_dates: è¯„æµ‹æ—¥æœŸåˆ—è¡¨
            historical_periods_data: å†å²æœŸé—´æ•°æ®åˆ—è¡¨
            
        Returns:
            dict: å¤„ç†åçš„æ‰¹é‡ç»“æœ
        """
        self.start_timer('batch_results_processing')
        
        # ä»batch_correlationsä¸­æå–æ•°æ®
        avg_correlations = batch_correlations.get('avg_correlations', [])  # [evaluation_days, num_historical_periods]
        summary = batch_correlations.get('summary', {})
        period_info = batch_correlations.get('period_info', [])
        
        # æ„å»ºè¯¦ç»†ç»“æœ
        detailed_results = []
        
        for eval_idx, eval_date in enumerate(evaluation_dates):
            if eval_idx < len(avg_correlations):
                eval_avg_correlations = avg_correlations[eval_idx]  # è¯¥è¯„æµ‹æ—¥æœŸçš„å¹³å‡ç›¸å…³æ€§åˆ—è¡¨
                
                # æ‰¾åˆ°é«˜ç›¸å…³æ€§æœŸé—´
                high_corr_periods = []
                for hist_idx, avg_correlation in enumerate(eval_avg_correlations):
                    if avg_correlation >= self.threshold and hist_idx < len(period_info):
                        period_data = period_info[hist_idx]
                        
                        high_corr_periods.append({
                            'start_date': period_data['start_date'],
                            'end_date': period_data['end_date'],
                            'avg_correlation': float(avg_correlation),
                            'stock_code': period_data['stock_code'],
                            'source': 'gpu_batch'
                        })
                
                # è®¡ç®—è¯¥è¯„æµ‹æ—¥æœŸçš„é¢„æµ‹ç»Ÿè®¡
                stats = self.calculate_future_performance_stats(self.data, high_corr_periods)
                
                detailed_results.append({
                    'evaluation_date': eval_date,
                    'high_correlation_periods': high_corr_periods,
                    'daily_high_count': len(high_corr_periods),
                    'prediction_stats': stats
                })
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        batch_results = {
            'detailed_results': detailed_results,
            'num_historical_periods': len(historical_periods_data),
            'summary': summary
        }
        
        self.logger.info("æ‰¹é‡ç»“æœå¤„ç†å®Œæˆ")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {summary.get('total_high_correlations', 0)}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {summary.get('avg_high_correlations_per_day', 0):.2f}")
        
        self.end_timer('batch_results_processing')
        return batch_results
    
    def analyze_batch(self, backtest_date=None, evaluation_days=None, window_size=None, 
                     threshold=None, comparison_mode=None, comparison_stocks=None, debug=None):
        """
        æ‰¹é‡åˆ†æä¸»å‡½æ•°
        
        Args:
            backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
            evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
            window_size: çª—å£å¤§å°
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            comparison_mode: å¯¹æ¯”æ¨¡å¼
            comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            debug: è°ƒè¯•æ¨¡å¼
            
        Returns:
            dict: æ‰¹é‡åˆ†æç»“æœ
        """
        self.start_timer('total_batch_analysis')
        
        # æ›´æ–°å‚æ•°
        if backtest_date is not None:
            self.backtest_date = pd.to_datetime(backtest_date)
        if evaluation_days is not None:
            self.evaluation_days = evaluation_days
        if window_size is not None:
            self.window_size = window_size
        if threshold is not None:
            self.threshold = threshold
        if comparison_mode is not None:
            self.comparison_mode = comparison_mode
        if comparison_stocks is not None:
            self.comparison_stocks = comparison_stocks
        if debug is not None:
            self.debug = debug
        
        self.logger.info("=" * 80)
        self.logger.info(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ")
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"å›æµ‹ç»“æŸæ—¥æœŸ: {self.backtest_date}")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {self.evaluation_days}")
        self.logger.info(f"çª—å£å¤§å°: {self.window_size}")
        self.logger.info(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {self.threshold}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {self.comparison_mode}")
        self.logger.info(f"GPUè®¾å¤‡: {self.device}")
        self.logger.info("=" * 80)
        
        # åŠ è½½æ•°æ®
        if not hasattr(self, 'data') or self.data is None:
            self.data = self.load_data()
            if self.data is None:
                self.logger.error("æ•°æ®åŠ è½½å¤±è´¥")
                return None
        
        # å‡†å¤‡è¯„æµ‹æ—¥æœŸ
        evaluation_dates = self.prepare_evaluation_dates(self.backtest_date)
        
        if not evaluation_dates:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹æ—¥æœŸ")
            return None
        
        # å‡†å¤‡æ‰¹é‡è¯„æµ‹æ•°æ®
        batch_recent_data, valid_dates = self.prepare_batch_evaluation_data(evaluation_dates)
        
        if batch_recent_data is None:
            self.logger.error("æ‰¹é‡è¯„æµ‹æ•°æ®å‡†å¤‡å¤±è´¥")
            return None
        
        # æ”¶é›†å†å²æœŸé—´æ•°æ®
        earliest_eval_date = min(valid_dates)
        historical_periods_data = self._collect_historical_periods_data(earliest_eval_date)
        
        if not historical_periods_data:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å†å²æœŸé—´æ•°æ®")
            return None
        
        # æ‰§è¡Œæ‰¹é‡GPUç›¸å…³æ€§è®¡ç®—
        batch_correlations = self.calculate_batch_gpu_correlation(batch_recent_data, historical_periods_data)
        
        if not batch_correlations:
            self.logger.error("æ‰¹é‡ç›¸å…³æ€§è®¡ç®—å¤±è´¥")
            return None
        
        # å¤„ç†æ‰¹é‡ç»“æœ
        batch_results = self.process_batch_results(batch_correlations, valid_dates, historical_periods_data)
        
        # ä¿å­˜ç»“æœæ ‡å¿—ï¼ˆæ·»åŠ ç¼ºå¤±çš„å±æ€§ï¼‰
        self.save_results = True
        
        # ä¿å­˜ç»“æœ
        if self.save_results:
            # æ„å»ºå®Œæ•´ç»“æœç”¨äºä¿å­˜
            save_result = {
                'stock_code': self.stock_code,
                'backtest_date': self.backtest_date,
                'evaluation_days': len(valid_dates),
                'window_size': self.window_size,
                'threshold': self.threshold,
                'evaluation_dates': valid_dates,
                'batch_results': batch_results
            }
            self.save_batch_results_to_csv(save_result)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            'stock_code': self.stock_code,
            'backtest_date': self.backtest_date,
            'evaluation_days': len(valid_dates),
            'window_size': self.window_size,
            'threshold': self.threshold,
            'evaluation_dates': valid_dates,
            'batch_results': batch_results,
            'performance_stats': self._get_performance_stats()
        }
        
        self.end_timer('total_batch_analysis')
        
        # è¾“å‡ºæ€§èƒ½æ€»ç»“
        self._log_performance_summary()
        
        # è¾“å‡ºåˆ†ææ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("æ‰¹é‡åˆ†æç»“æœæ€»ç»“:")
        self.logger.info(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {len(valid_dates)}")
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {batch_results['summary']['total_high_correlations']}")
        self.logger.info(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {batch_results['summary']['avg_high_correlations_per_day']:.2f}")
        self.logger.info(f"æœ€å¤§æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {batch_results['summary']['max_high_correlations_per_day']}")
        if batch_results['summary']['overall_avg_correlation'] > 0:
            self.logger.info(f"æ•´ä½“å¹³å‡ç›¸å…³ç³»æ•°: {batch_results['summary']['overall_avg_correlation']:.4f}")
        self.logger.info("=" * 80)
        
        return final_result
    
    def _collect_historical_periods_data(self, earliest_eval_date):
        """æ”¶é›†å†å²æœŸé—´æ•°æ®"""
        self.start_timer('historical_data_collection')
        
        historical_periods_data = []
        
        # æ”¶é›†è‡ªèº«å†å²æ•°æ®
        self_historical_data = self._collect_self_historical_data(earliest_eval_date)
        historical_periods_data.extend(self_historical_data)
        
        # æ”¶é›†å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        if self.comparison_mode != 'self_only':
            comparison_historical_data = self._collect_comparison_historical_data(earliest_eval_date)
            historical_periods_data.extend(comparison_historical_data)
        
        self.logger.info(f"æ”¶é›†åˆ° {len(historical_periods_data)} ä¸ªå†å²æœŸé—´æ•°æ®")
        self.end_timer('historical_data_collection')
        return historical_periods_data
    
    def _collect_self_historical_data(self, earliest_eval_date):
        """æ”¶é›†è‡ªèº«å†å²æ•°æ®"""
        historical_data = []
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸è¿›è¡Œæ—¥æœŸæˆªæ–­
        available_data = self.data
        
        if len(available_data) < self.window_size:
            return historical_data
        
        # ç”Ÿæˆå†å²æœŸé—´
        for i in range(len(available_data) - self.window_size + 1):
            period_data = available_data.iloc[i:i + self.window_size]
            start_date = period_data.index[0]
            end_date = period_data.index[-1]
            
            historical_data.append((period_data, start_date, end_date, self.stock_code))
        
        self.logger.info(f"æ”¶é›†åˆ° {len(historical_data)} ä¸ªè‡ªèº«å†å²æœŸé—´ï¼ˆåŒ…å«æ‰€æœ‰å¯ç”¨æ•°æ®ï¼‰")
        return historical_data
    
    def _collect_comparison_historical_data(self, earliest_eval_date):
        """æ”¶é›†å¯¹æ¯”è‚¡ç¥¨å†å²æ•°æ®"""
        historical_data = []
        
        for stock_code, stock_data in self.loaded_stocks_data.items():
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸è¿›è¡Œæ—¥æœŸæˆªæ–­
            available_data = stock_data
            
            if len(available_data) < self.window_size:
                continue
            
            # ç”Ÿæˆè¯¥è‚¡ç¥¨çš„å†å²æœŸé—´
            for i in range(len(available_data) - self.window_size + 1):
                period_data = available_data.iloc[i:i + self.window_size]
                start_date = period_data.index[0]
                end_date = period_data.index[-1]
                
                historical_data.append((period_data, start_date, end_date, stock_code))
        
        self.logger.info(f"æ”¶é›†åˆ° {len(historical_data)} ä¸ªå¯¹æ¯”è‚¡ç¥¨å†å²æœŸé—´ï¼ˆåŒ…å«æ‰€æœ‰å¯ç”¨æ•°æ®ï¼‰")
        return historical_data
    
    def _implement_topn_mode(self, comparison_stocks_data):
        """å®ç°TopNæ¨¡å¼"""
        max_stocks = self.max_comparison_stocks
        
        self.logger.info(f"å¯åŠ¨TopNæ¨¡å¼ï¼Œæœ€å¤§å¯¹æ¯”è‚¡ç¥¨æ•°: {max_stocks}")
        
        # å¦‚æœè‚¡ç¥¨æ•°é‡å·²ç»åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
        if len(comparison_stocks_data) <= max_stocks:
            self.logger.info(f"âœ… å½“å‰è‚¡ç¥¨æ•°é‡ {len(comparison_stocks_data)} åœ¨TopNé™åˆ¶å†…")
            return comparison_stocks_data
        
        # æ ¹æ®æ•°æ®è´¨é‡å’Œå¯ç”¨æ€§é€‰æ‹©TopNè‚¡ç¥¨
        stock_scores = []
        
        for stock_code, stock_data in comparison_stocks_data.items():
            # è®¡ç®—è‚¡ç¥¨æ•°æ®è´¨é‡åˆ†æ•°
            data_length = len(stock_data)
            data_completeness = 1.0 - (stock_data.isnull().sum().sum() / (len(stock_data) * len(stock_data.columns)))
            
            # è®¡ç®—æ•°æ®çš„æ—¶é—´è·¨åº¦
            date_range = (stock_data.index[-1] - stock_data.index[0]).days
            
            # ç»¼åˆè¯„åˆ†
            score = data_length * 0.4 + data_completeness * 0.3 + (date_range / 365) * 0.3
            
            stock_scores.append((stock_code, score, data_length))
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©TopN
        stock_scores.sort(key=lambda x: x[1], reverse=True)
        selected_stocks = stock_scores[:max_stocks]
        
        self.logger.info(f"ğŸ“Š TopNè‚¡ç¥¨é€‰æ‹©ç»“æœ:")
        for i, (stock_code, score, data_length) in enumerate(selected_stocks, 1):
            self.logger.info(f"   Top{i}: {stock_code} (è¯„åˆ†: {score:.2f}, æ•°æ®é‡: {data_length})")
        
        # æ„å»ºTopNè‚¡ç¥¨æ•°æ®å­—å…¸
        topn_data = {}
        for stock_code, score, data_length in selected_stocks:
            topn_data[stock_code] = comparison_stocks_data[stock_code]
        
        self.logger.info(f"âœ… TopNæ¨¡å¼å®Œæˆï¼Œä» {len(comparison_stocks_data)} åªè‚¡ç¥¨ä¸­é€‰æ‹©äº† {len(topn_data)} åª")
        
        return topn_data
    
    def prepare_topn_matrix_comparison(self, evaluation_dates, topn_stocks_data):
        """å‡†å¤‡TopNæ¨¡å¼çš„çŸ©é˜µæ¯”è¾ƒæ•°æ®"""
        self.logger.info(f"ğŸ”„ å‡†å¤‡TopNçŸ©é˜µæ¯”è¾ƒæ•°æ®")
        
        fields = ['open', 'high', 'low', 'close', 'volume']
        
        # ä¸ºæ¯ä¸ªè¯„æµ‹æ—¥æœŸå‡†å¤‡ç›®æ ‡è‚¡ç¥¨æ•°æ®
        target_matrices = []
        valid_eval_dates = []
        
        for eval_date in evaluation_dates:
            target_recent_data = self.data[self.data.index < eval_date].tail(self.window_size)
            
            if len(target_recent_data) == self.window_size:
                target_matrix = target_recent_data[fields].values  # [window_size, 5]
                target_matrices.append(target_matrix)
                valid_eval_dates.append(eval_date)
        
        if not target_matrices:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç›®æ ‡è‚¡ç¥¨è¯„æµ‹æ•°æ®")
            return None, None, []
        
        # è½¬æ¢ä¸ºå¼ é‡ [evaluation_days, window_size, 5]
        target_tensor = torch.tensor(np.stack(target_matrices, axis=0), dtype=torch.float32, device=self.device)
        
        # ä¸ºæ¯ä¸ªTopNè‚¡ç¥¨å‡†å¤‡åŒæ—¶æœŸæ•°æ®çŸ©é˜µ
        topn_matrices = []
        topn_stock_codes = []
        
        for stock_code, stock_data in topn_stocks_data.items():
            stock_matrices = []
            
            for eval_date in valid_eval_dates:
                # è·å–è¯¥è‚¡ç¥¨åœ¨åŒä¸€è¯„æµ‹æ—¥æœŸçš„åŒæ—¶æœŸæ•°æ®
                stock_recent_data = stock_data[stock_data.index < eval_date].tail(self.window_size)
                
                if len(stock_recent_data) == self.window_size:
                    stock_matrix = stock_recent_data[fields].values  # [window_size, 5]
                    stock_matrices.append(stock_matrix)
                else:
                    # å¦‚æœæ•°æ®ä¸è¶³ï¼Œç”¨é›¶çŸ©é˜µå¡«å……
                    stock_matrices.append(np.zeros((self.window_size, len(fields))))
            
            if stock_matrices:
                # [evaluation_days, window_size, 5]
                stock_tensor = torch.tensor(np.stack(stock_matrices, axis=0), dtype=torch.float32, device=self.device)
                topn_matrices.append(stock_tensor)
                topn_stock_codes.append(stock_code)
        
        if not topn_matrices:
            self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„TopNè‚¡ç¥¨æ•°æ®")
            return None, None, []
        
        # åˆå¹¶æ‰€æœ‰TopNè‚¡ç¥¨æ•°æ® [num_stocks, evaluation_days, window_size, 5]
        topn_tensor = torch.stack(topn_matrices, dim=0)
        
        self.logger.info(f"âœ… TopNçŸ©é˜µæ•°æ®å‡†å¤‡å®Œæˆ")
        self.logger.info(f"   ç›®æ ‡è‚¡ç¥¨æ•°æ®å½¢çŠ¶: {target_tensor.shape}")
        self.logger.info(f"   TopNè‚¡ç¥¨æ•°æ®å½¢çŠ¶: {topn_tensor.shape}")
        self.logger.info(f"   TopNè‚¡ç¥¨æ•°é‡: {len(topn_stock_codes)}")
        
        return target_tensor, topn_tensor, topn_stock_codes
    
    def calculate_topn_correlations(self, target_tensor, topn_tensor, topn_stock_codes):
        """è®¡ç®—TopNæ¨¡å¼çš„ç›¸å…³ç³»æ•°"""
        self.start_timer('topn_correlation_calculation')
        
        num_stocks, evaluation_days, window_size, num_fields = topn_tensor.shape
        
        self.logger.info(f"ğŸ”„ å¼€å§‹TopNç›¸å…³æ€§è®¡ç®—")
        self.logger.info(f"   è‚¡ç¥¨æ•°é‡: {num_stocks}")
        self.logger.info(f"   è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        
        # ç›‘æ§æ˜¾å­˜
        self.monitor_gpu_memory("TopNè®¡ç®—å¼€å§‹")
        
        all_correlations = []
        
        # å¯¹æ¯åªTopNè‚¡ç¥¨è®¡ç®—ä¸ç›®æ ‡è‚¡ç¥¨çš„ç›¸å…³æ€§
        for stock_idx, stock_code in enumerate(topn_stock_codes):
            stock_data = topn_tensor[stock_idx]  # [evaluation_days, window_size, 5]
            
            # è®¡ç®—ç›¸å…³ç³»æ•° [evaluation_days, 5]
            stock_correlations = self._compute_topn_correlation(target_tensor, stock_data)
            all_correlations.append(stock_correlations)
            
            if self.debug:
                self.logger.info(f"   å®Œæˆè‚¡ç¥¨ {stock_code} çš„ç›¸å…³æ€§è®¡ç®—")
        
        # åˆå¹¶ç»“æœ [num_stocks, evaluation_days, 5]
        correlations_tensor = torch.stack(all_correlations, dim=0)
        
        # ç›‘æ§æ˜¾å­˜
        self.monitor_gpu_memory("TopNè®¡ç®—å®Œæˆ")
        
        self.logger.info(f"âœ… TopNç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {correlations_tensor.shape}")
        
        self.end_timer('topn_correlation_calculation')
        
        return correlations_tensor
    
    def _compute_topn_correlation(self, target_data, comparison_data):
        """è®¡ç®—TopNæ¨¡å¼ä¸‹å•åªè‚¡ç¥¨çš„ç›¸å…³ç³»æ•°"""
        # target_data: [evaluation_days, window_size, 5]
        # comparison_data: [evaluation_days, window_size, 5]
        
        # è®¡ç®—å‡å€¼
        target_mean = target_data.mean(dim=1, keepdim=True)  # [evaluation_days, 1, 5]
        comparison_mean = comparison_data.mean(dim=1, keepdim=True)  # [evaluation_days, 1, 5]
        
        # ä¸­å¿ƒåŒ–
        target_centered = target_data - target_mean
        comparison_centered = comparison_data - comparison_mean
        
        # è®¡ç®—åæ–¹å·®
        covariance = (target_centered * comparison_centered).sum(dim=1)  # [evaluation_days, 5]
        
        # è®¡ç®—æ ‡å‡†å·®
        target_std = torch.sqrt((target_centered ** 2).sum(dim=1))  # [evaluation_days, 5]
        comparison_std = torch.sqrt((comparison_centered ** 2).sum(dim=1))  # [evaluation_days, 5]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = covariance / (target_std * comparison_std + 1e-8)
        
        return correlation
    
    def process_topn_results(self, correlations_tensor, topn_stock_codes, evaluation_dates):
        """å¤„ç†TopNæ¨¡å¼çš„ç»“æœ"""
        self.start_timer('topn_result_processing')
        
        num_stocks, evaluation_days, num_fields = correlations_tensor.shape
        correlations_np = correlations_tensor.cpu().numpy()
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•° [num_stocks, evaluation_days]
        avg_correlations = correlations_np.mean(axis=2)
        
        # æ„å»ºç»“æœ
        topn_results = {
            'num_stocks': num_stocks,
            'evaluation_days': evaluation_days,
            'stock_codes': topn_stock_codes,
            'avg_correlations': avg_correlations.tolist(),
            'detailed_correlations': correlations_np.tolist(),
            'high_correlation_summary': {}
        }
        
        # ç»Ÿè®¡é«˜ç›¸å…³æ€§
        total_high_correlations = 0
        daily_high_counts = []
        
        for eval_idx in range(evaluation_days):
            daily_high_count = 0
            for stock_idx in range(num_stocks):
                if avg_correlations[stock_idx, eval_idx] > self.threshold:
                    daily_high_count += 1
                    total_high_correlations += 1
            daily_high_counts.append(daily_high_count)
        
        topn_results['high_correlation_summary'] = {
            'total_high_correlations': total_high_correlations,
            'daily_high_counts': daily_high_counts,
            'avg_high_per_day': np.mean(daily_high_counts),
            'max_high_per_day': max(daily_high_counts) if daily_high_counts else 0
        }
        
        self.logger.info(f"âœ… TopNç»“æœå¤„ç†å®Œæˆ")
        self.logger.info(f"   æ€»é«˜ç›¸å…³æ€§: {total_high_correlations}")
        self.logger.info(f"   å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°: {np.mean(daily_high_counts):.2f}")
        
        self.end_timer('topn_result_processing')
        
        return topn_results
    
    def analyze_topn_mode(self, evaluation_dates):
        """æ‰§è¡ŒTopNæ¨¡å¼åˆ†æ"""
        if not self.topn_mode:
            self.logger.info("TopNæ¨¡å¼æœªå¯ç”¨ï¼Œè·³è¿‡TopNåˆ†æ")
            return None
        
        self.start_timer('topn_mode_analysis')
        
        self.logger.info("ğŸ” å¼€å§‹TopNæ¨¡å¼åˆ†æ")
        
        # å®ç°TopNè‚¡ç¥¨é€‰æ‹©
        topn_stocks_data = self._implement_topn_mode(self.loaded_stocks_data)
        
        if not topn_stocks_data:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„TopNè‚¡ç¥¨æ•°æ®")
            self.end_timer('topn_mode_analysis')
            return None
        
        # å‡†å¤‡TopNçŸ©é˜µæ¯”è¾ƒæ•°æ®
        target_tensor, topn_tensor, topn_stock_codes = self.prepare_topn_matrix_comparison(
            evaluation_dates, topn_stocks_data
        )
        
        if target_tensor is None or topn_tensor is None:
            self.logger.error("TopNçŸ©é˜µæ•°æ®å‡†å¤‡å¤±è´¥")
            self.end_timer('topn_mode_analysis')
            return None
        
        # æ£€æŸ¥æ˜¾å­˜éœ€æ±‚
        evaluation_days = target_tensor.shape[0]
        num_stocks = topn_tensor.shape[0]
        
        # ä¼°ç®—TopNæ¨¡å¼çš„æ˜¾å­˜éœ€æ±‚
        topn_memory_required = self.estimate_memory_requirement(
            evaluation_days, num_stocks, self.window_size, 5
        )
        
        # æ ¹æ®æ˜¾å­˜æƒ…å†µé€‰æ‹©å¤„ç†æ–¹å¼
        if self.check_gpu_memory_limit(topn_memory_required):
            # æ˜¾å­˜å……è¶³ï¼Œç›´æ¥è®¡ç®—
            correlations_tensor = self.calculate_topn_correlations(
                target_tensor, topn_tensor, topn_stock_codes
            )
        else:
            # æ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨è‡ªé€‚åº”å¤„ç†
            self.logger.info("ğŸ”„ TopNæ¨¡å¼æ˜¾å­˜ä¸è¶³ï¼Œå¯ç”¨è‡ªé€‚åº”å¤„ç†")
            correlations_tensor = self._adaptive_topn_processing(
                target_tensor, topn_tensor, topn_stock_codes
            )
        
        if correlations_tensor is None:
            self.logger.error("TopNç›¸å…³æ€§è®¡ç®—å¤±è´¥")
            self.end_timer('topn_mode_analysis')
            return None
        
        # å¤„ç†TopNç»“æœ
        topn_results = self.process_topn_results(
            correlations_tensor, topn_stock_codes, evaluation_dates
        )
        
        self.end_timer('topn_mode_analysis')
        
        self.logger.info("âœ… TopNæ¨¡å¼åˆ†æå®Œæˆ")
        return topn_results
    
    def _adaptive_topn_processing(self, target_tensor, topn_tensor, topn_stock_codes):
        """TopNæ¨¡å¼çš„è‡ªé€‚åº”å¤„ç†"""
        num_stocks, evaluation_days, window_size, num_fields = topn_tensor.shape
        
        # è®¡ç®—æ¯æ¬¡å¯ä»¥å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        if self.device.type == 'cuda':
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            available_memory = total_memory * self.gpu_memory_limit * 0.8
        else:
            available_memory = 4.0
        
        # ä¼°ç®—å•åªè‚¡ç¥¨çš„æ˜¾å­˜éœ€æ±‚
        single_stock_memory = self.estimate_memory_requirement(
            evaluation_days, 1, window_size, num_fields
        )
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°
        stocks_per_batch = max(1, int(available_memory / single_stock_memory))
        stocks_per_batch = min(stocks_per_batch, num_stocks)
        
        self.logger.info(f"ğŸ“¦ TopNè‡ªé€‚åº”å¤„ç†å‚æ•°:")
        self.logger.info(f"   æ€»è‚¡ç¥¨æ•°: {num_stocks}")
        self.logger.info(f"   æ¯æ‰¹è‚¡ç¥¨æ•°: {stocks_per_batch}")
        self.logger.info(f"   é¢„è®¡æ‰¹æ¬¡æ•°: {(num_stocks + stocks_per_batch - 1) // stocks_per_batch}")
        
        all_correlations = []
        
        for i in range(0, num_stocks, stocks_per_batch):
            end_idx = min(i + stocks_per_batch, num_stocks)
            batch_stocks = topn_tensor[i:end_idx]  # [batch_size, evaluation_days, window_size, 5]
            batch_codes = topn_stock_codes[i:end_idx]
            
            self.logger.info(f"ğŸ”„ å¤„ç†TopNç¬¬ {i//stocks_per_batch + 1} æ‰¹ (è‚¡ç¥¨ {i+1}-{end_idx})")
            
            # æ¸…ç†GPUç¼“å­˜
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
                gc.collect()
            
            # ç›‘æ§æ˜¾å­˜
            self.monitor_gpu_memory(f"TopNæ‰¹æ¬¡{i//stocks_per_batch + 1}å¼€å§‹")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_correlations = []
            for stock_idx, stock_code in enumerate(batch_codes):
                stock_data = batch_stocks[stock_idx]  # [evaluation_days, window_size, 5]
                stock_correlations = self._compute_topn_correlation(target_tensor, stock_data)
                batch_correlations.append(stock_correlations)
            
            # åˆå¹¶å½“å‰æ‰¹æ¬¡ç»“æœ
            batch_tensor = torch.stack(batch_correlations, dim=0)
            all_correlations.append(batch_tensor)
            
            # ç›‘æ§æ˜¾å­˜
            self.monitor_gpu_memory(f"TopNæ‰¹æ¬¡{i//stocks_per_batch + 1}å®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        if all_correlations:
            final_correlations = torch.cat(all_correlations, dim=0)
            self.logger.info("âœ… TopNè‡ªé€‚åº”å¤„ç†å®Œæˆ")
            return final_correlations
        else:
            return None
    
    def monitor_gpu_memory(self, stage_name):
        """ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == 'cuda':
            # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            current_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            current_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            
            # æ›´æ–°å³°å€¼è®°å½•
            self.gpu_memory_stats['peak_allocated'] = max(
                self.gpu_memory_stats['peak_allocated'], current_allocated
            )
            self.gpu_memory_stats['peak_reserved'] = max(
                self.gpu_memory_stats['peak_reserved'], current_reserved
            )
            
            # æ›´æ–°å½“å‰å€¼
            self.gpu_memory_stats['current_allocated'] = current_allocated
            self.gpu_memory_stats['current_reserved'] = current_reserved
            
            # è®°å½•æ—¥å¿—
            self.logger.info(f"ğŸ” GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]:")
            self.logger.info(f"   å½“å‰å·²åˆ†é…: {current_allocated:.2f}GB")
            self.logger.info(f"   å½“å‰å·²ä¿ç•™: {current_reserved:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"   å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ç‡
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            usage_rate = current_allocated / total_memory
            
            if usage_rate > 0.8:
                self.logger.warning(f"âš ï¸ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {usage_rate*100:.1f}%")
            elif usage_rate > 0.9:
                self.logger.error(f"âŒ GPUæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_rate*100:.1f}%ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º")
        else:
            self.logger.info(f"ğŸ” CPUæ¨¡å¼ï¼Œè·³è¿‡GPUæ˜¾å­˜ç›‘æ§ [{stage_name}]")
    
    def estimate_memory_requirement(self, evaluation_days, num_stocks, window_size, num_fields):
        """ä¼°ç®—æ˜¾å­˜éœ€æ±‚ï¼ˆGBï¼‰"""
        # è®¡ç®—å¼ é‡å¤§å°
        # ç›®æ ‡æ•°æ®: [evaluation_days, window_size, num_fields]
        target_size = evaluation_days * window_size * num_fields * 4  # float32 = 4 bytes
        
        # å¯¹æ¯”æ•°æ®: [num_stocks, evaluation_days, window_size, num_fields]
        comparison_size = num_stocks * evaluation_days * window_size * num_fields * 4
        
        # ç›¸å…³ç³»æ•°ç»“æœ: [num_stocks, evaluation_days, num_fields]
        correlation_size = num_stocks * evaluation_days * num_fields * 4
        
        # ä¸­é—´è®¡ç®—ç¼“å­˜ï¼ˆä¼°ç®—ä¸º2å€ï¼‰
        intermediate_size = (target_size + comparison_size) * 2
        
        # æ€»æ˜¾å­˜éœ€æ±‚
        total_bytes = target_size + comparison_size + correlation_size + intermediate_size
        total_gb = total_bytes / 1024**3
        
        self.logger.info(f"ğŸ“Š æ˜¾å­˜éœ€æ±‚ä¼°ç®—:")
        self.logger.info(f"   è¯„æµ‹æ—¥æœŸæ•°: {evaluation_days}")
        self.logger.info(f"   è‚¡ç¥¨æ•°é‡: {num_stocks}")
        self.logger.info(f"   çª—å£å¤§å°: {window_size}")
        self.logger.info(f"   é¢„è®¡æ˜¾å­˜éœ€æ±‚: {total_gb:.2f}GB")
        
        return total_gb
    
    def check_gpu_memory_limit(self, required_memory_gb):
        """æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if self.device.type != 'cuda':
            return True  # CPUæ¨¡å¼ä¸å—æ˜¾å­˜é™åˆ¶
        
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        available_memory = total_memory * self.gpu_memory_limit
        
        self.logger.info(f"ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
        self.logger.info(f"   æ€»æ˜¾å­˜: {total_memory:.2f}GB")
        self.logger.info(f"   å¯ç”¨æ˜¾å­˜: {available_memory:.2f}GB (é™åˆ¶: {self.gpu_memory_limit*100:.0f}%)")
        self.logger.info(f"   éœ€æ±‚æ˜¾å­˜: {required_memory_gb:.2f}GB")
        
        if required_memory_gb <= available_memory:
            self.logger.info(f"âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ç›´æ¥å¤„ç†")
            return True
        else:
            self.logger.warning(f"âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†")
            return False
    
    def _get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for timer_name, times in self.performance_timers.items():
            if times:
                stats[timer_name] = {
                    'total_time': sum(times),
                    'avg_time': sum(times) / len(times),
                    'max_time': max(times),
                    'min_time': min(times),
                    'count': len(times)
                }
        
        # æ·»åŠ GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            stats['gpu_memory'] = self.gpu_memory_stats.copy()
        
        return stats
    
    def _log_performance_summary(self):
        """è¾“å‡ºæ€§èƒ½æ€»ç»“"""
        self.logger.info("=" * 60)
        self.logger.info("æ€§èƒ½ç»Ÿè®¡æ€»ç»“:")
        
        for timer_name, times in self.performance_timers.items():
            if times:
                total_time = sum(times)
                avg_time = total_time / len(times)
                self.logger.info(f"  {timer_name}: æ€»è€—æ—¶={total_time:.3f}ç§’, å¹³å‡={avg_time:.3f}ç§’, æ¬¡æ•°={len(times)}")
        
        # GPUæ˜¾å­˜ç»Ÿè®¡
        if self.device.type == 'cuda':
            self.logger.info("GPUæ˜¾å­˜ç»Ÿè®¡:")
            self.logger.info(f"  å³°å€¼å·²åˆ†é…: {self.gpu_memory_stats['peak_allocated']:.2f}GB")
            self.logger.info(f"  å³°å€¼å·²ä¿ç•™: {self.gpu_memory_stats['peak_reserved']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²åˆ†é…: {self.gpu_memory_stats['current_allocated']:.2f}GB")
            self.logger.info(f"  å½“å‰å·²ä¿ç•™: {self.gpu_memory_stats['current_reserved']:.2f}GB")
        
        self.logger.info("=" * 60)
    
    def save_batch_results_to_csv(self, result):
        """ä¿å­˜æ‰¹é‡ç»“æœåˆ°CSVæ–‡ä»¶ - é€æ—¥è¯¦ç»†è®°å½•"""
        try:
            batch_results = result['batch_results']
            evaluation_dates = result['evaluation_dates']
            
            # è¯»å–ç°æœ‰CSVæ–‡ä»¶
            if os.path.exists(self.csv_results_file):
                df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
            else:
                df = pd.DataFrame()
            
            # ä¸ºæ¯ä¸ªè¯„æµ‹æ—¥æœŸåˆ›å»ºä¸€è¡Œè®°å½•
            new_rows = []
            for i, daily_result in enumerate(batch_results['detailed_results']):
                evaluation_date = evaluation_dates[i]
                prediction_stats = daily_result.get('prediction_stats', {})
                
                # è®¡ç®—å¯¹æ¯”è‚¡ç¥¨æ•°é‡
                # åœ¨self_onlyæ¨¡å¼ä¸‹ï¼Œåªå¯¹æ¯”è‡ªèº«å†å²æ•°æ®ï¼Œä¸éœ€è¦é¢å¤–åŠ 1
                # åœ¨å…¶ä»–æ¨¡å¼ä¸‹ï¼Œéœ€è¦åŠ ä¸Šç›®æ ‡è‚¡ç¥¨è‡ªèº«
                if self.comparison_mode == 'self_only':
                    comparison_stock_count = len(self.comparison_stocks)
                else:
                    comparison_stock_count = len(self.comparison_stocks) + 1
                
                # å‡†å¤‡å•æ—¥ç»“æœæ•°æ®
                row_data = {
                    'ä»£ç ': str(result['stock_code']),
                    'window_size': result['window_size'],
                    'é˜ˆå€¼': result['threshold'],
                    'è¯„æµ‹æ—¥æœŸ': evaluation_date.strftime('%Y-%m-%d'),
                    'å¯¹æ¯”è‚¡ç¥¨æ•°é‡': comparison_stock_count,
                    'ç›¸å…³æ•°é‡': daily_result.get('daily_high_count', 0),
                    'ä¸‹1æ—¥é«˜å¼€': f"{prediction_stats.get('next_day_gap_up_rate', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹1æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('next_1_day_up_rate', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹3æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('next_3_day_up_rate', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹5æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('next_5_day_up_rate', 0):.2%}" if prediction_stats else 'N/A',
                    'ä¸‹10æ—¥ä¸Šæ¶¨': f"{prediction_stats.get('next_10_day_up_rate', 0):.2%}" if prediction_stats else 'N/A'
                }
                new_rows.append(row_data)
            
            # æ·»åŠ æ‰€æœ‰æ–°è¡Œ
            if new_rows:
                new_df = pd.DataFrame(new_rows)
                df = pd.concat([df, new_df], ignore_index=True)
                
                # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
                df['ä»£ç '] = df['ä»£ç '].astype(str)
                
                # æŒ‰è¯„æµ‹æ—¥æœŸé™åºæ’åˆ—ï¼ˆæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
                df['è¯„æµ‹æ—¥æœŸ_æ’åº'] = pd.to_datetime(df['è¯„æµ‹æ—¥æœŸ'])
                df = df.sort_values('è¯„æµ‹æ—¥æœŸ_æ’åº', ascending=False)
                df = df.drop('è¯„æµ‹æ—¥æœŸ_æ’åº', axis=1)  # åˆ é™¤ä¸´æ—¶æ’åºåˆ—
                df = df.reset_index(drop=True)  # é‡ç½®ç´¢å¼•
                
                # ä¿å­˜CSVæ–‡ä»¶
                df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
                
                self.logger.info(f"âœ… æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
                self.logger.info(f"âœ… å…±ä¿å­˜ {len(new_rows)} æ¡é€æ—¥è¯„æµ‹è®°å½•")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœéœ€è¦ä¿å­˜")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")


def analyze_pearson_correlation_gpu_batch(stock_code, backtest_date=None, evaluation_days=100, 
                                         window_size=15, threshold=0.9, comparison_mode='default', 
                                         comparison_stocks=None, debug=False, csv_filename=None, 
                                         use_gpu=True, batch_size=1000):
    """
    GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æçš„ä¾¿æ·å‡½æ•°
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        backtest_date: å›æµ‹ç»“æŸæ—¥æœŸ
        evaluation_days: è¯„æµ‹æ—¥æœŸæ•°é‡
        window_size: çª—å£å¤§å°
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
        comparison_mode: å¯¹æ¯”æ¨¡å¼
        comparison_stocks: å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        debug: è°ƒè¯•æ¨¡å¼
        csv_filename: CSVæ–‡ä»¶å
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        batch_size: æ‰¹å¤„ç†å¤§å°
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    if backtest_date is None:
        backtest_date = datetime.now().strftime('%Y-%m-%d')
    
    if csv_filename is None:
        csv_filename = 'batch_evaluation_results.csv'
    
    analyzer = GPUBatchPearsonAnalyzer(
        stock_code=stock_code,
        window_size=window_size,
        threshold=threshold,
        evaluation_days=evaluation_days,
        debug=debug,
        comparison_stocks=comparison_stocks,
        comparison_mode=comparison_mode,
        backtest_date=backtest_date,
        csv_filename=csv_filename,
        use_gpu=use_gpu,
        batch_size=batch_size
    )
    
    result = analyzer.analyze_batch()
    
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='GPUæ‰¹é‡è¯„æµ‹Pearsonç›¸å…³æ€§åˆ†æ')
    parser.add_argument('--stock_code', type=str, required=True, help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--backtest_date', type=str, help='å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--evaluation_days', type=int, default=100, help='è¯„æµ‹æ—¥æœŸæ•°é‡')
    parser.add_argument('--window_size', type=int, default=15, help='åˆ†æçª—å£å¤§å°')
    parser.add_argument('--threshold', type=float, default=0.9, help='ç›¸å…³ç³»æ•°é˜ˆå€¼')
    parser.add_argument('--comparison_mode', type=str, default='top10', 
                       choices=['top10', 'industry', 'self_only'],
                       help='å¯¹æ¯”æ¨¡å¼: top10(å¸‚å€¼å‰10), industry(è¡Œä¸šè‚¡ç¥¨), self_only(ä»…è‡ªèº«å†å²)')
    parser.add_argument('--debug', action='store_true', help='å¼€å¯è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--csv_filename', type=str, default='evaluation_results.csv', help='CSVç»“æœæ–‡ä»¶å')
    parser.add_argument('--use_gpu', action='store_true', default=True, help='ä½¿ç”¨GPUåŠ é€Ÿ')
    parser.add_argument('--batch_size', type=int, default=1000, 
                       help='GPUæ‰¹å¤„ç†å¤§å° - æ§åˆ¶å•æ¬¡GPUè®¡ç®—çš„æ•°æ®é‡ï¼Œå½±å“å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚'
                            'æ¨èå€¼ï¼šRTX 3060(8GB)=500-1000, RTX 3080(10GB)=1000-2000, RTX 4090(24GB)=2000-5000')
    
    args = parser.parse_args()
    
    print(f"å¼€å§‹GPUæ‰¹é‡è¯„æµ‹åˆ†æï¼Œè‚¡ç¥¨ä»£ç : {args.stock_code}")
    print(f"è¯„æµ‹æ—¥æœŸæ•°é‡: {args.evaluation_days}")
    print(f"çª—å£å¤§å°: {args.window_size}")
    print(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {args.threshold}")
    
    result = analyze_pearson_correlation_gpu_batch(
        stock_code=args.stock_code,
        backtest_date=args.backtest_date,
        evaluation_days=args.evaluation_days,
        window_size=args.window_size,
        threshold=args.threshold,
        comparison_mode=args.comparison_mode,
        debug=args.debug,
        csv_filename=args.csv_filename,
        use_gpu=args.use_gpu,
        batch_size=args.batch_size
    )
    
    if result:
        print(f"åˆ†æå®Œæˆï¼Œè¯„æµ‹äº† {result['evaluation_days']} ä¸ªæ—¥æœŸ")
        print(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´: {result['batch_results']['summary']['total_high_correlations']}")
        print(f"å¹³å‡æ¯æ—¥é«˜ç›¸å…³æ•°é‡: {result['batch_results']['summary']['avg_high_correlations_per_day']:.2f}")
    else:
        print("åˆ†æå¤±è´¥")