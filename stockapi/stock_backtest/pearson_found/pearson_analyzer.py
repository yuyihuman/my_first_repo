"""
è‚¡ç¥¨æ•°æ®Pearsonç›¸å…³ç³»æ•°åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆ†æè‚¡ç¥¨æ•°æ®çš„Pearsonç›¸å…³ç³»æ•°ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£çš„æ–¹å¼
è®¡ç®—æœ€è¿‘20ä¸ªäº¤æ˜“æ—¥ä¸å†å²20ä¸ªäº¤æ˜“æ—¥æ•°æ®çš„ç›¸å…³æ€§ã€‚

åŠŸèƒ½ï¼š
1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ä¼ å…¥è‚¡ç¥¨ä»£ç 
2. åŠ è½½è‚¡ç¥¨å†å²æ•°æ®
3. è®¡ç®—å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æˆäº¤é‡çš„Pearsonç›¸å…³ç³»æ•°
4. æ‰¾å‡ºç›¸å…³ç³»æ•°å¤§äº0.90çš„æ•°æ®
5. å°†ç»“æœè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
python pearson_analyzer.py --stock_code 000001

ä½œè€…ï¼šStock Backtest System
åˆ›å»ºæ—¶é—´ï¼š2024å¹´
"""

import argparse
import logging
import os
from datetime import datetime
import numpy as np
from scipy.stats import pearsonr
from data_loader import StockDataLoader
import matplotlib.pyplot as plt
import mplfinance as mpf
import pandas as pd
from stock_config import get_comparison_stocks
import time
import threading
from collections import defaultdict


class PearsonAnalyzer:
    def __init__(self, stock_code, log_dir='logs', window_size=15, threshold=0.85, debug=False, 
                 comparison_stocks=None, comparison_mode='default', backtest_date=None, csv_filename='evaluation_results.csv',
                 earliest_date='2020-01-01'):
        """
        åˆå§‹åŒ–Pearsonç›¸å…³æ€§åˆ†æå™¨
        
        Args:
            stock_code: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            log_dir: æ—¥å¿—ç›®å½•
            window_size: åˆ†æçª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥æ•°é‡ï¼‰
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼
            debug: æ˜¯å¦å¼€å¯debugæ¨¡å¼ï¼ˆå½±å“æ€§èƒ½ï¼‰
            comparison_stocks: è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
            comparison_mode: å¯¹æ¯”æ¨¡å¼ ('default', 'top10', 'banks', 'tech', 'new_energy', 'healthcare', 'consumer', 'self_only')
            backtest_date: å›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä»è¯¥æ—¥æœŸå¾€å‰æ•°è·å–æ•°æ®æ®µè¿›è¡Œåˆ†æ
            csv_filename: CSVç»“æœæ–‡ä»¶å (é»˜è®¤: evaluation_results.csv)
            earliest_date: æ•°æ®è·å–çš„æœ€æ—©æ—¥æœŸé™åˆ¶ (æ ¼å¼: YYYY-MM-DDï¼Œé»˜è®¤: 2020-01-01)
        """
        self.stock_code = stock_code
        
        # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
        script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
        self.log_dir = os.path.join(script_dir, 'logs')
        self.csv_results_file = os.path.join(script_dir, csv_filename)
        
        self.window_size = window_size
        self.threshold = threshold
        self.debug = debug
        self.comparison_mode = comparison_mode
        self.backtest_date = backtest_date
        self.earliest_date = pd.to_datetime(earliest_date)
        self.data_loader = None
        self.logger = None
        
        # è®¾ç½®å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨
        if comparison_stocks:
            self.comparison_stocks = comparison_stocks
        elif comparison_mode == 'self_only':
            self.comparison_stocks = [stock_code]  # åªå¯¹æ¯”è‡ªå·±çš„å†å²æ•°æ®
        else:
            self.comparison_stocks = get_comparison_stocks(comparison_mode)
            # ç¡®ä¿ç›®æ ‡è‚¡ç¥¨ä¸åœ¨å¯¹æ¯”åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
            if stock_code in self.comparison_stocks:
                self.comparison_stocks.remove(stock_code)
        
        # å­˜å‚¨å·²åŠ è½½çš„è‚¡ç¥¨æ•°æ®
        self.loaded_stocks_data = {}
        
        # æ€§èƒ½è®¡æ—¶å™¨
        self.performance_timers = defaultdict(list)
        self.current_timers = {}
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # è®¾ç½®CSVæ–‡ä»¶
        self._setup_csv_file()
        
        self.logger.info(f"åˆå§‹åŒ–Pearsonåˆ†æå™¨ï¼Œç›®æ ‡è‚¡ç¥¨: {stock_code}")
        self.logger.info(f"çª—å£å¤§å°: {window_size}, é˜ˆå€¼: {threshold}, Debugæ¨¡å¼: {debug}")
        self.logger.info(f"å¯¹æ¯”æ¨¡å¼: {comparison_mode}, å¯¹æ¯”è‚¡ç¥¨æ•°é‡: {len(self.comparison_stocks)}")
        if self.debug:
            self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨: {self.comparison_stocks[:10]}{'...' if len(self.comparison_stocks) > 10 else ''}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ›å»ºå­æ–‡ä»¶å¤¹
        stock_log_dir = os.path.join(self.log_dir, self.stock_code)
        os.makedirs(stock_log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        thread_id = threading.get_ident()
        log_filename = f"pearson_analysis_{self.stock_code}_{timestamp}_thread_{thread_id}.log"
        log_path = os.path.join(stock_log_dir, log_filename)
        
        # åˆ›å»ºlogger
        self.logger = logging.getLogger(f'PearsonAnalyzer_{self.stock_code}')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶handler
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºformatter
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ handleråˆ°logger
        self.logger.addHandler(file_handler)
        
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_path}")
    
    def _setup_csv_file(self):
        """è®¾ç½®CSVæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(self.csv_results_file):
            if self.debug:
                self.logger.info(f"ğŸ†• Debug: CSVç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {self.csv_results_file}")
            
            # åˆ›å»ºCSVæ–‡ä»¶çš„è¡¨å¤´
            header = ['ä»£ç ', 'window_size', 'é˜ˆå€¼', 'è¯„æµ‹æ—¥æœŸ', 'å¯¹æ¯”è‚¡ç¥¨æ•°é‡', 'ç›¸å…³æ•°é‡', 'ä¸‹1æ—¥é«˜å¼€', 'ä¸‹1æ—¥ä¸Šæ¶¨', 'ä¸‹3æ—¥ä¸Šæ¶¨', 'ä¸‹5æ—¥ä¸Šæ¶¨', 'ä¸‹10æ—¥ä¸Šæ¶¨']
            df = pd.DataFrame(columns=header)
            # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"ğŸ†• Debug: CSVæ–‡ä»¶åˆ›å»ºå®Œæˆï¼Œè¡¨å¤´: {header}")
                self.logger.info(f"ğŸ†• Debug: åˆå§‹æ–‡ä»¶å¤§å°: {file_size} bytes")
        else:
            if self.debug:
                file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"âœ… Debug: CSVç»“æœæ–‡ä»¶å·²å­˜åœ¨: {self.csv_results_file}")
                self.logger.info(f"âœ… Debug: ç°æœ‰æ–‡ä»¶å¤§å°: {file_size} bytes")
    
    def save_evaluation_result(self, evaluation_date, stats, correlation_count=0):
        """
        ä¿å­˜å•æ¬¡è¯„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
        
        Args:
            evaluation_date: è¯„æµ‹æ—¥æœŸï¼ˆè¯„æµ‹åºåˆ—çš„æœ€åä¸€å¤©ï¼‰
            stats: ç»Ÿè®¡ç»“æœå­—å…¸
            correlation_count: ç›¸å…³æ•°é‡ï¼ˆé«˜ç›¸å…³æ€§æœŸé—´çš„æ•°é‡ï¼‰
        """
        try:
            # è®¡ç®—å¯¹æ¯”è‚¡ç¥¨æ•°é‡
            # åœ¨self_onlyæ¨¡å¼ä¸‹ï¼Œåªå¯¹æ¯”è‡ªèº«å†å²æ•°æ®ï¼Œä¸éœ€è¦é¢å¤–åŠ 1
            # åœ¨å…¶ä»–æ¨¡å¼ä¸‹ï¼Œéœ€è¦åŠ ä¸Šç›®æ ‡è‚¡ç¥¨è‡ªèº«
            if self.comparison_mode == 'self_only':
                comparison_stock_count = len(self.comparison_stocks)
            else:
                comparison_stock_count = len(self.comparison_stocks) + 1
            
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
            result_data = {
                'ä»£ç ': str(self.stock_code),  # ç¡®ä¿è‚¡ç¥¨ä»£ç ä¸ºå­—ç¬¦ä¸²
                'window_size': self.window_size,
                'é˜ˆå€¼': self.threshold,
                'è¯„æµ‹æ—¥æœŸ': evaluation_date.strftime('%Y-%m-%d'),
                'å¯¹æ¯”è‚¡ç¥¨æ•°é‡': comparison_stock_count,
                'ç›¸å…³æ•°é‡': correlation_count,
                'ä¸‹1æ—¥é«˜å¼€': f"{stats['ratios']['next_day_gap_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹1æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_1_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹3æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_3_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹5æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_5_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A',
                'ä¸‹10æ—¥ä¸Šæ¶¨': f"{stats['ratios']['next_10_day_up']:.2%}" if stats and 'ratios' in stats else 'N/A'
            }
            
            # è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶ï¼ŒæŒ‡å®šä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            if os.path.exists(self.csv_results_file):
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: å¼€å§‹è¯»å–ç°æœ‰CSVæ–‡ä»¶: {self.csv_results_file}")
                    file_size = os.path.getsize(self.csv_results_file)
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶å¤§å°: {file_size} bytes")
                
                df = pd.read_csv(self.csv_results_file, encoding='utf-8-sig', dtype={'ä»£ç ': str})
                
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶è¯»å–å®Œæˆï¼Œå…± {len(df)} è¡Œæ•°æ®")
                    if not df.empty:
                        self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
                        self.logger.info(f"ğŸ“– Debug: æœ€åä¸€æ¡è®°å½•çš„è‚¡ç¥¨ä»£ç : {df.iloc[-1]['ä»£ç '] if 'ä»£ç ' in df.columns else 'N/A'}")
            else:
                if self.debug:
                    self.logger.info(f"ğŸ“– Debug: CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„DataFrame: {self.csv_results_file}")
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„DataFrame
                df = pd.DataFrame()
            
            # æ·»åŠ æ–°çš„ç»“æœè¡Œ
            new_row = pd.DataFrame([result_data])
            df = pd.concat([df, new_row], ignore_index=True)
            
            # ç¡®ä¿ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df['ä»£ç '] = df['ä»£ç '].astype(str)
            
            if self.debug:
                self.logger.info(f"ğŸ’¾ Debug: å‡†å¤‡ä¿å­˜CSVæ–‡ä»¶ï¼Œå½“å‰DataFrameå…± {len(df)} è¡Œæ•°æ®")
                self.logger.info(f"ğŸ’¾ Debug: æ–°å¢æ•°æ®: {result_data}")
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            df.to_csv(self.csv_results_file, index=False, encoding='utf-8-sig')
            
            if self.debug:
                # éªŒè¯ä¿å­˜åçš„æ–‡ä»¶
                saved_file_size = os.path.getsize(self.csv_results_file)
                self.logger.info(f"ğŸ’¾ Debug: CSVæ–‡ä»¶ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {saved_file_size} bytes")
            
            self.logger.info(f"è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {self.csv_results_file}")
            self.logger.info(f"ä¿å­˜çš„ç»“æœ: {result_data}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def start_timer(self, timer_name):
        """å¼€å§‹è®¡æ—¶"""
        self.current_timers[timer_name] = time.time()
        if self.debug:
            self.logger.info(f"â±ï¸ å¼€å§‹è®¡æ—¶: {timer_name}")
    
    def end_timer(self, timer_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if timer_name in self.current_timers:
            elapsed_time = time.time() - self.current_timers[timer_name]
            self.performance_timers[timer_name].append(elapsed_time)
            del self.current_timers[timer_name]
            if self.debug:
                self.logger.info(f"â±ï¸ ç»“æŸè®¡æ—¶: {timer_name} - è€—æ—¶: {elapsed_time:.3f}ç§’")
            return elapsed_time
        return 0
    
    def get_timer_stats(self, timer_name):
        """è·å–è®¡æ—¶å™¨ç»Ÿè®¡ä¿¡æ¯"""
        times = self.performance_timers[timer_name]
        if not times:
            return None
        return {
            'count': len(times),
            'total': sum(times),
            'average': sum(times) / len(times),
            'min': min(times),
            'max': max(times)
        }
    
    def log_performance_summary(self):
        """è¾“å‡ºæ€§èƒ½ç»Ÿè®¡è¡¨"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        self.logger.info("=" * 80)
        
        # è®¡ç®—æ€»è€—æ—¶
        total_analysis_time = sum(self.performance_timers.get('total_analysis', [0]))
        
        # åˆ›å»ºç»Ÿè®¡è¡¨
        stats_table = []
        stats_table.append(f"{'é˜¶æ®µ':<25} {'æ¬¡æ•°':<8} {'æ€»è€—æ—¶(ç§’)':<12} {'å¹³å‡è€—æ—¶(ç§’)':<15} {'æœ€å°è€—æ—¶(ç§’)':<15} {'æœ€å¤§è€—æ—¶(ç§’)':<15}")
        stats_table.append("-" * 90)
        
        # å®šä¹‰å…³é”®é˜¶æ®µçš„æ˜¾ç¤ºé¡ºåºå’Œä¸­æ–‡åç§°
        stage_names = {
            'total_analysis': 'æ€»åˆ†ææ—¶é—´',
            'data_loading': 'æ•°æ®åŠ è½½',
            'target_stock_loading': 'ç›®æ ‡è‚¡ç¥¨æ•°æ®åŠ è½½',
            'comparison_stocks_loading': 'å¯¹æ¯”è‚¡ç¥¨æ•°æ®åŠ è½½',
            'self_analysis': 'è‡ªèº«å†å²æ•°æ®åˆ†æ',
            'comparison_analysis': 'è·¨è‚¡ç¥¨å¯¹æ¯”åˆ†æ',
            'correlation_calculation': 'ç›¸å…³æ€§è®¡ç®—',
            'plotting': 'Kçº¿å›¾ç»˜åˆ¶',
            'stats_calculation': 'ç»Ÿè®¡è®¡ç®—'
        }
        
        for timer_name, display_name in stage_names.items():
            stats = self.get_timer_stats(timer_name)
            if stats:
                stats_table.append(
                    f"{display_name:<25} {stats['count']:<8} {stats['total']:<12.3f} "
                    f"{stats['average']:<15.3f} {stats['min']:<15.3f} {stats['max']:<15.3f}"
                )
        
        # è¾“å‡ºç»Ÿè®¡è¡¨
        for line in stats_table:
            self.logger.info(line)
        
        # è¾“å‡ºæ€§èƒ½åˆ†æ
        self.logger.info("-" * 90)
        if total_analysis_time > 0:
            data_loading_time = sum(self.performance_timers.get('data_loading', [0]))
            analysis_time = sum(self.performance_timers.get('self_analysis', [0])) + sum(self.performance_timers.get('comparison_analysis', [0]))
            plotting_time = sum(self.performance_timers.get('plotting', [0]))
            
            self.logger.info(f"ğŸ“Š æ€§èƒ½åˆ†æ:")
            self.logger.info(f"   æ•°æ®åŠ è½½å æ¯”: {(data_loading_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   åˆ†æè®¡ç®—å æ¯”: {(analysis_time/total_analysis_time)*100:.1f}%")
            self.logger.info(f"   å›¾è¡¨ç»˜åˆ¶å æ¯”: {(plotting_time/total_analysis_time)*100:.1f}%")
        
        self.logger.info("=" * 80)
    
    def load_data(self):
        """åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®"""
        self.start_timer('target_stock_loading')
        self.logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨")
        self.data_loader = StockDataLoader()
        
        self.logger.info(f"å¼€å§‹åŠ è½½ç›®æ ‡è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
        data = self.data_loader.load_stock_data(self.stock_code)
        
        if data is None or data.empty:
            self.logger.error(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {self.stock_code} çš„æ•°æ®")
            self.end_timer('target_stock_loading')
            return None
        
        # æ•°æ®è¿‡æ»¤ï¼šç¡®ä¿ä»·æ ¼ä¸ºæ­£æ•°ï¼Œæˆäº¤é‡å¤§äº0
        self.data = self._filter_data(data, self.stock_code)
        self.end_timer('target_stock_loading')
        
        # åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®
        self._load_comparison_stocks_data()
        
        return self.data
    
    def _filter_data(self, data, stock_code):
        """è¿‡æ»¤è‚¡ç¥¨æ•°æ®ï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œæ—¥æœŸèŒƒå›´"""
        if data is None or data.empty:
            return data
            
        original_count = len(data)
        
        # é¦–å…ˆæŒ‰æ—¥æœŸè¿‡æ»¤
        data = data[data.index >= self.earliest_date]
        date_filtered_count = len(data)
        date_removed_count = original_count - date_filtered_count
        
        # ç„¶åæŒ‰æ•°æ®è´¨é‡è¿‡æ»¤
        data = data[
            (data['open'] > 0) & 
            (data['high'] > 0) & 
            (data['low'] > 0) & 
            (data['close'] > 0) & 
            (data['volume'] > 0)
        ]
        final_count = len(data)
        quality_removed_count = date_filtered_count - final_count
        
        if date_removed_count > 0:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æ—¥æœŸè¿‡æ»¤å®Œæˆï¼Œç§»é™¤æ—©äº {self.earliest_date.strftime('%Y-%m-%d')} çš„ {date_removed_count} æ¡æ•°æ®")
        
        if quality_removed_count > 0:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®è´¨é‡è¿‡æ»¤å®Œæˆï¼Œç§»é™¤ {quality_removed_count} æ¡å¼‚å¸¸æ•°æ®")
        
        if not data.empty:
            self.logger.info(f"è‚¡ç¥¨ {stock_code} æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        
        return data
    
    def _load_comparison_stocks_data(self):
        """åŠ è½½å¯¹æ¯”è‚¡ç¥¨æ•°æ®"""
        if self.comparison_mode == 'self_only':
            self.logger.info("ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”æ¨¡å¼ï¼Œè·³è¿‡å…¶ä»–è‚¡ç¥¨æ•°æ®åŠ è½½")
            return
        
        self.start_timer('comparison_stocks_loading')
        self.logger.info(f"å¼€å§‹åŠ è½½ {len(self.comparison_stocks)} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        successful_loads = 0
        
        for stock_code in self.comparison_stocks:
            try:
                if self.debug:
                    self.logger.info(f"æ­£åœ¨åŠ è½½å¯¹æ¯”è‚¡ç¥¨: {stock_code}")
                
                data = self.data_loader.load_stock_data(stock_code)
                if data is not None and not data.empty:
                    # è¿‡æ»¤æ•°æ®
                    filtered_data = self._filter_data(data, stock_code)
                    if not filtered_data.empty:
                        self.loaded_stocks_data[stock_code] = filtered_data
                        successful_loads += 1
                    else:
                        if self.debug:
                            self.logger.warning(f"è‚¡ç¥¨ {stock_code} è¿‡æ»¤åæ•°æ®ä¸ºç©º")
                else:
                    if self.debug:
                        self.logger.warning(f"æ— æ³•åŠ è½½è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
                        
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        self.logger.info(f"æˆåŠŸåŠ è½½ {successful_loads} åªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®")
        if successful_loads == 0:
            self.logger.warning("æœªèƒ½åŠ è½½ä»»ä½•å¯¹æ¯”è‚¡ç¥¨æ•°æ®ï¼Œå°†ä½¿ç”¨è‡ªèº«å†å²æ•°æ®å¯¹æ¯”")
        self.end_timer('comparison_stocks_loading')
    
    def plot_kline_comparison(self, recent_data, historical_data, correlation_info):
        """
        ç»˜åˆ¶æœ€è¿‘æ•°æ®å’Œå†å²é«˜ç›¸å…³æ€§æ•°æ®çš„Kçº¿å›¾å¯¹æ¯”
        
        Args:
            recent_data: æœ€è¿‘çš„æ•°æ®
            historical_data: å†å²é«˜ç›¸å…³æ€§æ•°æ®
            correlation_info: ç›¸å…³æ€§ä¿¡æ¯å­—å…¸
        """
        self.start_timer('plotting')
        try:
            # åˆ›å»ºå›¾è¡¨ç›®å½•ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç ç»„ç»‡ï¼‰
            chart_dir = os.path.join(self.log_dir, self.stock_code, 'charts')
            os.makedirs(chart_dir, exist_ok=True)
            
            # å‡†å¤‡æ•°æ® - ç¡®ä¿åˆ—åç¬¦åˆmplfinanceè¦æ±‚
            recent_df = recent_data[['open', 'high', 'low', 'close', 'volume']].copy()
            recent_df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            
            historical_df = historical_data[['open', 'high', 'low', 'close', 'volume']].copy()
            historical_df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            
            # è®¾ç½®å›¾è¡¨æ ·å¼
            mc = mpf.make_marketcolors(up='red', down='green', edge='inherit',
                                     wick={'up':'red', 'down':'green'},
                                     volume='in')
            s = mpf.make_mpf_style(marketcolors=mc, gridstyle='-', y_on_right=False)
            
            # è·å–æ—¥æœŸä¿¡æ¯ç”¨äºå¯¹æ¯”å›¾
            historical_start = correlation_info['start_date'].strftime('%Y-%m-%d')
            historical_end = correlation_info['end_date'].strftime('%Y-%m-%d')
            recent_start = recent_data.index[0].strftime('%Y-%m-%d')
            recent_end = recent_data.index[-1].strftime('%Y-%m-%d')
            
            # è·å–æ¥æºè‚¡ç¥¨ä¿¡æ¯
            source_stock = correlation_info.get('source_stock', self.stock_code)
            source_label = f"Stock {source_stock}" if source_stock != self.stock_code else f"Target Stock {self.stock_code}"
            
            # ç»˜åˆ¶Kçº¿å¯¹æ¯”å›¾ï¼ˆä¸¤ä¸ªå­å›¾ä¸Šä¸‹æ’åˆ—ï¼ŒåŒ…å«æˆäº¤é‡ï¼‰
            # åˆ›å»ºä¸€ä¸ªåŒ…å«å››ä¸ªå­å›¾çš„å›¾å½¢ï¼ˆä»·æ ¼+æˆäº¤é‡å„ä¸¤ä¸ªï¼‰
            fig = plt.figure(figsize=(14, 12))
            
            # ä¸Šæ–¹ - å†å²æ•°æ®ï¼ˆä»·æ ¼å’Œæˆäº¤é‡ï¼‰
            ax1 = plt.subplot(4, 1, 1)
            ax1_vol = plt.subplot(4, 1, 2)
            mpf.plot(historical_df,
                    type='candle',
                    style=s,
                    ax=ax1,
                    volume=ax1_vol,
                    warn_too_much_data=10000)
            ax1.set_title(f'Historical High Correlation Period - {source_label}\n({historical_start} to {historical_end}) | Avg Correlation: {correlation_info["avg_correlation"]:.4f}')
            ax1.set_ylabel('Price')
            ax1_vol.set_ylabel('Volume')
            
            # ä¸‹æ–¹ - æœ€è¿‘æ•°æ®ï¼ˆä»·æ ¼å’Œæˆäº¤é‡ï¼‰
            ax2 = plt.subplot(4, 1, 3)
            ax2_vol = plt.subplot(4, 1, 4)
            mpf.plot(recent_df,
                    type='candle',
                    style=s,
                    ax=ax2,
                    volume=ax2_vol,
                    warn_too_much_data=10000)
            ax2.set_title(f'Recent Trading Period - Target Stock {self.stock_code}\n({recent_start} to {recent_end})')
            ax2.set_ylabel('Price')
            ax2_vol.set_ylabel('Volume')
            
            plt.tight_layout()
            comparison_file = os.path.join(chart_dir, f'kline_comparison_{self.stock_code}.png')
            plt.savefig(comparison_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Kçº¿å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_file}")
            self.end_timer('plotting')
            
        except Exception as e:
            self.logger.error(f"ç»˜åˆ¶Kçº¿å›¾æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.end_timer('plotting')
    
    def calculate_pearson_correlation_vectorized(self, recent_data, historical_data):
        """
        å‘é‡åŒ–è®¡ç®—Pearsonç›¸å…³ç³»æ•° - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            recent_data: æœ€è¿‘çš„æ•°æ® (numpy array or DataFrame)
            historical_data: å†å²æ•°æ® (numpy array or DataFrame)
            
        Returns:
            tuple: (å¹³å‡ç›¸å…³ç³»æ•°, å„å­—æ®µç›¸å…³ç³»æ•°å­—å…¸)
        """
        fields = ['open', 'high', 'low', 'close', 'volume']
        correlations = {}
        
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥æé«˜æ€§èƒ½
            if hasattr(recent_data, 'values'):
                recent_values = recent_data[fields].values
            else:
                recent_values = recent_data
                
            if hasattr(historical_data, 'values'):
                historical_values = historical_data[fields].values
            else:
                historical_values = historical_data
            
            # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰å­—æ®µçš„ç›¸å…³ç³»æ•°
            correlations_matrix = np.corrcoef(recent_values.T, historical_values.T)
            
            # æå–å¯¹è§’çº¿ä¸Šçš„ç›¸å…³ç³»æ•°ï¼ˆrecent vs historical for each fieldï¼‰
            n_fields = len(fields)
            field_correlations = np.diag(correlations_matrix[:n_fields, n_fields:])
            
            # æ„å»ºç»“æœå­—å…¸
            for i, field in enumerate(fields):
                corr_coef = field_correlations[i]
                if np.isnan(corr_coef) or np.isinf(corr_coef):
                    correlations[field] = {'correlation': np.nan, 'p_value': np.nan}
                else:
                    # å¯¹äºå‘é‡åŒ–ç‰ˆæœ¬ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸è®¡ç®—p_valueä»¥æé«˜æ€§èƒ½
                    # å¦‚æœéœ€è¦p_valueï¼Œå¯ä»¥åœ¨å¿…è¦æ—¶å•ç‹¬è®¡ç®—
                    correlations[field] = {'correlation': corr_coef, 'p_value': np.nan}
            
            # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°ï¼ˆå¿½ç•¥NaNå€¼ï¼‰
            valid_correlations = [corr['correlation'] for corr in correlations.values() 
                                if not np.isnan(corr['correlation'])]
            avg_correlation = np.mean(valid_correlations) if valid_correlations else 0
            
            return avg_correlation, correlations
            
        except Exception as e:
            if self.debug:
                self.logger.warning(f"å‘é‡åŒ–ç›¸å…³ç³»æ•°è®¡ç®—å‡ºé”™ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            return self.calculate_pearson_correlation_original(recent_data, historical_data)
    
    def calculate_pearson_correlation_original(self, recent_data, historical_data):
        """
        åŸå§‹çš„Pearsonç›¸å…³ç³»æ•°è®¡ç®—æ–¹æ³•ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        
        Args:
            recent_data: æœ€è¿‘çš„æ•°æ®
            historical_data: å†å²æ•°æ®
            
        Returns:
            tuple: (å¹³å‡ç›¸å…³ç³»æ•°, å„å­—æ®µç›¸å…³ç³»æ•°å­—å…¸)
        """
        fields = ['open', 'high', 'low', 'close', 'volume']
        correlations = {}
        
        for field in fields:
            try:
                corr_coef, p_value = pearsonr(recent_data[field], historical_data[field])
                correlations[field] = {'correlation': corr_coef, 'p_value': p_value}
            except Exception as e:
                if self.debug:
                    self.logger.warning(f"è®¡ç®— {field} ç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}")
                correlations[field] = {'correlation': np.nan, 'p_value': np.nan}
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°ï¼ˆå¿½ç•¥NaNå€¼ï¼‰
        valid_correlations = [corr['correlation'] for corr in correlations.values() 
                            if not np.isnan(corr['correlation'])]
        avg_correlation = np.mean(valid_correlations) if valid_correlations else 0
        
        return avg_correlation, correlations
    
    def calculate_pearson_correlation(self, recent_data, historical_data):
        """
        è®¡ç®—Pearsonç›¸å…³ç³»æ•° - ä½¿ç”¨ä¼˜åŒ–åçš„å‘é‡åŒ–æ–¹æ³•
        """
        return self.calculate_pearson_correlation_vectorized(recent_data, historical_data)
    
    def calculate_future_performance_stats(self, data, high_correlation_periods):
        """
        è®¡ç®—é«˜ç›¸å…³æ€§æœŸé—´çš„æœªæ¥äº¤æ˜“æ—¥è¡¨ç°ç»Ÿè®¡
        
        Args:
            data: å®Œæ•´çš„è‚¡ç¥¨æ•°æ®
            high_correlation_periods: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
            
        Returns:
            dict: ç»Ÿè®¡ç»“æœ
        """
        if not high_correlation_periods:
            return None
        
        stats = {
            'total_periods': len(high_correlation_periods),
            'next_day_gap_up': 0,  # ä¸‹1ä¸ªäº¤æ˜“æ—¥é«˜å¼€
            'next_1_day_up': 0,    # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_3_day_up': 0,    # ä¸‹3ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_5_day_up': 0,    # ä¸‹5ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'next_10_day_up': 0,   # ä¸‹10ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨
            'valid_periods': {
                'next_day': 0,
                'next_3_day': 0,
                'next_5_day': 0,
                'next_10_day': 0
            }
        }
        
        for i, period in enumerate(high_correlation_periods, 1):
            end_date = period['end_date']
            start_date = period['start_date']
            avg_correlation = period['avg_correlation']
            source_stock_code = period['stock_code']
            source_type = period['source']
            
            # æ ¹æ®æ¥æºè‚¡ç¥¨ä»£ç è·å–æ­£ç¡®çš„æ•°æ®æº
            if source_stock_code == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                source_data = self.loaded_stocks_data.get(source_stock_code)
                if source_data is None:
                    self.logger.warning(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {source_stock_code} çš„æ•°æ®ï¼Œè·³è¿‡æœŸé—´ #{i}")
                    continue
            
            # æ‰¾åˆ°è¯¥æœŸé—´ç»“æŸåçš„æ•°æ®ä½ç½®
            try:
                end_idx = source_data.index.get_loc(end_date)
            except KeyError:
                self.logger.warning(f"åœ¨è‚¡ç¥¨ {source_stock_code} æ•°æ®ä¸­æ‰¾ä¸åˆ°æ—¥æœŸ {end_date}ï¼Œè·³è¿‡æœŸé—´ #{i}")
                continue
            
            # è·å–æœŸé—´æœ€åä¸€å¤©çš„æ”¶ç›˜ä»·
            period_close = source_data.iloc[end_idx]['close']
            
            # Debugæ¨¡å¼ä¸‹è®°å½•æ¯ä¸ªæœŸé—´çš„è¯¦ç»†ä¿¡æ¯
            if self.debug:
                self.logger.info(f"é«˜ç›¸å…³æ€§æœŸé—´ #{i}:")
                self.logger.info(f"  æœŸé—´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
                self.logger.info(f"  æ¥æºè‚¡ç¥¨: {source_stock_code} ({'è‡ªèº«å†å²' if source_type == 'self' else 'å¯¹æ¯”è‚¡ç¥¨'})")
                self.logger.info(f"  ç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
                self.logger.info(f"  æœŸé—´æ”¶ç›˜ä»·: {period_close:.2f}")
            
            # æ ¹æ®é«˜ç›¸å…³æ€§æœŸé—´çš„æ¥æºè‚¡ç¥¨è·å–å¯¹åº”çš„æ•°æ®æº
            period_stock_code = period['stock_code']
            if period_stock_code == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„æ•°æ®
                source_data = self.loaded_stocks_data.get(period_stock_code)
                if source_data is None:
                    self.logger.warning(f"æ— æ³•è·å–è‚¡ç¥¨ {period_stock_code} çš„æ•°æ®ï¼Œè·³è¿‡æœŸé—´ #{i} çš„æœªæ¥è¡¨ç°åˆ†æ")
                    continue
            
            # æ‰¾åˆ°å¯¹åº”çš„æ—¥æœŸåœ¨æ•°æ®æºä¸­çš„ä½ç½®
            try:
                source_end_idx = source_data.index.get_loc(end_date)
            except KeyError:
                self.logger.warning(f"åœ¨è‚¡ç¥¨ {period_stock_code} æ•°æ®ä¸­æ‰¾ä¸åˆ°æ—¥æœŸ {end_date}ï¼Œè·³è¿‡æœŸé—´ #{i} çš„æœªæ¥è¡¨ç°åˆ†æ")
                continue
            
            # æ£€æŸ¥ä¸‹1ä¸ªäº¤æ˜“æ—¥
            if source_end_idx + 1 < len(source_data):
                next_day_data = source_data.iloc[source_end_idx + 1]
                next_day_date = source_data.index[source_end_idx + 1]
                next_day_open = next_day_data['open']
                next_day_close = next_day_data['close']
                
                stats['valid_periods']['next_day'] += 1
                
                # é«˜å¼€åˆ¤æ–­
                gap_up = next_day_open > period_close
                if gap_up:
                    stats['next_day_gap_up'] += 1
                
                # ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨åˆ¤æ–­
                day_1_up = next_day_close > period_close
                if day_1_up:
                    stats['next_1_day_up'] += 1
                
                # Debugæ¨¡å¼ä¸‹è®°å½•è¯¦ç»†ä¿¡æ¯
                if self.debug:
                    gap_up_str = "æ˜¯" if gap_up else "å¦"
                    day_1_up_str = "æ˜¯" if day_1_up else "å¦"
                    self.logger.info(f"  ä¸‹1æ—¥({next_day_date.strftime('%Y-%m-%d')}): å¼€ç›˜{next_day_open:.2f} æ”¶ç›˜{next_day_close:.2f} | é«˜å¼€:{gap_up_str} ä¸Šæ¶¨:{day_1_up_str}")
            
            # æ£€æŸ¥ä¸‹3ä¸ªäº¤æ˜“æ—¥
            if source_end_idx + 3 < len(source_data):
                day_3_data = source_data.iloc[source_end_idx + 3]
                day_3_date = source_data.index[source_end_idx + 3]
                day_3_close = day_3_data['close']
                stats['valid_periods']['next_3_day'] += 1
                
                day_3_up = day_3_close > period_close
                if day_3_up:
                    stats['next_3_day_up'] += 1
                
                # Debugæ¨¡å¼ä¸‹è®°å½•è¯¦ç»†ä¿¡æ¯
                if self.debug:
                    day_3_up_str = "æ˜¯" if day_3_up else "å¦"
                    self.logger.info(f"  ä¸‹3æ—¥({day_3_date.strftime('%Y-%m-%d')}): æ”¶ç›˜{day_3_close:.2f} | ä¸Šæ¶¨:{day_3_up_str}")
            
            # æ£€æŸ¥ä¸‹5ä¸ªäº¤æ˜“æ—¥
            if source_end_idx + 5 < len(source_data):
                day_5_data = source_data.iloc[source_end_idx + 5]
                day_5_date = source_data.index[source_end_idx + 5]
                day_5_close = day_5_data['close']
                stats['valid_periods']['next_5_day'] += 1
                
                day_5_up = day_5_close > period_close
                if day_5_up:
                    stats['next_5_day_up'] += 1
                
                # Debugæ¨¡å¼ä¸‹è®°å½•è¯¦ç»†ä¿¡æ¯
                if self.debug:
                    day_5_up_str = "æ˜¯" if day_5_up else "å¦"
                    self.logger.info(f"  ä¸‹5æ—¥({day_5_date.strftime('%Y-%m-%d')}): æ”¶ç›˜{day_5_close:.2f} | ä¸Šæ¶¨:{day_5_up_str}")
            
            # æ£€æŸ¥ä¸‹10ä¸ªäº¤æ˜“æ—¥
            if source_end_idx + 10 < len(source_data):
                day_10_data = source_data.iloc[source_end_idx + 10]
                day_10_date = source_data.index[source_end_idx + 10]
                day_10_close = day_10_data['close']
                stats['valid_periods']['next_10_day'] += 1
                
                day_10_up = day_10_close > period_close
                if day_10_up:
                    stats['next_10_day_up'] += 1
                
                # Debugæ¨¡å¼ä¸‹è®°å½•è¯¦ç»†ä¿¡æ¯
                if self.debug:
                    day_10_up_str = "æ˜¯" if day_10_up else "å¦"
                    self.logger.info(f"  ä¸‹10æ—¥({day_10_date.strftime('%Y-%m-%d')}): æ”¶ç›˜{day_10_close:.2f} | ä¸Šæ¶¨:{day_10_up_str}")
            
            # Debugæ¨¡å¼ä¸‹æ·»åŠ åˆ†éš”çº¿
            if self.debug:
                self.logger.info("  " + "-" * 50)
        
        # è®¡ç®—æ¯”ä¾‹
        stats['ratios'] = {}
        if stats['valid_periods']['next_day'] > 0:
            stats['ratios']['next_day_gap_up'] = stats['next_day_gap_up'] / stats['valid_periods']['next_day']
            stats['ratios']['next_1_day_up'] = stats['next_1_day_up'] / stats['valid_periods']['next_day']
        
        if stats['valid_periods']['next_3_day'] > 0:
            stats['ratios']['next_3_day_up'] = stats['next_3_day_up'] / stats['valid_periods']['next_3_day']
        
        if stats['valid_periods']['next_5_day'] > 0:
            stats['ratios']['next_5_day_up'] = stats['next_5_day_up'] / stats['valid_periods']['next_5_day']
        
        if stats['valid_periods']['next_10_day'] > 0:
            stats['ratios']['next_10_day_up'] = stats['next_10_day_up'] / stats['valid_periods']['next_10_day']
        
        return stats
    
    def log_performance_stats(self, stats):
        """
        è®°å½•ç»Ÿè®¡ç»“æœåˆ°æ—¥å¿—
        
        Args:
            stats: ç»Ÿè®¡ç»“æœå­—å…¸
        """
        if not stats:
            self.logger.info("æ— ç»Ÿè®¡æ•°æ®å¯è¾“å‡º")
            return
        
        self.logger.info("=" * 60)
        self.logger.info("é«˜ç›¸å…³æ€§æœŸé—´æœªæ¥è¡¨ç°ç»Ÿè®¡")
        self.logger.info("=" * 60)
        self.logger.info(f"æ€»é«˜ç›¸å…³æ€§æœŸé—´æ•°: {stats['total_periods']}")
        
        # ç®€æ´çš„å•è¡Œè¾“å‡ºæ ¼å¼
        if stats['valid_periods']['next_day'] > 0:
            self.logger.info(f"ä¸‹1æ—¥é«˜å¼€: {stats['ratios']['next_day_gap_up']:.1%}({stats['next_day_gap_up']}/{stats['valid_periods']['next_day']})")
            self.logger.info(f"ä¸‹1æ—¥ä¸Šæ¶¨: {stats['ratios']['next_1_day_up']:.1%}({stats['next_1_day_up']}/{stats['valid_periods']['next_day']})")
        
        if stats['valid_periods']['next_3_day'] > 0:
            self.logger.info(f"ä¸‹3æ—¥ä¸Šæ¶¨: {stats['ratios']['next_3_day_up']:.1%}({stats['next_3_day_up']}/{stats['valid_periods']['next_3_day']})")
        
        if stats['valid_periods']['next_5_day'] > 0:
            self.logger.info(f"ä¸‹5æ—¥ä¸Šæ¶¨: {stats['ratios']['next_5_day_up']:.1%}({stats['next_5_day_up']}/{stats['valid_periods']['next_5_day']})")
        
        if stats['valid_periods']['next_10_day'] > 0:
            self.logger.info(f"ä¸‹10æ—¥ä¸Šæ¶¨: {stats['ratios']['next_10_day_up']:.1%}({stats['next_10_day_up']}/{stats['valid_periods']['next_10_day']})")
    
    def _print_detailed_evaluation_data(self, high_correlation_periods, recent_data):
        """
        Debugæ¨¡å¼ä¸‹æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            high_correlation_periods: é«˜ç›¸å…³æ€§æœŸé—´åˆ—è¡¨
            recent_data: æœ€è¿‘çš„æ•°æ®
        """
        self.logger.info("=" * 80)
        self.logger.info("DEBUGæ¨¡å¼ - å‰10æ¡è¯„æµ‹æ•°æ®è¯¦ç»†ä¿¡æ¯:")
        self.logger.info("=" * 80)
        
        # ä¸ºäº†ä¸GPUæ‰¹é‡ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿå…¶é€»è¾‘ï¼š
        # 1. å°†æ‰€æœ‰å†å²æœŸé—´æŒ‰æ—¶é—´é¡ºåºæ’åºï¼ˆæ¨¡æ‹Ÿå†å²æœŸé—´ç´¢å¼•é¡ºåºï¼‰
        # 2. å–å‰10æ¡æ•°æ®ï¼ˆæ— è®ºæ˜¯å¦é«˜ç›¸å…³ï¼‰
        
        # é¦–å…ˆè·å–æ‰€æœ‰å¯èƒ½çš„å†å²æœŸé—´æ•°æ®
        data = self.load_data()
        if data is None:
            self.logger.error("æ— æ³•åŠ è½½æ•°æ®ç”¨äºè¯¦ç»†ä¿¡æ¯æ‰“å°")
            return
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å†å²æœŸé—´ï¼ˆæ¨¡æ‹ŸGPUæ‰¹é‡ç‰ˆæœ¬çš„å†å²æœŸé—´åˆ—è¡¨ï¼‰
        all_historical_periods = []
        
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„å†å²æœŸé—´
        for i in range(len(data) - self.window_size + 1):
            start_idx = i
            end_idx = i + self.window_size - 1
            
            if end_idx < len(data):
                start_date = data.index[start_idx]
                end_date = data.index[end_idx]
                
                # è·å–è¯¥æœŸé—´çš„æ•°æ®
                period_data = data.iloc[start_idx:end_idx + 1]
                
                # è®¡ç®—ä¸æœ€è¿‘æ•°æ®çš„ç›¸å…³æ€§
                if len(period_data) == self.window_size and len(recent_data) == self.window_size:
                    fields = ['open', 'high', 'low', 'close', 'volume']
                    correlations = {}
                    correlation_values = []
                    
                    for field in fields:
                        if field in recent_data.columns and field in period_data.columns:
                            recent_values = recent_data[field].values
                            historical_values = period_data[field].values
                            
                            # è®¡ç®—ç›¸å…³ç³»æ•°
                            corr = np.corrcoef(recent_values, historical_values)[0, 1]
                            if not np.isnan(corr):
                                correlations[field] = corr
                                correlation_values.append(corr)
                    
                    # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°
                    avg_correlation = np.mean(correlation_values) if correlation_values else 0.0
                    is_high_correlation = avg_correlation >= self.threshold
                    
                    all_historical_periods.append({
                        'hist_idx': i,
                        'start_date': start_date,
                        'end_date': end_date,
                        'stock_code': self.stock_code,
                        'avg_correlation': avg_correlation,
                        'correlations': correlations,
                        'is_high_correlation': is_high_correlation,
                        'period_data': period_data
                    })
        
        # å–å‰10æ¡æ•°æ®ï¼ˆæŒ‰å†å²æœŸé—´ç´¢å¼•é¡ºåºï¼Œæ¨¡æ‹ŸGPUæ‰¹é‡ç‰ˆæœ¬ï¼‰
        periods_to_print = all_historical_periods[:10]
        
        count = 0
        for period in periods_to_print:
            count += 1
            
            self.logger.info(f"è¯„æµ‹æ•°æ® #{count}:")
            self.logger.info(f"  å†å²æœŸé—´: {period['start_date'].strftime('%Y-%m-%d')} åˆ° {period['end_date'].strftime('%Y-%m-%d')}")
            self.logger.info(f"  æ¥æºè‚¡ç¥¨: {period['stock_code']}")
            self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {period['avg_correlation']:.6f}")
            self.logger.info(f"  é«˜ç›¸å…³æ€§çŠ¶æ€: {'æ˜¯' if period['is_high_correlation'] else 'å¦'}")
            
            # æ‰“å°å„å­—æ®µç›¸å…³ç³»æ•°
            self.logger.info(f"  å„å­—æ®µç›¸å…³ç³»æ•°:")
            for field, corr in period['correlations'].items():
                if not np.isnan(corr):
                    self.logger.info(f"    {field}: {corr:.6f}")
            
            # æ‰“å°æ¯”è¾ƒæ•°ç»„è¯¦æƒ…
            self._print_comparison_array_details(recent_data, period['period_data'], field_name="è‡ªèº«å†å²")
            
            self.logger.info("  " + "-" * 60)
        
        self.logger.info(f"DEBUGæ¨¡å¼ - å·²æ‰“å°å‰{count}æ¡è¯„æµ‹æ•°æ®è¯¦ç»†ä¿¡æ¯")
        self.logger.info("=" * 80)

    def _print_comparison_array_details(self, recent_data, historical_data, field_name):
        """
        æ‰“å°æ¯”è¾ƒæ•°ç»„çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            recent_data: æœ€è¿‘æ•°æ®
            historical_data: å†å²æ•°æ®
            field_name: å­—æ®µåç§°
        """
        self.logger.info(f"  æ¯”è¾ƒæ•°ç»„è¯¦æƒ… ({field_name}):")
        self.logger.info(f"    æ•°æ®çª—å£å¤§å°: {len(recent_data)} vs {len(historical_data)}")
        
        # æ‰“å°å„å­—æ®µçš„æ¯”è¾ƒè¯¦æƒ…
        for field in ['open', 'high', 'low', 'close', 'volume']:
            if field in recent_data.columns and field in historical_data.columns:
                recent_values = recent_data[field].values
                historical_values = historical_data[field].values
                
                self.logger.info(f"    {field}å­—æ®µæ¯”è¾ƒ:")
                
                # å‰5å¤©æ•°æ®æ¯”è¾ƒ
                recent_first_5 = recent_values[:5] if len(recent_values) >= 5 else recent_values
                historical_first_5 = historical_values[:5] if len(historical_values) >= 5 else historical_values
                
                self.logger.info(f"      è¯„æµ‹æ•°æ®å‰5å¤©: {[f'{x:.2f}' for x in recent_first_5]}")
                self.logger.info(f"      å†å²æ•°æ®å‰5å¤©: {[f'{x:.2f}' for x in historical_first_5]}")
                
                # å5å¤©æ•°æ®æ¯”è¾ƒ
                recent_last_5 = recent_values[-5:] if len(recent_values) >= 5 else recent_values
                historical_last_5 = historical_values[-5:] if len(historical_values) >= 5 else historical_values
                
                self.logger.info(f"      è¯„æµ‹æ•°æ®å5å¤©: {[f'{x:.2f}' for x in recent_last_5]}")
                self.logger.info(f"      å†å²æ•°æ®å5å¤©: {[f'{x:.2f}' for x in historical_last_5]}")
                
                # ç»Ÿè®¡ä¿¡æ¯
                recent_mean = np.mean(recent_values)
                recent_std = np.std(recent_values)
                historical_mean = np.mean(historical_values)
                historical_std = np.std(historical_values)
                
                self.logger.info(f"      è¯„æµ‹æ•°æ®ç»Ÿè®¡: å‡å€¼={recent_mean:.2f}, æ ‡å‡†å·®={recent_std:.2f}")
                self.logger.info(f"      å†å²æ•°æ®ç»Ÿè®¡: å‡å€¼={historical_mean:.2f}, æ ‡å‡†å·®={historical_std:.2f}")

    def save_stats_to_file(self, stats):
        """
        å°†ç»Ÿè®¡ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶
        
        Args:
            stats: ç»Ÿè®¡ç»“æœå­—å…¸
        """
        if not stats:
            return
        
        self.start_timer('stats_saving')
        
        # åˆ›å»ºç»Ÿè®¡ç»“æœç›®å½•
        stats_dir = os.path.join(self.log_dir, 'stats')
        os.makedirs(stats_dir, exist_ok=True)
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯
        csv_data.append(['è‚¡ç¥¨ä»£ç ', self.stock_code])
        csv_data.append(['åˆ†ææ—¶é—´', timestamp])
        csv_data.append(['æ€»é«˜ç›¸å…³æ€§æœŸé—´æ•°', stats['total_periods']])
        csv_data.append(['ç›¸å…³ç³»æ•°é˜ˆå€¼', self.threshold])
        csv_data.append([''])  # ç©ºè¡Œ
        
        # æ·»åŠ ç»Ÿè®¡ç»“æœ
        csv_data.append(['ç»Ÿè®¡é¡¹ç›®', 'æœ‰æ•ˆæ ·æœ¬æ•°', 'æˆåŠŸæ¬¡æ•°', 'æˆåŠŸæ¯”ä¾‹'])
        
        if stats['valid_periods']['next_day'] > 0:
            csv_data.append(['ä¸‹1ä¸ªäº¤æ˜“æ—¥é«˜å¼€', stats['valid_periods']['next_day'], 
                           stats['next_day_gap_up'], f"{stats['ratios']['next_day_gap_up']:.2%}"])
            csv_data.append(['ä¸‹1ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨', stats['valid_periods']['next_day'], 
                           stats['next_1_day_up'], f"{stats['ratios']['next_1_day_up']:.2%}"])
        
        if stats['valid_periods']['next_3_day'] > 0:
            csv_data.append(['ä¸‹3ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨', stats['valid_periods']['next_3_day'], 
                           stats['next_3_day_up'], f"{stats['ratios']['next_3_day_up']:.2%}"])
        
        if stats['valid_periods']['next_5_day'] > 0:
            csv_data.append(['ä¸‹5ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨', stats['valid_periods']['next_5_day'], 
                           stats['next_5_day_up'], f"{stats['ratios']['next_5_day_up']:.2%}"])
        
        if stats['valid_periods']['next_10_day'] > 0:
            csv_data.append(['ä¸‹10ä¸ªäº¤æ˜“æ—¥ä¸Šæ¶¨', stats['valid_periods']['next_10_day'], 
                           stats['next_10_day_up'], f"{stats['ratios']['next_10_day_up']:.2%}"])
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        import csv
        csv_file = os.path.join(stats_dir, f'performance_stats_{self.stock_code}_{timestamp}.csv')
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerows(csv_data)
        
        self.logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
        self.end_timer('stats_saving')

    def analyze(self):
        """æ‰§è¡ŒPearsonç›¸å…³æ€§åˆ†æ"""
        self.start_timer('total_analysis')
        self.start_timer('data_loading')
        
        # åŠ è½½ç›®æ ‡è‚¡ç¥¨æ•°æ®
        data = self.load_data()
        if data is None:
            self.end_timer('data_loading')
            self.end_timer('total_analysis')
            return
        
        # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
        if len(data) < self.window_size * 2:
            self.logger.error(f"æ•°æ®é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.window_size * 2} æ¡è®°å½•")
            self.end_timer('data_loading')
            self.end_timer('total_analysis')
            return
        
        self.end_timer('data_loading')
        
        # è·å–æœ€è¿‘çš„æ•°æ®ï¼ˆæ”¯æŒè‡ªå®šä¹‰å›æµ‹æ—¥æœŸï¼‰
        if self.backtest_date:
            # å¦‚æœæŒ‡å®šäº†å›æµ‹æ—¥æœŸï¼Œä»è¯¥æ—¥æœŸå¾€å‰æ•°è·å–æ•°æ®æ®µ
            try:
                backtest_datetime = pd.to_datetime(self.backtest_date)
                
                # æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦åœ¨æ•°æ®èŒƒå›´å†…
                if backtest_datetime < data.index.min():
                    self.logger.error(f"æŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ—©äºæ‰€æœ‰å¯ç”¨æ•°æ®ï¼ˆæœ€æ—©æ•°æ®æ—¥æœŸ: {data.index.min().strftime('%Y-%m-%d')}ï¼‰")
                    print(f"é”™è¯¯ï¼šæŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ—©äºæ‰€æœ‰å¯ç”¨æ•°æ®")
                    return
                
                if backtest_datetime > data.index.max():
                    self.logger.error(f"æŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ™šäºæ‰€æœ‰å¯ç”¨æ•°æ®ï¼ˆæœ€æ™šæ•°æ®æ—¥æœŸ: {data.index.max().strftime('%Y-%m-%d')}ï¼‰")
                    print(f"é”™è¯¯ï¼šæŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ™šäºæ‰€æœ‰å¯ç”¨æ•°æ®")
                    return
                
                # æ‰¾åˆ°æŒ‡å®šæ—¥æœŸæˆ–ä¹‹å‰æœ€è¿‘çš„äº¤æ˜“æ—¥
                available_dates = data.index[data.index <= backtest_datetime]
                if len(available_dates) == 0:
                    self.logger.error(f"æŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ²¡æœ‰å¯¹åº”çš„äº¤æ˜“æ—¥æ•°æ®")
                    print(f"é”™è¯¯ï¼šæŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} æ²¡æœ‰å¯¹åº”çš„äº¤æ˜“æ—¥æ•°æ®")
                    return
                
                # æ£€æŸ¥æŒ‡å®šæ—¥æœŸå½“å¤©æ˜¯å¦æœ‰æ•°æ®
                exact_date_data = data[data.index.date == backtest_datetime.date()]
                if exact_date_data.empty:
                    # å¦‚æœæŒ‡å®šæ—¥æœŸå½“å¤©æ²¡æœ‰æ•°æ®ï¼Œç›´æ¥ç»“æŸç¨‹åº
                    self.logger.error(f"æŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} å½“å¤©æ²¡æœ‰äº¤æ˜“æ•°æ®ï¼Œç¨‹åºç»“æŸ")
                    print(f"é”™è¯¯ï¼šæŒ‡å®šçš„å›æµ‹æ—¥æœŸ {self.backtest_date} å½“å¤©æ²¡æœ‰äº¤æ˜“æ•°æ®ï¼Œç¨‹åºç»“æŸ")
                    return
                else:
                    self.logger.info(f"æ‰¾åˆ°æŒ‡å®šå›æµ‹æ—¥æœŸ {self.backtest_date} çš„äº¤æ˜“æ•°æ®")
                
                # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä½†ä»¥æŒ‡å®šæ—¥æœŸä¸ºåˆ†æç»ˆç‚¹
                recent_data = data[data.index <= backtest_datetime]
                
                if len(recent_data) < self.window_size:
                    self.logger.error(f"ä»æŒ‡å®šæ—¥æœŸ {self.backtest_date} å¾€å‰æ•°æ®ä¸è¶³ {self.window_size} ä¸ªäº¤æ˜“æ—¥ï¼Œå®é™…è·å– {len(recent_data)} ä¸ªäº¤æ˜“æ—¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                    print(f"é”™è¯¯ï¼šä»æŒ‡å®šæ—¥æœŸ {self.backtest_date} å¾€å‰æ•°æ®ä¸è¶³ {self.window_size} ä¸ªäº¤æ˜“æ—¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                    return
                
            except Exception as e:
                self.logger.error(f"è§£æå›æµ‹æ—¥æœŸ {self.backtest_date} å¤±è´¥: {str(e)}")
                print(f"é”™è¯¯ï¼šè§£æå›æµ‹æ—¥æœŸ {self.backtest_date} å¤±è´¥: {str(e)}")
                return
        else:
            # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œåˆ†æ
            recent_data = data
        
        recent_start_date = recent_data.index[0]
        recent_end_date = recent_data.index[-1]
        
        self.logger.info(f"å¼€å§‹Pearsonç›¸å…³æ€§åˆ†æ")
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨: {self.stock_code}")
        self.logger.info(f"åˆ†æçš„æœ€è¿‘äº¤æ˜“æ—¥æœŸé—´: {recent_start_date} åˆ° {recent_end_date}")
        self.logger.info(f"çª—å£å¤§å°: {self.window_size}, é˜ˆå€¼: {self.threshold}")
        
        # å­˜å‚¨é«˜ç›¸å…³æ€§ç»“æœ
        high_correlation_periods = []
        max_correlation = 0
        max_correlation_period = None
        
        # 1. åˆ†æè‡ªèº«å†å²æ•°æ®
        self.start_timer('self_analysis')
        self.start_timer('correlation_calculation')
        self.logger.info(f"å¼€å§‹åˆ†æè‡ªèº«å†å²æ•°æ®...")
        comparison_count = 0
        
        # ä¸ºè‡ªèº«å†å²æ•°æ®åˆ†æé‡æ–°å®šä¹‰recent_dataï¼ŒåªåŒ…å«æœ€è¿‘window_sizeå¤©çš„æ•°æ®
        if self.backtest_date:
            # æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œå–å›æµ‹æ—¥æœŸä¹‹å‰çš„æœ€è¿‘window_sizeå¤©æ•°æ®
            backtest_data = data[data.index <= pd.to_datetime(self.backtest_date)]
            recent_data_for_self = backtest_data.iloc[-self.window_size:]
            # æ¯”è¾ƒå›æµ‹æ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®æ®µï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„çª—å£
            max_historical_periods = len(backtest_data) - self.window_size + 1
        else:
            # æ²¡æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œå–æœ€è¿‘window_sizeå¤©æ•°æ®
            recent_data_for_self = data.iloc[-self.window_size:]
            # æ¯”è¾ƒé™¤äº†æœ€åwindow_sizeå¤©ä¹‹å¤–çš„å†å²æ•°æ®æ®µï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„çª—å£
            max_historical_periods = len(data) - self.window_size + 1
        
        for i in range(max_historical_periods):
            historical_data = data.iloc[i:i + self.window_size]
            historical_start_date = historical_data.index[0]
            historical_end_date = historical_data.index[-1]
            
            comparison_count += 1
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            avg_correlation, correlations = self.calculate_pearson_correlation(recent_data_for_self, historical_data)
            
            # æ›´æ–°æœ€é«˜ç›¸å…³ç³»æ•°ï¼ˆå‰”é™¤ç›¸å…³æ€§ç³»æ•°>=0.9999çš„ç»“æœï¼‰
            if avg_correlation > max_correlation and avg_correlation < 0.9999:
                max_correlation = avg_correlation
                max_correlation_period = (historical_start_date, historical_end_date, self.stock_code)
            
            # Debugæ¨¡å¼ä¸‹çš„è¯¦ç»†æ—¥å¿—
            if self.debug and comparison_count % 500 == 0:
                self.logger.info(f"DEBUG - è‡ªèº«å†å²ç¬¬{comparison_count}æ¬¡æ¯”è¾ƒ:")
                self.logger.info(f"  å†å²æœŸé—´: {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
                self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.6f}")
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼Œå¹¶å‰”é™¤ç›¸å…³æ€§ç³»æ•°>=0.9999çš„ç»“æœ
            if avg_correlation >= self.threshold and avg_correlation < 0.9999:
                high_correlation_periods.append({
                    'start_date': historical_start_date,
                    'end_date': historical_end_date,
                    'avg_correlation': avg_correlation,
                    'correlations': correlations,
                    'stock_code': self.stock_code,
                    'source': 'self'
                })
                
                # è®°å½•å‘ç°çš„é«˜ç›¸å…³æ€§æ•°æ®
                self.logger.info("å‘ç°é«˜ç›¸å…³æ€§æ•°æ® (è‡ªèº«å†å²):")
                self.logger.info(f"  å†å²æœŸé—´: {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
                self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
            elif avg_correlation >= 1.0:
                # è®°å½•è¢«è¿‡æ»¤çš„ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„æ•°æ®
                if self.debug:
                    self.logger.info(f"è¿‡æ»¤ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„æ•°æ® (è‡ªèº«å†å²): {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
        
        # ç»“æŸè‡ªèº«å†å²çš„ç›¸å…³æ€§è®¡ç®—è®¡æ—¶
        correlation_elapsed_time = self.end_timer('correlation_calculation')
        self.logger.info(f"è‡ªèº«å†å²æ•°æ®åˆ†æå®Œæˆï¼Œæ¯”è¾ƒäº† {comparison_count} ä¸ªæœŸé—´ï¼Œç›¸å…³æ€§è®¡ç®—è€—æ—¶: {correlation_elapsed_time:.3f}ç§’")
        self.end_timer('self_analysis')
        
        # 2. åˆ†æå¯¹æ¯”è‚¡ç¥¨æ•°æ®
        if self.comparison_stocks:
            self.start_timer('comparison_analysis')
            self.logger.info(f"å¼€å§‹åˆ†æå¯¹æ¯”è‚¡ç¥¨æ•°æ®...")
            cross_comparison_count = 0
            
            # ä¸ºå¯¹æ¯”è‚¡ç¥¨åˆ†æé‡æ–°å®šä¹‰recent_dataï¼ŒåªåŒ…å«ç›®æ ‡è‚¡ç¥¨æœ€è¿‘window_sizeå¤©çš„æ•°æ®
            if self.backtest_date:
                # æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œå–ç›®æ ‡è‚¡ç¥¨å›æµ‹æ—¥æœŸä¹‹å‰çš„æœ€è¿‘window_sizeå¤©æ•°æ®
                backtest_data = data[data.index <= pd.to_datetime(self.backtest_date)]
                recent_data_for_comparison = backtest_data.iloc[-self.window_size:]
            else:
                # æ²¡æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œå–ç›®æ ‡è‚¡ç¥¨æœ€è¿‘window_sizeå¤©æ•°æ®
                recent_data_for_comparison = data.iloc[-self.window_size:]
            
            # è®¡ç®—æœ‰æ•ˆçš„å¯¹æ¯”è‚¡ç¥¨æ€»æ•°
            valid_comparison_stocks = [
                (code, data) for code, data in self.loaded_stocks_data.items() 
                if data is not None and len(data) >= self.window_size
            ]
            total_comparison_stocks = len(valid_comparison_stocks)
            current_stock_index = 0
            
            for comp_stock_code, comp_data in valid_comparison_stocks:
                current_stock_index += 1
                
                # å¼€å§‹å•ä¸ªè‚¡ç¥¨çš„ç›¸å…³æ€§è®¡ç®—è®¡æ—¶
                self.start_timer('correlation_calculation')
                self.logger.info(f"æ­£åœ¨åˆ†æå¯¹æ¯”è‚¡ç¥¨: {comp_stock_code} ({current_stock_index}/{total_comparison_stocks})")
                stock_comparison_count = 0
                
                # éå†å¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                # å½“ä½¿ç”¨å…¨éƒ¨æ•°æ®æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒä¸åŒçš„å†å²æ—¶é—´æ®µ
                # å¦‚æœæŒ‡å®šäº†å›æµ‹æ—¥æœŸï¼Œåˆ™æ¯”è¾ƒè¯¥æ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®æ®µ
                # å¦‚æœæ²¡æœ‰æŒ‡å®šå›æµ‹æ—¥æœŸï¼Œåˆ™æ¯”è¾ƒé™¤äº†æœ€åwindow_sizeå¤©ä¹‹å¤–çš„å†å²æ•°æ®æ®µ
                if self.backtest_date:
                    # æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œæ¯”è¾ƒå›æµ‹æ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®æ®µï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„çª—å£
                    max_comp_historical_periods = len(comp_data[comp_data.index <= pd.to_datetime(self.backtest_date)]) - self.window_size + 1
                else:
                    # æ²¡æœ‰å›æµ‹æ—¥æœŸæ—¶ï¼Œæ¯”è¾ƒé™¤äº†æœ€åwindow_sizeå¤©ä¹‹å¤–çš„å†å²æ•°æ®æ®µï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„çª—å£
                    max_comp_historical_periods = len(comp_data) - self.window_size + 1
                
                for i in range(max_comp_historical_periods):
                    historical_data = comp_data.iloc[i:i + self.window_size]
                    historical_start_date = historical_data.index[0]
                    historical_end_date = historical_data.index[-1]
                    
                    stock_comparison_count += 1
                    cross_comparison_count += 1
                    
                    # è®¡ç®—ç›¸å…³ç³»æ•°
                    avg_correlation, correlations = self.calculate_pearson_correlation(recent_data_for_comparison, historical_data)
                    
                    # æ›´æ–°æœ€é«˜ç›¸å…³ç³»æ•°ï¼ˆå‰”é™¤ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„ç»“æœï¼‰
                    if avg_correlation > max_correlation and avg_correlation < 1.0:
                        max_correlation = avg_correlation
                        max_correlation_period = (historical_start_date, historical_end_date, comp_stock_code)
                    
                    # Debugæ¨¡å¼ä¸‹çš„è¯¦ç»†æ—¥å¿—
                    if self.debug and cross_comparison_count % 1000 == 0:
                        self.logger.info(f"DEBUG - è·¨è‚¡ç¥¨ç¬¬{cross_comparison_count}æ¬¡æ¯”è¾ƒ ({comp_stock_code}):")
                        self.logger.info(f"  å†å²æœŸé—´: {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
                        self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.6f}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼Œå¹¶å‰”é™¤ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„ç»“æœ
                    if avg_correlation >= self.threshold and avg_correlation < 1.0:
                        high_correlation_periods.append({
                            'start_date': historical_start_date,
                            'end_date': historical_end_date,
                            'avg_correlation': avg_correlation,
                            'correlations': correlations,
                            'stock_code': comp_stock_code,
                            'source': 'comparison'
                        })
                        
                        # è®°å½•å‘ç°çš„é«˜ç›¸å…³æ€§æ•°æ®
                        self.logger.info(f"å‘ç°é«˜ç›¸å…³æ€§æ•°æ® (å¯¹æ¯”è‚¡ç¥¨ {comp_stock_code}):")
                        self.logger.info(f"  å†å²æœŸé—´: {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
                        self.logger.info(f"  å¹³å‡ç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
                    elif avg_correlation >= 1.0:
                        # è®°å½•è¢«è¿‡æ»¤çš„ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„æ•°æ®
                        if self.debug:
                            self.logger.info(f"è¿‡æ»¤ç›¸å…³æ€§ç³»æ•°ç­‰äº1çš„æ•°æ® (å¯¹æ¯”è‚¡ç¥¨ {comp_stock_code}): {historical_start_date.strftime('%Y-%m-%d')} åˆ° {historical_end_date.strftime('%Y-%m-%d')}")
                
                # ç»“æŸå•ä¸ªè‚¡ç¥¨çš„ç›¸å…³æ€§è®¡ç®—è®¡æ—¶
                elapsed_time = self.end_timer('correlation_calculation')
                self.logger.info(f"å¯¹æ¯”è‚¡ç¥¨ {comp_stock_code} åˆ†æå®Œæˆï¼Œæ¯”è¾ƒäº† {stock_comparison_count} ä¸ªæœŸé—´ï¼Œè€—æ—¶: {elapsed_time:.3f}ç§’")
            
            self.logger.info(f"è·¨è‚¡ç¥¨æ•°æ®åˆ†æå®Œæˆï¼Œæ€»å…±æ¯”è¾ƒäº† {cross_comparison_count} ä¸ªæœŸé—´")
            self.end_timer('comparison_analysis')
        
        # å¼€å§‹ç»Ÿè®¡è®¡ç®—
        self.start_timer('stats_calculation')
        
        # è¾“å‡ºåˆ†æç»“æœ
        self.logger.info("=" * 80)
        self.logger.info("åˆ†ææ€»ç»“")
        self.logger.info("=" * 80)
        self.logger.info(f"ç›®æ ‡è‚¡ç¥¨ä»£ç : {self.stock_code}")
        self.logger.info(f"åˆ†æçš„æœ€è¿‘äº¤æ˜“æ—¥æœŸé—´: {recent_start_date.strftime('%Y-%m-%d')} åˆ° {recent_end_date.strftime('%Y-%m-%d')}")
        
        # ç»Ÿè®¡ä¸åŒæ¥æºçš„æ¯”è¾ƒæ¬¡æ•°
        total_comparisons = comparison_count
        if self.comparison_stocks:
            cross_comparison_count = sum(1 for _ in self.loaded_stocks_data.values() if _ is not None)
            total_comparisons += cross_comparison_count
            self.logger.info(f"è‡ªèº«å†å²æœŸé—´æ¯”è¾ƒæ•°: {comparison_count}")
            self.logger.info(f"è·¨è‚¡ç¥¨æœŸé—´æ¯”è¾ƒæ•°: {cross_comparison_count}")
            self.logger.info(f"æ€»æ¯”è¾ƒæœŸé—´æ•°: {total_comparisons}")
        else:
            self.logger.info(f"æ€»å…±æ¯”è¾ƒçš„å†å²æœŸé—´æ•°: {comparison_count}")
        
        self.logger.info(f"ç›¸å…³ç³»æ•°é˜ˆå€¼: {self.threshold}")
        self.logger.info(f"å‘ç°çš„é«˜ç›¸å…³æ€§æœŸé—´æ•°: {len(high_correlation_periods)}")
        
        # æŒ‰æ¥æºç»Ÿè®¡é«˜ç›¸å…³æ€§æœŸé—´
        if high_correlation_periods:
            self_periods = [p for p in high_correlation_periods if p['source'] == 'self']
            comparison_periods = [p for p in high_correlation_periods if p['source'] == 'comparison']
            
            self.logger.info(f"  - æ¥è‡ªè‡ªèº«å†å²: {len(self_periods)} ä¸ª")
            self.logger.info(f"  - æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨: {len(comparison_periods)} ä¸ª")
            
            if comparison_periods:
                # ç»Ÿè®¡å¯¹æ¯”è‚¡ç¥¨çš„åˆ†å¸ƒ
                stock_distribution = {}
                for period in comparison_periods:
                    stock_code = period['stock_code']
                    stock_distribution[stock_code] = stock_distribution.get(stock_code, 0) + 1
                
                self.logger.info("å¯¹æ¯”è‚¡ç¥¨é«˜ç›¸å…³æ€§æœŸé—´åˆ†å¸ƒ:")
                for stock_code, count in sorted(stock_distribution.items(), key=lambda x: x[1], reverse=True)[:10]:
                    self.logger.info(f"  {stock_code}: {count} ä¸ª")
                if len(stock_distribution) > 10:
                    self.logger.info(f"  ... è¿˜æœ‰ {len(stock_distribution) - 10} ä¸ªè‚¡ç¥¨")
        
        if max_correlation_period:
            self.logger.info(f"æœ€é«˜å¹³å‡ç›¸å…³ç³»æ•°: {max_correlation:.4f}")
            self.logger.info(f"å¯¹åº”å†å²æœŸé—´: {max_correlation_period[0].strftime('%Y-%m-%d')} åˆ° {max_correlation_period[1].strftime('%Y-%m-%d')}")
            self.logger.info(f"æ¥æºè‚¡ç¥¨: {max_correlation_period[2]}")
        
        if high_correlation_periods:
            avg_high_correlation = np.mean([period['avg_correlation'] for period in high_correlation_periods])
            self.logger.info(f"é«˜ç›¸å…³æ€§æœŸé—´çš„å¹³å‡ç›¸å…³ç³»æ•°: {avg_high_correlation:.4f}")
            
            # Debugæ¨¡å¼ä¸‹è¾“å‡ºæœ€é«˜ç›¸å…³æ€§æœŸé—´çš„è¯¦ç»†ä¿¡æ¯
            if self.debug and max_correlation_period:
                # æ‰¾åˆ°æœ€é«˜ç›¸å…³æ€§æœŸé—´çš„è¯¦ç»†æ•°æ®
                for period in high_correlation_periods:
                    if (period['start_date'] == max_correlation_period[0] and 
                        period['end_date'] == max_correlation_period[1] and
                        period['stock_code'] == max_correlation_period[2]):
                        self.logger.info("æœ€é«˜ç›¸å…³æ€§æœŸé—´è¯¦ç»†ä¿¡æ¯:")
                        self.logger.info(f"  æ¥æº: {period['source']} ({'è‡ªèº«å†å²' if period['source'] == 'self' else 'å¯¹æ¯”è‚¡ç¥¨'})")
                        self.logger.info(f"  è‚¡ç¥¨ä»£ç : {period['stock_code']}")
                        self.logger.info(f"  å„å­—æ®µç›¸å…³ç³»æ•°:")
                        for field, corr_data in period['correlations'].items():
                            corr = corr_data['correlation']
                            if not np.isnan(corr):
                                self.logger.info(f"    {field}: {corr:.6f}")
                        break
        else:
            self.logger.info(f"æœªå‘ç°ç›¸å…³ç³»æ•°è¶…è¿‡ {self.threshold} çš„å†å²æœŸé—´")
            # å³ä½¿æœªå‘ç°é«˜ç›¸å…³æ€§æœŸé—´ï¼Œä¹Ÿä¿å­˜åŸºæœ¬ä¿¡æ¯åˆ°CSV
            self.save_evaluation_result(recent_end_date, None, 0)
        
        # Debugæ¨¡å¼ä¸‹æ‰“å°å‰10æ¡è¯„æµ‹æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        if self.debug and high_correlation_periods:
            self._print_detailed_evaluation_data(high_correlation_periods, recent_data_for_self if 'recent_data_for_self' in locals() else recent_data)
        
        # Debugæ¨¡å¼ä¸‹ç»˜åˆ¶Kçº¿å›¾å¯¹æ¯”
        if self.debug and max_correlation_period and high_correlation_periods:
            self.logger.info("=" * 80)
            self.logger.info("å¼€å§‹ç»˜åˆ¶Kçº¿å›¾å¯¹æ¯”")
            self.logger.info("=" * 80)
            
            # æ‰¾åˆ°æœ€é«˜ç›¸å…³æ€§æœŸé—´çš„æ•°æ®
            max_period_start = max_correlation_period[0]
            max_period_end = max_correlation_period[1]
            max_period_stock = max_correlation_period[2]
            
            # æ ¹æ®è‚¡ç¥¨ä»£ç è·å–å¯¹åº”çš„å†å²æ•°æ®
            if max_period_stock == self.stock_code:
                # æ¥è‡ªç›®æ ‡è‚¡ç¥¨è‡ªèº«çš„å†å²æ•°æ®
                source_data = data
                self.logger.info(f"æœ€é«˜ç›¸å…³æ€§æœŸé—´æ¥è‡ªç›®æ ‡è‚¡ç¥¨ {self.stock_code} çš„å†å²æ•°æ®")
            else:
                # æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨çš„å†å²æ•°æ®
                source_data = self.loaded_stocks_data.get(max_period_stock)
                self.logger.info(f"æœ€é«˜ç›¸å…³æ€§æœŸé—´æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨ {max_period_stock} çš„å†å²æ•°æ®")
            
            if source_data is not None:
                try:
                    # è·å–å†å²æ•°æ®
                    historical_data = source_data.loc[max_period_start:max_period_end]
                    
                    # å‡†å¤‡ç›¸å…³æ€§ä¿¡æ¯
                    correlation_info = {
                        'start_date': max_period_start,
                        'end_date': max_period_end,
                        'avg_correlation': max_correlation,
                        'source_stock': max_period_stock
                    }
                    
                    # å‡†å¤‡ç»˜å›¾ç”¨çš„recent_data - åªåŒ…å«æœ€è¿‘window_sizeå¤©çš„æ•°æ®
                    if self.backtest_date:
                        # å¦‚æœæŒ‡å®šäº†å›æµ‹æ—¥æœŸï¼Œè·å–è¯¥æ—¥æœŸä¹‹å‰çš„æœ€è¿‘window_sizeå¤©æ•°æ®
                        backtest_end_idx = data.index.get_loc(self.backtest_date)
                        plot_recent_start_idx = max(0, backtest_end_idx - self.window_size + 1)
                        plot_recent_data = data.iloc[plot_recent_start_idx:backtest_end_idx + 1]
                    else:
                        # å¦‚æœæ²¡æœ‰æŒ‡å®šå›æµ‹æ—¥æœŸï¼Œè·å–æœ€æ–°çš„window_sizeå¤©æ•°æ®
                        plot_recent_data = data.iloc[-self.window_size:]
                    
                    # ç»˜åˆ¶Kçº¿å›¾å¯¹æ¯”
                    self.plot_kline_comparison(plot_recent_data, historical_data, correlation_info)
                except Exception as e:
                    self.logger.error(f"è·å–å†å²æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    self.logger.error(f"æœŸé—´: {max_period_start} åˆ° {max_period_end}, è‚¡ç¥¨: {max_period_stock}")
            else:
                self.logger.error(f"æ— æ³•æ‰¾åˆ°è‚¡ç¥¨ {max_period_stock} çš„æ•°æ®")
        
        self.end_timer('stats_calculation')
        
        # è®¡ç®—å¹¶è¾“å‡ºç»Ÿè®¡ç»“æœ
        if high_correlation_periods:
            self.logger.info("=" * 80)
            self.logger.info("å¼€å§‹è®¡ç®—æœªæ¥è¡¨ç°ç»Ÿè®¡")
            self.logger.info("=" * 80)
            
            stats = self.calculate_future_performance_stats(data, high_correlation_periods)
            if stats:
                self.log_performance_stats(stats)
                # self.save_stats_to_file(stats)  # å·²ç§»é™¤statsæ–‡ä»¶å¤¹ç”ŸæˆåŠŸèƒ½
                # ä¿å­˜è¯„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
                self.save_evaluation_result(recent_end_date, stats, len(high_correlation_periods))
            else:
                self.logger.info("æ— æ³•è®¡ç®—ç»Ÿè®¡æ•°æ®")
                # å³ä½¿æ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼Œä¹Ÿä¿å­˜åŸºæœ¬ä¿¡æ¯åˆ°CSV
                self.save_evaluation_result(recent_end_date, None, len(high_correlation_periods))
        
        # ç»“æŸæ€»åˆ†æè®¡æ—¶å¹¶è¾“å‡ºæ€§èƒ½ç»Ÿè®¡è¡¨
        self.end_timer('total_analysis')
        self.log_performance_summary()
        
        self.logger.info("åˆ†æå®Œæˆ")
        
        return high_correlation_periods

def main():
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨Pearsonç›¸å…³æ€§åˆ†æå·¥å…·')
    parser.add_argument('stock_code', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--log_dir', default='logs', help='æ—¥å¿—ç›®å½• (é»˜è®¤: logs)')
    parser.add_argument('--window_size', type=int, default=15, help='åˆ†æçª—å£å¤§å° (é»˜è®¤: 15)')
    parser.add_argument('--threshold', type=float, default=0.85, help='ç›¸å…³ç³»æ•°é˜ˆå€¼ (é»˜è®¤: 0.85)')
    parser.add_argument('--debug', action='store_true', help='å¼€å¯debugæ¨¡å¼ï¼ˆä¼šå½±å“æ€§èƒ½ï¼‰')
    
    # è·¨è‚¡ç¥¨å¯¹æ¯”å‚æ•°
    parser.add_argument('--comparison_mode', choices=['none', 'top10', 'industry', 'custom'],
                        default='top10', help='å¯¹æ¯”æ¨¡å¼: none(ä»…è‡ªèº«), top10(å¸‚å€¼å‰10), industry(åŒè¡Œä¸š), custom(è‡ªå®šä¹‰) (é»˜è®¤: top10)')
    parser.add_argument('--comparison_stocks', nargs='*', 
                       help='è‡ªå®šä¹‰å¯¹æ¯”è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš” (ä»…åœ¨comparison_mode=customæ—¶æœ‰æ•ˆ)')
    parser.add_argument('--no_comparison', action='store_true', 
                       help='ç¦ç”¨è·¨è‚¡ç¥¨å¯¹æ¯”ï¼Œä»…åˆ†æè‡ªèº«å†å²æ•°æ®')
    parser.add_argument('--backtest_date', type=str, 
                       help='æŒ‡å®šå›æµ‹èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œä»è¯¥æ—¥æœŸå¾€å‰æ•°è·å–æ•°æ®æ®µè¿›è¡Œåˆ†æï¼Œé»˜è®¤ä½¿ç”¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥')
    parser.add_argument('--csv_filename', type=str, default='evaluation_results.csv',
                       help='æŒ‡å®šCSVç»“æœæ–‡ä»¶å (é»˜è®¤: evaluation_results.csv)')
    parser.add_argument('--earliest_date', type=str, default='2020-01-01',
                       help='æ•°æ®è¿‡æ»¤çš„æœ€æ—©æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, é»˜è®¤: 2020-01-01)')
    
    args = parser.parse_args()
    
    # æ¸…ç©ºlogsæ–‡ä»¶å¤¹
    def clear_logs_directory(log_dir):
        """æ¸…ç©ºlogsç›®å½•ä¸‹çš„å†…å®¹ï¼Œä½†ä¿ç•™æ‰€æœ‰CSVæ–‡ä»¶"""
        import shutil
        if os.path.exists(log_dir):
            try:
                # åˆ é™¤ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹ï¼Œä½†ä¿ç•™CSVæ–‡ä»¶
                for item in os.listdir(log_dir):
                    if item.endswith('.csv'):
                        continue  # è·³è¿‡æ‰€æœ‰CSVæ–‡ä»¶
                    item_path = os.path.join(log_dir, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
                print(f"å·²æ¸…ç©º {log_dir} ç›®å½•ï¼ˆä¿ç•™æ‰€æœ‰CSVæ–‡ä»¶ï¼‰")
            except Exception as e:
                print(f"æ¸…ç©º {log_dir} ç›®å½•æ—¶å‡ºé”™: {e}")
        else:
            print(f"{log_dir} ç›®å½•ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»º")
    
    # è®¾ç½®å›ºå®šçš„ç»å¯¹è·¯å¾„
    script_dir = r'C:\Users\17701\github\my_first_repo\stockapi\stock_backtest\pearson_found'
    fixed_log_dir = os.path.join(script_dir, 'logs')
    
    # æ¸…ç©ºlogsç›®å½• - å·²æ³¨é‡Šï¼Œä¸å†åˆ é™¤æ—¥å¿—æ–‡ä»¶
    # clear_logs_directory(fixed_log_dir)
    
    # å¤„ç†å¯¹æ¯”æ¨¡å¼
    if args.no_comparison:
        comparison_mode = 'none'
        comparison_stocks = None
    else:
        comparison_mode = args.comparison_mode
        comparison_stocks = args.comparison_stocks if args.comparison_mode == 'custom' else None
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = PearsonAnalyzer(
        stock_code=args.stock_code,
        log_dir=args.log_dir,  # è¿™ä¸ªå‚æ•°ç°åœ¨åœ¨PearsonAnalyzerå†…éƒ¨ä¼šè¢«å¿½ç•¥ï¼Œä½¿ç”¨å›ºå®šè·¯å¾„
        window_size=args.window_size,
        threshold=args.threshold,
        debug=args.debug,
        comparison_mode=comparison_mode,
        comparison_stocks=comparison_stocks,
        backtest_date=args.backtest_date,
        csv_filename=args.csv_filename,
        earliest_date=args.earliest_date
    )
    
    results = analyzer.analyze()
    
    # è¾“å‡ºç®€è¦ç»“æœåˆ°æ§åˆ¶å°
    if results:
        print(f"åˆ†æå®Œæˆï¼å‘ç° {len(results)} ä¸ªé«˜ç›¸å…³æ€§æœŸé—´ï¼Œç›¸å…³ç³»æ•°é˜ˆå€¼: {args.threshold}")
        
        # ç»Ÿè®¡ä¸åŒæ¥æºçš„ç»“æœ
        self_periods = [p for p in results if p['source'] == 'self']
        comparison_periods = [p for p in results if p['source'] == 'comparison']
        
        print(f"  - æ¥è‡ªè‡ªèº«å†å²: {len(self_periods)} ä¸ª")
        print(f"  - æ¥è‡ªå¯¹æ¯”è‚¡ç¥¨: {len(comparison_periods)} ä¸ª")
        
        if results:
            max_corr = max(results, key=lambda x: x['avg_correlation'])
            print(f"æœ€é«˜å¹³å‡ç›¸å…³ç³»æ•°: {max_corr['avg_correlation']:.4f}")
            print(f"å¯¹åº”æœŸé—´: {max_corr['start_date'].strftime('%Y-%m-%d')} åˆ° {max_corr['end_date'].strftime('%Y-%m-%d')}")
            print(f"æ¥æºè‚¡ç¥¨: {max_corr['stock_code']} ({'è‡ªèº«å†å²' if max_corr['source'] == 'self' else 'å¯¹æ¯”è‚¡ç¥¨'})")
    else:
        print(f"åˆ†æå®Œæˆï¼æœªå‘ç°ç›¸å…³ç³»æ•°è¶…è¿‡ {args.threshold} çš„å†å²æœŸé—´")
    
    print(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {analyzer.log_dir}")

if __name__ == "__main__":
    main()