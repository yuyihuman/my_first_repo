"""
ä»æ‰¹é‡Pearsonåˆ†ææ—¥å¿—ä¸­è§£æé«˜ç›¸å…³æœŸé—´ï¼Œå¹¶ç»˜åˆ¶ä¸Šä¸‹å¯¹æ¯”çš„Kçº¿å›¾ã€‚

åŠŸèƒ½æ¦‚è¿°ï¼š
- è§£ææ—¥å¿—æ–‡ä»¶ï¼Œè·å–è¯„æµ‹çª—å£ï¼ˆæºæ•°æ®çª—å£ï¼‰ä¸å„æ¡é«˜ç›¸å…³æœŸé—´ï¼ˆè‚¡ç¥¨ä»£ç ã€å¼€å§‹/ç»“æŸæ—¥æœŸã€ç›¸å…³ç³»æ•°ï¼‰ã€‚
- é€šè¿‡é¡¹ç›®ä¸­çš„ StockDataLoader åŠ è½½æ—¥çº¿æ•°æ®ã€‚
- ä¸Šå›¾ç»˜åˆ¶æºæ•°æ®çª—å£ï¼ˆç›®æ ‡è‚¡ç¥¨ï¼‰ï¼Œä¸‹å›¾ç»˜åˆ¶å†å²æ•°æ®çª—å£ï¼ˆæ¥æºè‚¡ç¥¨ï¼‰ã€‚
- æ¯ä¸ªå›¾åŒ…å«å¼€ç›˜ã€æ”¶ç›˜çš„èœ¡çƒ›å›¾ä»¥åŠæˆäº¤é‡æŸ±çŠ¶å›¾ã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
python plot_kline_from_log.py \
  --log "c:\\Users\\17701\\github\\my_first_repo\\stockapi\\stock_backtest\\pearson_found\\logs\\batch_pearson_analysis_list_20251111_233136_thread_22560.log" \
  --output-dir ./kline_plots \
  --only-index 1

å‚æ•°è¯´æ˜ï¼š
- --log: å¿…å¡«ï¼Œæ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚
- --output-dir: è¾“å‡ºå›¾ç‰‡ç›®å½•ï¼Œé»˜è®¤åœ¨æ—¥å¿—åŒç›®å½•ä¸‹çš„ kline_plotsã€‚
- --source-stock: å¯é€‰ï¼Œè¦†ç›–æºæ•°æ®è‚¡ç¥¨ä»£ç ï¼›å¦‚ä¸æŒ‡å®šåˆ™ä»æ—¥å¿—â€œç›®æ ‡è‚¡ç¥¨â€åˆ—è¡¨å–é¦–ä¸ªã€‚
- --only-index: ä»…ç»˜åˆ¶æŒ‡å®šâ€œæœŸé—´#Xâ€çš„ç´¢å¼•ï¼ˆå¦‚1ï¼‰ï¼›ä¸æŒ‡å®šåˆ™ç»˜åˆ¶æ—¥å¿—ä¸­æ‰€æœ‰è§£æåˆ°çš„æœŸé—´ã€‚

æ³¨æ„ï¼š
- æ•°æ®åŠ è½½ä¾èµ– StockDataLoaderï¼Œéœ€ç¡®ä¿å†å²æ•°æ®CSVå·²ç”Ÿæˆå¹¶è·¯å¾„æœ‰æ•ˆã€‚
- è‹¥æ•°æ®ç¼ºå¤±ä¼šè·³è¿‡å¯¹åº”æœŸé—´å¹¶ç»™å‡ºæç¤ºã€‚
"""

import os
import re
import argparse
from datetime import datetime
from typing import List, Optional, Tuple, Dict

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import mplfinance as mpf

from data_loader import StockDataLoader


# å…¨å±€ä¸­æ–‡å­—ä½“é…ç½®ï¼Œé¿å…ä¸­æ–‡æ ‡é¢˜/æ ‡ç­¾æ— æ³•æ˜¾ç¤º
mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = [
    'Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi',
    'Arial Unicode MS', 'DejaVu Sans'
]
mpl.rcParams['axes.unicode_minus'] = False

# æ—¥å¿—è§£æçš„æ­£åˆ™æ¨¡å¼
RE_TARGET_STOCKS = re.compile(r"ç›®æ ‡è‚¡ç¥¨:\s*\[(.*?)\]")
RE_PROCESS_STOCK = re.compile(r"å¤„ç†è‚¡ç¥¨\s*\d+\s*:\s*(\d+)")
RE_EVAL_WINDOW = re.compile(r"è¯„æµ‹æ•°æ®çª—å£:\s*([0-9\-: ]+) åˆ° ([0-9\-: ]+)")
RE_PERIOD_DEBUG = re.compile(
    r"æœŸé—´#(?P<idx>\d+): è‚¡ç¥¨:(?P<stock>\d+), æœŸé—´:(?P<start>[0-9\-: ]+)~(?P<end>[0-9\-: ]+), ç›¸å…³ç³»æ•°:(?P<corr>[0-9\.]+)"
)
RE_PERIOD_INFO_BLOCK = re.compile(
    r"#(?P<idx>\d+) å†å²æœŸé—´ .*: (?P<start>[0-9\-: ]+) åˆ° (?P<end>[0-9\-: ]+)"
)
RE_SOURCE_STOCK_IN_BLOCK = re.compile(r"æ¥æºè‚¡ç¥¨:\s*(?P<stock>\d+)")


def parse_log(log_path: str) -> Tuple[Optional[str], Optional[str], List[Dict]]:
    """
    è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–ï¼š
    - æºæ•°æ®è¯„æµ‹çª—å£ï¼ˆstart_datetime_str, end_datetime_strï¼‰
    - æºæ•°æ®è‚¡ç¥¨ï¼ˆç›®æ ‡è‚¡ç¥¨åˆ—è¡¨é¦–ä¸ªï¼‰
    - é«˜ç›¸å…³æœŸé—´åˆ—è¡¨ï¼š[{idx, stock, start, end, corr}]
    """
    if not os.path.exists(log_path):
        raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")

    with open(log_path, 'r', encoding='utf-8-sig') as f:
        content = f.read()

    # è¯„æµ‹çª—å£
    eval_window_match = RE_EVAL_WINDOW.search(content)
    eval_start, eval_end = None, None
    if eval_window_match:
        eval_start = eval_window_match.group(1).strip()
        eval_end = eval_window_match.group(2).strip()

    # ä»ç»“æ„åŒ–æ—¥å¿—è‡ªåŠ¨ç¡®å®šæºè‚¡ç¥¨ï¼š
    # ä¼˜å…ˆä½¿ç”¨â€œğŸ“ [è¯¦ç»†ç»“æœæ„å»º] å¤„ç†è‚¡ç¥¨X: CODEâ€ï¼Œå¦åˆ™å›é€€åˆ°â€œç›®æ ‡è‚¡ç¥¨: [...]â€é¦–ä¸ªã€‚
    source_stock: Optional[str] = None

    proc_match = RE_PROCESS_STOCK.search(content)
    if proc_match:
        source_stock = proc_match.group(1)
    else:
        tgt_match = RE_TARGET_STOCKS.search(content)
        if tgt_match:
            raw = tgt_match.group(1)
            # å»æ‰å¼•å·å’Œç©ºæ ¼ï¼Œåˆ†å‰²åˆ—è¡¨
            items = [s.strip().strip("'").strip('"') for s in raw.split(',') if s.strip()]
            if items:
                source_stock = items[0]

    periods: List[Dict] = []

    # è§£æ DEBUG æ ·å¼çš„æœŸé—´è¡Œ
    for m in RE_PERIOD_DEBUG.finditer(content):
        periods.append({
            'idx': int(m.group('idx')),
            'stock': m.group('stock'),
            'start': m.group('start').strip(),
            'end': m.group('end').strip(),
            'corr': float(m.group('corr')),
        })

    # è§£æ INFO æ ·å¼çš„æœŸé—´å—ï¼ˆ#n å†å²æœŸé—´ ... æ¥æºè‚¡ç¥¨: ...ï¼‰
    # ä¸ºäº†ç¨³å¦¥ï¼Œé€è¡Œæ‰«æï¼Œå°†å—å†…æ¥æºè‚¡ç¥¨æ‹¼æ¥åˆ°å¯¹åº”idx
    lines = content.splitlines()
    info_block_buffer: Dict[int, Dict] = {}
    for i, line in enumerate(lines):
        m = RE_PERIOD_INFO_BLOCK.search(line)
        if m:
            idx = int(m.group('idx'))
            info_block_buffer[idx] = {
                'idx': idx,
                'start': m.group('start').strip(),
                'end': m.group('end').strip(),
                'stock': None,
                'corr': None,  # å¹³å‡ç›¸å…³ç³»æ•°å¯èƒ½åœ¨åç»­è¡Œ
            }
            # å‘åæ£€æŸ¥è‹¥å¹²è¡Œä»¥æ‰¾â€œæ¥æºè‚¡ç¥¨â€ä¸â€œå¹³å‡ç›¸å…³ç³»æ•°â€
            j = i + 1
            while j < len(lines) and (lines[j].strip().startswith('202') or lines[j].strip().startswith('ğŸ”') or 'INFO' in lines[j] or 'DEBUG' in lines[j]):
                ms = RE_SOURCE_STOCK_IN_BLOCK.search(lines[j])
                if ms:
                    info_block_buffer[idx]['stock'] = ms.group('stock')
                if 'å¹³å‡ç›¸å…³ç³»æ•°:' in lines[j]:
                    try:
                        info_block_buffer[idx]['corr'] = float(lines[j].split('å¹³å‡ç›¸å…³ç³»æ•°:')[-1].strip())
                    except Exception:
                        pass
                # åˆ°ä¸‹ä¸€ä¸ªå—çš„åˆ†éš”å°±åœ
                if lines[j].strip().startswith('------------------------------------------------------------'):
                    break
                j += 1

    # åˆå¹¶ INFO å—åˆ° periods åˆ—è¡¨ï¼ˆè‹¥è¯¥ idx æœªå‡ºç°äº DEBUG åˆ—è¡¨ä¸­ï¼‰
    existing_idxs = {p['idx'] for p in periods}
    for idx, info in info_block_buffer.items():
        if idx not in existing_idxs and info.get('stock') and info.get('start') and info.get('end'):
            periods.append(info)

    # æŒ‰ idx æ’åº
    periods.sort(key=lambda x: x['idx'])

    return eval_start, source_stock, periods


def _to_date_str(dt_str: str) -> str:
    """å°†æ—¥å¿—ä¸­çš„æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸º YYYY-MM-DDï¼ˆå»æ‰æ—¶é—´éƒ¨åˆ†ï¼‰ã€‚"""
    if not dt_str:
        return dt_str
    try:
        # æ—¥å¿—æ ¼å¼é€šå¸¸ä¸º "YYYY-MM-DD 00:00:00"
        return datetime.strptime(dt_str.strip(), '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d')
    except ValueError:
        # å…œåº•ï¼šè‹¥å·²æ˜¯æ—¥æœŸæ ¼å¼ï¼Œç›´æ¥è¿”å›
        return dt_str.strip().split(' ')[0]


def prepare_ohlcv(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    å°† DataFrame è½¬æ¢ä¸º mplfinance éœ€è¦çš„ OHLCV åˆ—æ ¼å¼ã€‚
    è‹¥ä¸å­˜åœ¨é«˜ä½ä»·ï¼Œåˆ™ä»¥ open/close åˆæˆï¼š
    high=max(open, close), low=min(open, close)
    """
    if df is None or df.empty:
        return None

    # éœ€è¦ open/close/volume ä¸‰åˆ—
    needed = ['open', 'close', 'volume']
    for col in needed:
        if col not in df.columns:
            return None

    out = pd.DataFrame(index=df.index.copy())
    out['Open'] = df['open']
    out['Close'] = df['close']
    out['Volume'] = df['volume'] if 'volume' in df.columns else 0
    # åˆæˆ High/Low
    high = pd.concat([df['open'], df['close']], axis=1).max(axis=1)
    low = pd.concat([df['open'], df['close']], axis=1).min(axis=1)
    out['High'] = high
    out['Low'] = low
    return out[['Open', 'High', 'Low', 'Close', 'Volume']]


def pad_with_blank_rows(df: pd.DataFrame, target_len: int) -> pd.DataFrame:
    """
    å°† df æœ«å°¾ç”¨â€œç©ºç™½è¡Œâ€(OHLCV ä¸º NaN)è¡¥é½åˆ° target_lenï¼Œ
    ä»¥ä¿è¯ä¸å¦ä¸€é¢æ¿çš„Kçº¿æ•°é‡å¯¹é½ã€‚

    è§„åˆ™ï¼š
    - è‹¥ df å·²>=target_lenï¼Œä¸åšå¤„ç†ã€‚
    - ä½¿ç”¨â€œå·¥ä½œæ—¥â€é¢‘ç‡(B)ç”Ÿæˆè¡¥é½ç´¢å¼•ï¼Œä» df æœ€åä¸€ä¸ªæ—¥æœŸçš„ä¸‹ä¸€å·¥ä½œæ—¥èµ·è¿ç»­è¡¥é½ã€‚
    - è‹¥ç´¢å¼•ä¸æ˜¯ DatetimeIndexï¼Œå°è¯•è½¬æ¢ä¸º datetimeï¼›å¤±è´¥åˆ™ä¿æŒåŸç´¢å¼•å¹¶ç›´æ¥è¿”å›ï¼ˆä¸è¡¥é½ï¼‰ã€‚
    """
    if df is None or df.empty:
        return df

    cur_len = len(df)
    if target_len <= cur_len:
        return df

    # ç¡®ä¿ç´¢å¼•ä¸º DatetimeIndex
    try:
        if not isinstance(df.index, pd.DatetimeIndex):
            df = df.copy()
            df.index = pd.to_datetime(df.index)
    except Exception:
        # ç´¢å¼•è½¬æ¢å¤±è´¥åˆ™è·³è¿‡è¡¥é½ï¼Œé¿å…ç ´åæ•°æ®
        return df

    last_dt = df.index[-1]
    need = target_len - cur_len
    # ä»ä¸‹ä¸€å·¥ä½œæ—¥å¼€å§‹è¡¥é½
    pad_index = pd.bdate_range(last_dt + pd.Timedelta(days=1), periods=need)
    blank = pd.DataFrame(index=pad_index, columns=df.columns)
    # OHLCV å…¨éƒ¨ NaNï¼Œmplfinance ä¼šåœ¨ç›¸åº”ä½ç½®ä¸ç»˜åˆ¶èœ¡çƒ›ï¼Œä½†å ä½ç¡®ä¿æ•°é‡å¯¹é½
    return pd.concat([df, blank])


def plot_two_panels(source_ohlcv: pd.DataFrame,
                    hist_ohlcv: pd.DataFrame,
                    title_top: str,
                    title_bottom: str,
                    save_path: str) -> None:
    """ç»˜åˆ¶ä¸Šä¸‹ä¸¤ä¸ªé¢æ¿çš„èœ¡çƒ›å›¾ä¸æˆäº¤é‡ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    # æ„é€ å›¾ä¸å››ä¸ªå­è½´ï¼šä¸Šä»·ã€ä¸Šé‡ã€ä¸‹ä»·ã€ä¸‹é‡
    fig = plt.figure(figsize=(12, 8))
    gs = fig.add_gridspec(4, 1, height_ratios=[3, 1, 3, 1], hspace=0.35)

    ax_price_top = fig.add_subplot(gs[0])
    ax_vol_top = fig.add_subplot(gs[1], sharex=ax_price_top)
    ax_price_bottom = fig.add_subplot(gs[2])
    ax_vol_bottom = fig.add_subplot(gs[3], sharex=ax_price_bottom)

    # è®¾ç½®çº¢æ¶¨ç»¿è·Œçš„é…è‰²
    mc = mpf.make_marketcolors(
        up='red',
        down='green',
        edge='inherit',
        wick='inherit',
        volume='inherit'
    )
    style_rg = mpf.make_mpf_style(marketcolors=mc)

    # ä¸Šé¢æ¿
    mpf.plot(source_ohlcv, type='candle', ax=ax_price_top, volume=ax_vol_top,
             style=style_rg, xrotation=0, datetime_format='%Y-%m-%d')
    ax_price_top.set_title(title_top, fontsize=11)

    # ä¸‹é¢æ¿
    mpf.plot(hist_ohlcv, type='candle', ax=ax_price_bottom, volume=ax_vol_bottom,
             style=style_rg, xrotation=0, datetime_format='%Y-%m-%d')
    ax_price_bottom.set_title(title_bottom, fontsize=11)

    # ä¿å­˜
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    fig.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close(fig)


def main():
    parser = argparse.ArgumentParser(description='ä»Pearsonåˆ†ææ—¥å¿—ç»˜åˆ¶ä¸Šä¸‹å¯¹æ¯”Kçº¿å›¾')
    parser.add_argument('--log', required=True, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default=None, help='è¾“å‡ºå›¾ç‰‡ç›®å½•')
    parser.add_argument('--source-stock', default=None, help='è¦†ç›–æºæ•°æ®è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--only-index', type=int, default=None, help='ä»…ç»˜åˆ¶æŒ‡å®šæœŸé—´ç´¢å¼•ï¼Œå¦‚ 1')
    args = parser.parse_args()

    log_path = args.log
    out_dir = args.output_dir

    eval_start, inferred_source_stock, periods = parse_log(log_path)

    # æºè‚¡ç¥¨ç¡®å®šä¼˜å…ˆçº§ï¼šå‚æ•°è¦†ç›– > æ—¥å¿—ä¸­ç›®æ ‡è‚¡ç¥¨é¦–ä¸ª
    source_stock = args.source_stock or inferred_source_stock
    if source_stock is None:
        raise ValueError('æ— æ³•ç¡®å®šæºæ•°æ®è‚¡ç¥¨ï¼Œè¯·é€šè¿‡ --source-stock æŒ‡å®šæˆ–ç¡®ä¿æ—¥å¿—åŒ…å«â€œç›®æ ‡è‚¡ç¥¨â€è¡Œã€‚')

    if eval_start is None:
        raise ValueError('æ— æ³•åœ¨æ—¥å¿—ä¸­è§£æâ€œè¯„æµ‹æ•°æ®çª—å£â€ï¼Œè¯·ç¡®è®¤æ—¥å¿—åŒ…å«è¯¥è¡Œã€‚')

    # è¯„æµ‹çª—å£æ—¥æœŸï¼ˆæˆªå»æ—¶é—´éƒ¨åˆ†ï¼‰
    eval_start_date = _to_date_str(eval_start)
    # eval_end åœ¨æ—¥å¿—ä¸­å­˜åœ¨ï¼Œä½†æˆ‘ä»¬ç›´æ¥æŒ‰ç´¢å¼•åˆ‡ç‰‡æ›´ç¨³å¦¥ï¼›è‹¥è§£æåˆ°åˆ™ä½¿ç”¨
    eval_end_date = None
    # ä»æ—¥å¿—å†å–ä¸€æ¬¡ end
    eval_window_match = RE_EVAL_WINDOW.search(open(log_path, 'r', encoding='utf-8-sig').read())
    if eval_window_match:
        eval_end_date = _to_date_str(eval_window_match.group(2).strip())

    if not periods:
        raise ValueError('æ—¥å¿—ä¸­æœªè§£æåˆ°ä»»ä½•é«˜ç›¸å…³æœŸé—´è®°å½•ã€‚')

    # è¾“å‡ºç›®å½•
    if out_dir is None:
        out_dir = os.path.join(os.path.dirname(log_path), 'kline_plots')
    os.makedirs(out_dir, exist_ok=True)

    # åŠ è½½å™¨
    loader = StockDataLoader()

    # åŠ è½½æºè‚¡ç¥¨å…¨é‡æ•°æ®å¹¶æŒ‰è¯„æµ‹çª—å£æˆªå–ï¼ˆç´¢å¼•ä¸ºdatetimeï¼‰
    src_df_full = loader.load_stock_data(source_stock, time_frame='daily')
    if src_df_full is None or src_df_full.empty:
        raise ValueError(f'æ— æ³•åŠ è½½æºè‚¡ç¥¨ {source_stock} çš„æ—¥çº¿æ•°æ®ã€‚')

    # åªä¿ç•™ open/close/volume
    src_df_full = src_df_full[['open', 'close', 'volume']].copy()

    # è¿‡æ»¤è¯„æµ‹çª—å£ï¼šä» eval_start_date èµ·ï¼Œé•¿åº¦ä¸å†å²æœŸé—´é•¿åº¦ä¸€è‡´ï¼›è‹¥ eval_end_date å¯ç”¨åˆ™æŒ‰åŒºé—´
    # æ—¥å¿—ä¸­çª—å£å¤§å°é€šå¸¸ä¸ºå›ºå®šå€¼ï¼ˆå¦‚15ï¼‰ï¼›æˆ‘ä»¬æ ¹æ®æ¯ä¸ªå†å²æœŸé—´çš„é•¿åº¦æ¥ç¡®å®šæºçª—å£é•¿åº¦

    drawn_count = 0
    for period in periods:
        if args.only_index is not None and period['idx'] != args.only_index:
            continue

        hist_stock = period['stock']
        hist_start_date = _to_date_str(period['start'])
        hist_end_date = _to_date_str(period['end'])
        corr_value = period.get('corr')

        # åŠ è½½å†å²è‚¡ç¥¨çª—å£æ•°æ®
        hist_df_full = loader.load_stock_data(hist_stock, time_frame='daily')
        if hist_df_full is None or hist_df_full.empty:
            print(f"è·³è¿‡ æœŸé—´#{period['idx']} - æ— æ³•åŠ è½½å†å²è‚¡ç¥¨ {hist_stock} æ•°æ®")
            continue
        hist_df_full = hist_df_full[['open', 'close', 'volume']].copy()

        try:
            hist_slice = hist_df_full.loc[hist_start_date:hist_end_date]
        except Exception:
            # è‹¥ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œå°è¯•è½¬æ¢
            hist_df_full.index = pd.to_datetime(hist_df_full.index)
            hist_slice = hist_df_full.loc[hist_start_date:hist_end_date]

        if hist_slice.empty:
            print(f"è·³è¿‡ æœŸé—´#{period['idx']} - å†å²çª—å£åˆ‡ç‰‡ä¸ºç©º: {hist_stock} {hist_start_date}~{hist_end_date}")
            continue

        window_len = len(hist_slice)

        # å†å²é¢æ¿ï¼šåœ¨å¯¹æ¯”åŒºé—´åè¿½åŠ 10ä¸ªäº¤æ˜“æ—¥
        # ä¿æŒæºé¢æ¿é•¿åº¦ä¸å˜ï¼ˆä»æŒ‰ window_len å¯¹é½ï¼‰ï¼Œä»…åº•éƒ¨é¢æ¿æ‰©å±•
        # ç¡®ä¿ç´¢å¼•ä¸º DatetimeIndex
        if not isinstance(hist_df_full.index, pd.DatetimeIndex):
            hist_df_full.index = pd.to_datetime(hist_df_full.index)
        extra_after = pd.DataFrame()
        try:
            end_idx = hist_slice.index[-1]
            # æ‰¾åˆ°ç»“æŸæ—¥æœŸåœ¨å…¨é‡æ•°æ®ä¸­çš„ä½ç½®
            pos = hist_df_full.index.get_loc(end_idx)
            extra_after = hist_df_full.iloc[pos + 1: pos + 1 + 10]
        except Exception:
            extra_after = pd.DataFrame()

        # æºçª—å£åˆ‡ç‰‡ï¼šä»è¯„æµ‹çª—å£ç»ˆç‚¹å‘å‰å–åŒæ ·é•¿åº¦ï¼Œæˆ–ç›´æ¥æŒ‰è¯„æµ‹çª—å£çš„èµ·æ­¢æ—¥æœŸ
        if eval_end_date is not None:
            try:
                src_eval_slice = src_df_full.loc[eval_start_date:eval_end_date]
            except Exception:
                src_df_full.index = pd.to_datetime(src_df_full.index)
                src_eval_slice = src_df_full.loc[eval_start_date:eval_end_date]
        else:
            # è‹¥ç¼ºå°‘è¯„æµ‹çª—å£ç»“æŸæ—¥æœŸï¼Œåˆ™ä»¥è¯„æµ‹å¼€å§‹æ—¥æœŸä½œä¸ºçª—å£æœ«å°¾ï¼Œå‘å‰å– window_len
            src_df_full.index = pd.to_datetime(src_df_full.index)
            # æ‰¾åˆ°è¯„æµ‹å¼€å§‹æ—¥æœŸåœ¨ç´¢å¼•ä¸­çš„ä½ç½®
            if eval_start_date in src_df_full.index.strftime('%Y-%m-%d'):
                # å®šä½è¯¥æ—¥æœŸçš„ç´¢å¼•ä½ç½®
                idx_pos = src_df_full.index.strftime('%Y-%m-%d').tolist().index(eval_start_date)
                start_pos = max(0, idx_pos - window_len + 1)
                src_eval_slice = src_df_full.iloc[start_pos:idx_pos + 1]
            else:
                # è‹¥æ‰¾ä¸åˆ°ï¼Œç›´æ¥å–æœ€å window_len æ¡ä½œä¸ºè¯„æµ‹çª—å£
                src_eval_slice = src_df_full.iloc[-window_len:]

        # å¯¹é½é•¿åº¦ï¼šè‹¥è¯„æµ‹çª—å£ä¸å†å²çª—å£é•¿åº¦ä¸ä¸€è‡´ï¼Œå°½é‡æˆªæ–­ä¸ºç›¸åŒé•¿åº¦
        min_len = min(len(src_eval_slice), window_len)
        src_eval_slice = src_eval_slice.tail(min_len)
        # å†å²å¯¹æ¯”ç”¨äºå¯¹é½çš„ä¸»ä½“ï¼ˆä¸æºåŒé•¿åº¦ï¼‰
        hist_aligned = hist_slice.tail(min_len)
        # å†å²é¢æ¿ç”¨äºç»˜å›¾çš„åˆ‡ç‰‡ï¼ˆé™„åŠ åç»­10æ—¥ï¼‰
        hist_plot_slice = pd.concat([hist_aligned, extra_after]) if not extra_after.empty else hist_aligned

        if min_len < 3:
            print(f"è·³è¿‡ æœŸé—´#{period['idx']} - æœ‰æ•ˆé•¿åº¦è¿‡çŸ­: {min_len}")
            continue

        # ä¸ºä¿è¯ä¸Šä¸‹é¢æ¿çš„Kçº¿æ•°é‡å¯¹é½ï¼š
        # è‹¥åº•éƒ¨é¢æ¿ï¼ˆhist_plot_sliceï¼‰æ¯”é¡¶éƒ¨ï¼ˆsrc_eval_sliceï¼‰æ›´é•¿ï¼Œåˆ™ä¸ºé¡¶éƒ¨è¡¥é½ç©ºç™½è¡Œ
        src_eval_slice = pad_with_blank_rows(src_eval_slice, len(hist_plot_slice))

        # å‡†å¤‡OHLCV
        src_ohlcv = prepare_ohlcv(src_eval_slice)
        hist_ohlcv = prepare_ohlcv(hist_plot_slice)
        if src_ohlcv is None or hist_ohlcv is None:
            print(f"è·³è¿‡ æœŸé—´#{period['idx']} - OHLCVå‡†å¤‡å¤±è´¥ï¼ˆç¼ºå°‘åˆ—ï¼‰")
            continue

        title_top = f"æºæ•°æ® {source_stock} | è¯„æµ‹çª—å£: {src_eval_slice.index.min().strftime('%Y-%m-%d')}~{src_eval_slice.index.max().strftime('%Y-%m-%d')}"
        title_bottom = (
            f"å†å² {hist_stock} | {hist_start_date}~{hist_end_date}"
            f" (+åç»­10äº¤æ˜“æ—¥) | ç›¸å…³: {corr_value if corr_value is not None else 'N/A'}"
        )

        # åœ¨æ–‡ä»¶åä¸­é™„åŠ ç›¸å…³ç³»æ•°ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œä¾¿äºæ£€ç´¢
        if corr_value is not None:
            corr_str = f"{corr_value:.6f}"
            save_name = (
                f"kline_compare_idx{period['idx']}_"
                f"{source_stock}_vs_{hist_stock}_"
                f"{hist_start_date}_{hist_end_date}_corr{corr_str}.png"
            )
        else:
            save_name = (
                f"kline_compare_idx{period['idx']}_"
                f"{source_stock}_vs_{hist_stock}_"
                f"{hist_start_date}_{hist_end_date}.png"
            )
        save_path = os.path.join(out_dir, save_name)

        try:
            plot_two_panels(src_ohlcv, hist_ohlcv, title_top, title_bottom, save_path)
            drawn_count += 1
            print(f"âœ… å·²ç”Ÿæˆ: {save_path}")
        except Exception as e:
            print(f"âŒ ç»˜åˆ¶å¤±è´¥ æœŸé—´#{period['idx']}: {e}")

    if drawn_count == 0:
        print('æœªç”Ÿæˆä»»ä½•å›¾åƒï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ ¼å¼ä¸æ•°æ®å¯ç”¨æ€§ã€‚')


if __name__ == '__main__':
    main()