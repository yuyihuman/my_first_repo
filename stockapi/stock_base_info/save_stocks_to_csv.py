from xtquant import xtdata
import pandas as pd
import os
from datetime import datetime
import time
import logging
import multiprocessing
from multiprocessing import Pool, Lock
import math

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªè¿›ç¨‹çš„æ—¥å¿—è®°å½•å™¨
process_loggers = {}

# åˆ›å»ºä¸€ä¸ªè¿›ç¨‹ç‰¹å®šçš„æ—¥å¿—è®°å½•å‡½æ•°
def safe_log(msg, level="info"):
    """è¿›ç¨‹ç‰¹å®šçš„æ—¥å¿—è®°å½•å‡½æ•°ï¼Œæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨è‡ªå·±çš„æ—¥å¿—è®°å½•å™¨"""
    # è·å–å½“å‰è¿›ç¨‹ID
    process_id = multiprocessing.current_process().name
    
    # å¦‚æœå½“å‰è¿›ç¨‹è¿˜æ²¡æœ‰æ—¥å¿—è®°å½•å™¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª
    if process_id not in process_loggers:
        process_loggers[process_id] = setup_process_logging(process_id)
    
    # ä½¿ç”¨è¿›ç¨‹ç‰¹å®šçš„æ—¥å¿—è®°å½•å™¨è®°å½•æ—¥å¿—
    logger = process_loggers[process_id]
    if level == "info":
        logger.info(msg)
    elif level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    elif level == "debug":
        logger.debug(msg)
    elif level == "critical":
        logger.critical(msg)

def save_stock_data_to_csv(stock_code, stock_name, base_folder="all_stocks_data"):
    """
    å°†å•ä¸ªè‚¡ç¥¨çš„æ•°æ®ä¿å­˜ä¸ºCSVæ–‡ä»¶
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        base_folder: æ•°æ®ä¿å­˜çš„åŸºç¡€æ–‡ä»¶å¤¹
    
    Returns:
        tuple: (success_count, total_attempts, detailed_results)
        detailed_results: dict with detailed status for each data type
    """
    safe_log(f"ğŸ”„ å¼€å§‹å¤„ç†è‚¡ç¥¨: {stock_code} ({stock_name})")
    
    # åˆ›å»ºè‚¡ç¥¨ä¸“ç”¨æ•°æ®æ–‡ä»¶å¤¹
    stock_folder = os.path.join(base_folder, f"stock_{stock_code}_data")
    if not os.path.exists(stock_folder):
        os.makedirs(stock_folder)
    
    success_count = 0
    total_attempts = 4  # 1åˆ†é’Ÿæ•°æ® + 5åˆ†é’Ÿæ•°æ® + 30åˆ†é’Ÿæ•°æ® + æ—¥çº¿æ•°æ®
    
    # è¯¦ç»†ç»“æœè®°å½•
    detailed_results = {
        'stock_code': stock_code,
        'stock_name': stock_name,
        'daily_data': {'status': 'pending', 'records': 0, 'error': None},
        '1min_data': {'status': 'pending', 'records': 0, 'error': None},
        '5min_data': {'status': 'pending', 'records': 0, 'error': None},
        '30min_data': {'status': 'pending', 'records': 0, 'error': None}
    }
    
    # æ„é€ è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆéœ€è¦æ·»åŠ äº¤æ˜“æ‰€åç¼€ï¼‰
    if stock_code.startswith('6'):
        full_code = f"{stock_code}.SH"  # ä¸Šæµ·äº¤æ˜“æ‰€ï¼ˆåŒ…æ‹¬ä¸»æ¿å’Œç§‘åˆ›æ¿ï¼‰
        exchange = "ä¸Šæµ·äº¤æ˜“æ‰€"
    elif stock_code.startswith('0') or stock_code.startswith('3'):
        full_code = f"{stock_code}.SZ"  # æ·±åœ³äº¤æ˜“æ‰€
        exchange = "æ·±åœ³äº¤æ˜“æ‰€"
    else:
        error_msg = f"ä¸æ”¯æŒçš„è‚¡ç¥¨ä»£ç æ ¼å¼: {stock_code}"
        safe_log(f"âŒ {error_msg}", "warning")
        for key in ['daily_data', '1min_data', '5min_data', '30min_data']:
            detailed_results[key]['status'] = 'failed'
            detailed_results[key]['error'] = error_msg
        return 0, 0, detailed_results
    
    safe_log(f"ğŸ“Š è‚¡ç¥¨ä¿¡æ¯: {stock_code} ({stock_name}) - {exchange} - å®Œæ•´ä»£ç : {full_code}")
    
    # 1. è·å–æ—¥çº¿æ•°æ®
    safe_log(f"ğŸ“ˆ [1/4] å¼€å§‹è·å–æ—¥çº¿æ•°æ®...")
    try:
        daily_data = xtdata.get_market_data([], [full_code], period='1d', start_time='19900101', dividend_type='none')
        
        if daily_data and isinstance(daily_data, dict):
            try:
                # è·å–æ—¶é—´åºåˆ—ï¼ˆæ—¥æœŸï¼‰
                time_df = daily_data.get('time')
                if time_df is not None and not time_df.empty:
                    # è·å–è‚¡ç¥¨åœ¨DataFrameä¸­çš„æ•°æ®
                    if full_code in time_df.index:
                        dates = time_df.loc[full_code].values
                        
                        # æ„å»ºæ–°çš„DataFrameï¼Œè¡Œä¸ºæ—¥æœŸï¼Œåˆ—ä¸ºå„ä¸ªæŒ‡æ ‡
                        df_data = {'date': dates}
                        
                        # æå–å„ä¸ªå­—æ®µçš„æ•°æ®
                        for field_name, field_df in daily_data.items():
                            if field_name != 'time' and field_df is not None and not field_df.empty:
                                if full_code in field_df.index:
                                    df_data[field_name] = field_df.loc[full_code].values
                        
                        # åˆ›å»ºæœ€ç»ˆçš„DataFrame
                        daily_df = pd.DataFrame(df_data)
                        
                        # æŒ‰æ—¶é—´æ’åºï¼ˆç¡®ä¿æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰
                        daily_df = daily_df.sort_values('date').reset_index(drop=True)
                        
                        # æ·»åŠ å¯è¯»çš„æ—¥æœŸæ—¶é—´åˆ—ï¼ˆç´§è·Ÿåœ¨dateåˆ—åé¢ï¼‰
                        if 'date' in daily_df.columns:
                            datetime_col = pd.to_datetime(daily_df['date'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
                            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†datetimeæ”¾åœ¨dateåé¢
                            cols = list(daily_df.columns)
                            date_idx = cols.index('date')
                            cols.insert(date_idx + 1, 'datetime')
                            daily_df['datetime'] = datetime_col
                            daily_df = daily_df[cols]
                        
                        # è®¡ç®—ç§»åŠ¨å¹³å‡å€¼ï¼ˆå¦‚æœæœ‰æ”¶ç›˜ä»·æ•°æ®ï¼‰
                        if 'close' in daily_df.columns:
                            data_count = len(daily_df)
                            # åªæœ‰åœ¨æ•°æ®é‡è¶³å¤Ÿæ—¶æ‰è®¡ç®—ç›¸åº”çš„ç§»åŠ¨å¹³å‡å€¼
                            if data_count >= 5:
                                daily_df['close_5d_avg'] = daily_df['close'].rolling(window=5, min_periods=5).mean()
                            if data_count >= 10:
                                daily_df['close_10d_avg'] = daily_df['close'].rolling(window=10, min_periods=10).mean()
                            if data_count >= 20:
                                daily_df['close_20d_avg'] = daily_df['close'].rolling(window=20, min_periods=20).mean()
                            if data_count >= 30:
                                daily_df['close_30d_avg'] = daily_df['close'].rolling(window=30, min_periods=30).mean()
                            if data_count >= 60:
                                daily_df['close_60d_avg'] = daily_df['close'].rolling(window=60, min_periods=60).mean()
                        
                        # è®¡ç®—æˆäº¤é‡ç§»åŠ¨å¹³å‡å€¼ï¼ˆå¦‚æœæœ‰æˆäº¤é‡æ•°æ®ï¼‰
                        if 'volume' in daily_df.columns:
                            data_count = len(daily_df)
                            # åªæœ‰åœ¨æ•°æ®é‡è¶³å¤Ÿæ—¶æ‰è®¡ç®—ç›¸åº”çš„æˆäº¤é‡ç§»åŠ¨å¹³å‡å€¼
                            if data_count >= 5:
                                daily_df['volume_5d_avg'] = daily_df['volume'].rolling(window=5, min_periods=5).mean()
                            if data_count >= 10:
                                daily_df['volume_10d_avg'] = daily_df['volume'].rolling(window=10, min_periods=10).mean()
                            if data_count >= 20:
                                daily_df['volume_20d_avg'] = daily_df['volume'].rolling(window=20, min_periods=20).mean()
                            if data_count >= 30:
                                daily_df['volume_30d_avg'] = daily_df['volume'].rolling(window=30, min_periods=30).mean()
                            if data_count >= 60:
                                daily_df['volume_60d_avg'] = daily_df['volume'].rolling(window=60, min_periods=60).mean()
                        
                        # è®¡ç®—å¼€ç›˜ä»·ç§»åŠ¨å¹³å‡å€¼ï¼ˆå¦‚æœæœ‰å¼€ç›˜ä»·æ•°æ®ï¼‰
                        if 'open' in daily_df.columns:
                            data_count = len(daily_df)
                            # åªæœ‰åœ¨æ•°æ®é‡è¶³å¤Ÿæ—¶æ‰è®¡ç®—ç›¸åº”çš„ç§»åŠ¨å¹³å‡å€¼
                            if data_count >= 5:
                                daily_df['open_5d_avg'] = daily_df['open'].rolling(window=5, min_periods=5).mean()
                            if data_count >= 10:
                                daily_df['open_10d_avg'] = daily_df['open'].rolling(window=10, min_periods=10).mean()
                            if data_count >= 20:
                                daily_df['open_20d_avg'] = daily_df['open'].rolling(window=20, min_periods=20).mean()
                            if data_count >= 30:
                                daily_df['open_30d_avg'] = daily_df['open'].rolling(window=30, min_periods=30).mean()
                            if data_count >= 60:
                                daily_df['open_60d_avg'] = daily_df['open'].rolling(window=60, min_periods=60).mean()
                        
                        daily_filename = os.path.join(stock_folder, f"{stock_code}_daily_history.csv")
                        daily_df.to_csv(daily_filename, encoding='utf-8-sig', index=False)
                        
                        record_count = len(daily_df)
                        detailed_results['daily_data']['status'] = 'success'
                        detailed_results['daily_data']['records'] = record_count
                        safe_log(f"âœ… æ—¥çº¿æ•°æ®è·å–æˆåŠŸ: {record_count} æ¡è®°å½•")
                        success_count += 1
                    else:
                        error_msg = f"è‚¡ç¥¨ä»£ç  {full_code} ä¸åœ¨è¿”å›æ•°æ®ä¸­"
                        detailed_results['daily_data']['status'] = 'failed'
                        detailed_results['daily_data']['error'] = error_msg
                        safe_log(f"âŒ æ—¥çº¿æ•°æ®è·å–å¤±è´¥: {error_msg}", "error")
                else:
                    error_msg = "æ—¶é—´æ•°æ®ä¸ºç©º"
                    detailed_results['daily_data']['status'] = 'failed'
                    detailed_results['daily_data']['error'] = error_msg
                    safe_log(f"âŒ æ—¥çº¿æ•°æ®è·å–å¤±è´¥: {error_msg}", "error")
            except Exception as e:
                error_msg = f"æ—¥çº¿æ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
                detailed_results['daily_data']['status'] = 'failed'
                detailed_results['daily_data']['error'] = error_msg
                safe_log(f"âŒ {error_msg}", "error")
        else:
            error_msg = "æ—¥çº¿æ•°æ®è·å–å¤±è´¥: æ— æ•°æ®è¿”å›"
            detailed_results['daily_data']['status'] = 'failed'
            detailed_results['daily_data']['error'] = error_msg
            safe_log(f"âŒ {error_msg}", "error")
    except Exception as e:
        error_msg = f"æ—¥çº¿æ•°æ®è·å–å¼‚å¸¸: {str(e)}"
        detailed_results['daily_data']['status'] = 'failed'
        detailed_results['daily_data']['error'] = error_msg
        safe_log(f"âŒ {error_msg}", "error")
    
    # 2. è·å–1åˆ†é’Ÿæ•°æ®
    safe_log(f"ğŸ“Š [2/4] å¼€å§‹è·å–1åˆ†é’Ÿæ•°æ®...")
    try:
        minute_data = xtdata.get_market_data([], [full_code], period='1m', start_time='19900101', dividend_type='none')
        
        if minute_data and isinstance(minute_data, dict):
            try:
                # è·å–æ—¶é—´åºåˆ—
                time_df = minute_data.get('time')
                if time_df is not None and not time_df.empty:
                    # è·å–è‚¡ç¥¨åœ¨DataFrameä¸­çš„æ•°æ®
                    if full_code in time_df.index:
                        times = time_df.loc[full_code].values
                        
                        # æ„å»ºæ–°çš„DataFrameï¼Œè¡Œä¸ºæ—¶é—´ï¼Œåˆ—ä¸ºå„ä¸ªæŒ‡æ ‡
                        df_data = {'time': times}
                        
                        # æå–å„ä¸ªå­—æ®µçš„æ•°æ®
                        for field_name, field_df in minute_data.items():
                            if field_name != 'time' and field_df is not None and not field_df.empty:
                                if full_code in field_df.index:
                                    df_data[field_name] = field_df.loc[full_code].values
                        
                        # åˆ›å»ºæœ€ç»ˆçš„DataFrame
                        minute_df = pd.DataFrame(df_data)
                        
                        # æ·»åŠ å¯è¯»çš„æ—¥æœŸæ—¶é—´åˆ—ï¼ˆç´§è·Ÿåœ¨timeåˆ—åé¢ï¼‰
                        if 'time' in minute_df.columns:
                            datetime_col = pd.to_datetime(minute_df['time'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
                            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†datetimeæ”¾åœ¨timeåé¢
                            cols = list(minute_df.columns)
                            time_idx = cols.index('time')
                            cols.insert(time_idx + 1, 'datetime')
                            minute_df['datetime'] = datetime_col
                            minute_df = minute_df[cols]
                        
                        minute_filename = os.path.join(stock_folder, f"{stock_code}_1minute_history.csv")
                        minute_df.to_csv(minute_filename, encoding='utf-8-sig', index=False)
                        
                        record_count = len(minute_df)
                        detailed_results['1min_data']['status'] = 'success'
                        detailed_results['1min_data']['records'] = record_count
                        safe_log(f"âœ… 1åˆ†é’Ÿæ•°æ®è·å–æˆåŠŸ: {record_count} æ¡è®°å½•")
                        success_count += 1
                    else:
                        error_msg = f"è‚¡ç¥¨ä»£ç  {full_code} ä¸åœ¨1åˆ†é’Ÿæ•°æ®ä¸­"
                        detailed_results['1min_data']['status'] = 'failed'
                        detailed_results['1min_data']['error'] = error_msg
                        safe_log(f"âš ï¸ 1åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
                else:
                    error_msg = "1åˆ†é’Ÿæ•°æ®æ—¶é—´åºåˆ—ä¸ºç©º"
                    detailed_results['1min_data']['status'] = 'failed'
                    detailed_results['1min_data']['error'] = error_msg
                    safe_log(f"âš ï¸ 1åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
            except Exception as e:
                error_msg = f"1åˆ†é’Ÿæ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
                detailed_results['1min_data']['status'] = 'failed'
                detailed_results['1min_data']['error'] = error_msg
                safe_log(f"âš ï¸ 1åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
        else:
            error_msg = "1åˆ†é’Ÿæ•°æ®ä¸å¯ç”¨"
            detailed_results['1min_data']['status'] = 'failed'
            detailed_results['1min_data']['error'] = error_msg
            safe_log(f"âš ï¸ 1åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
    except Exception as e:
        error_msg = f"1åˆ†é’Ÿæ•°æ®è·å–å¼‚å¸¸: {str(e)}"
        detailed_results['1min_data']['status'] = 'failed'
        detailed_results['1min_data']['error'] = error_msg
        safe_log(f"âš ï¸ 1åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
    
    # 3. è·å–5åˆ†é’Ÿæ•°æ®
    safe_log(f"ğŸ“Š [3/4] å¼€å§‹è·å–5åˆ†é’Ÿæ•°æ®...")
    try:
        minute_5_data = xtdata.get_market_data([], [full_code], period='5m', start_time='19900101', dividend_type='none')
        
        if minute_5_data and isinstance(minute_5_data, dict):
            try:
                # è·å–æ—¶é—´åºåˆ—
                time_df = minute_5_data.get('time')
                if time_df is not None and not time_df.empty:
                    # è·å–è‚¡ç¥¨åœ¨DataFrameä¸­çš„æ•°æ®
                    if full_code in time_df.index:
                        times = time_df.loc[full_code].values
                        
                        # æ„å»ºæ–°çš„DataFrameï¼Œè¡Œä¸ºæ—¶é—´ï¼Œåˆ—ä¸ºå„ä¸ªæŒ‡æ ‡
                        df_data = {'time': times}
                        
                        # æå–å„ä¸ªå­—æ®µçš„æ•°æ®
                        for field_name, field_df in minute_5_data.items():
                            if field_name != 'time' and field_df is not None and not field_df.empty:
                                if full_code in field_df.index:
                                    df_data[field_name] = field_df.loc[full_code].values
                        
                        # åˆ›å»ºæœ€ç»ˆçš„DataFrame
                        minute_5_df = pd.DataFrame(df_data)
                        
                        # æ·»åŠ å¯è¯»çš„æ—¥æœŸæ—¶é—´åˆ—ï¼ˆç´§è·Ÿåœ¨timeåˆ—åé¢ï¼‰
                        if 'time' in minute_5_df.columns:
                            datetime_col = pd.to_datetime(minute_5_df['time'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
                            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†datetimeæ”¾åœ¨timeåé¢
                            cols = list(minute_5_df.columns)
                            time_idx = cols.index('time')
                            cols.insert(time_idx + 1, 'datetime')
                            minute_5_df['datetime'] = datetime_col
                            minute_5_df = minute_5_df[cols]
                        
                        minute_5_filename = os.path.join(stock_folder, f"{stock_code}_5minute_history.csv")
                        minute_5_df.to_csv(minute_5_filename, encoding='utf-8-sig', index=False)
                        
                        record_count = len(minute_5_df)
                        detailed_results['5min_data']['status'] = 'success'
                        detailed_results['5min_data']['records'] = record_count
                        safe_log(f"âœ… 5åˆ†é’Ÿæ•°æ®è·å–æˆåŠŸ: {record_count} æ¡è®°å½•")
                        success_count += 1
                    else:
                        error_msg = f"è‚¡ç¥¨ä»£ç  {full_code} ä¸åœ¨5åˆ†é’Ÿæ•°æ®ä¸­"
                        detailed_results['5min_data']['status'] = 'failed'
                        detailed_results['5min_data']['error'] = error_msg
                        safe_log(f"âš ï¸ 5åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
                else:
                    error_msg = "5åˆ†é’Ÿæ•°æ®æ—¶é—´åºåˆ—ä¸ºç©º"
                    detailed_results['5min_data']['status'] = 'failed'
                    detailed_results['5min_data']['error'] = error_msg
                    safe_log(f"âš ï¸ 5åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
            except Exception as e:
                error_msg = f"5åˆ†é’Ÿæ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
                detailed_results['5min_data']['status'] = 'failed'
                detailed_results['5min_data']['error'] = error_msg
                safe_log(f"âš ï¸ 5åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
        else:
            error_msg = "5åˆ†é’Ÿæ•°æ®ä¸å¯ç”¨"
            detailed_results['5min_data']['status'] = 'failed'
            detailed_results['5min_data']['error'] = error_msg
            safe_log(f"âš ï¸ 5åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
    except Exception as e:
        error_msg = f"5åˆ†é’Ÿæ•°æ®è·å–å¼‚å¸¸: {str(e)}"
        detailed_results['5min_data']['status'] = 'failed'
        detailed_results['5min_data']['error'] = error_msg
        safe_log(f"âš ï¸ 5åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
    
    # 4. è·å–30åˆ†é’Ÿæ•°æ®
    safe_log(f"ğŸ“Š [4/4] å¼€å§‹è·å–30åˆ†é’Ÿæ•°æ®...")
    try:
        minute_30_data = xtdata.get_market_data([], [full_code], period='30m', start_time='19900101', dividend_type='none')
        
        if minute_30_data and isinstance(minute_30_data, dict):
            try:
                # è·å–æ—¶é—´åºåˆ—
                time_df = minute_30_data.get('time')
                if time_df is not None and not time_df.empty:
                    # è·å–è‚¡ç¥¨åœ¨DataFrameä¸­çš„æ•°æ®
                    if full_code in time_df.index:
                        times = time_df.loc[full_code].values
                        
                        # æ„å»ºæ–°çš„DataFrameï¼Œè¡Œä¸ºæ—¶é—´ï¼Œåˆ—ä¸ºå„ä¸ªæŒ‡æ ‡
                        df_data = {'time': times}
                        
                        # æå–å„ä¸ªå­—æ®µçš„æ•°æ®
                        for field_name, field_df in minute_30_data.items():
                            if field_name != 'time' and field_df is not None and not field_df.empty:
                                if full_code in field_df.index:
                                    df_data[field_name] = field_df.loc[full_code].values
                        
                        # åˆ›å»ºæœ€ç»ˆçš„DataFrame
                        minute_30_df = pd.DataFrame(df_data)
                        
                        # æ·»åŠ å¯è¯»çš„æ—¥æœŸæ—¶é—´åˆ—ï¼ˆç´§è·Ÿåœ¨timeåˆ—åé¢ï¼‰
                        if 'time' in minute_30_df.columns:
                            datetime_col = pd.to_datetime(minute_30_df['time'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')
                            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†datetimeæ”¾åœ¨timeåé¢
                            cols = list(minute_30_df.columns)
                            time_idx = cols.index('time')
                            cols.insert(time_idx + 1, 'datetime')
                            minute_30_df['datetime'] = datetime_col
                            minute_30_df = minute_30_df[cols]
                        
                        minute_30_filename = os.path.join(stock_folder, f"{stock_code}_30minute_history.csv")
                        minute_30_df.to_csv(minute_30_filename, encoding='utf-8-sig', index=False)
                        
                        record_count = len(minute_30_df)
                        detailed_results['30min_data']['status'] = 'success'
                        detailed_results['30min_data']['records'] = record_count
                        safe_log(f"âœ… 30åˆ†é’Ÿæ•°æ®è·å–æˆåŠŸ: {record_count} æ¡è®°å½•")
                        success_count += 1
                    else:
                        error_msg = f"è‚¡ç¥¨ä»£ç  {full_code} ä¸åœ¨30åˆ†é’Ÿæ•°æ®ä¸­"
                        detailed_results['30min_data']['status'] = 'failed'
                        detailed_results['30min_data']['error'] = error_msg
                        safe_log(f"âš ï¸ 30åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
                else:
                    error_msg = "30åˆ†é’Ÿæ•°æ®æ—¶é—´åºåˆ—ä¸ºç©º"
                    detailed_results['30min_data']['status'] = 'failed'
                    detailed_results['30min_data']['error'] = error_msg
                    safe_log(f"âš ï¸ 30åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
            except Exception as e:
                error_msg = f"30åˆ†é’Ÿæ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
                detailed_results['30min_data']['status'] = 'failed'
                detailed_results['30min_data']['error'] = error_msg
                safe_log(f"âš ï¸ 30åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
        else:
            error_msg = "30åˆ†é’Ÿæ•°æ®ä¸å¯ç”¨"
            detailed_results['30min_data']['status'] = 'failed'
            detailed_results['30min_data']['error'] = error_msg
            safe_log(f"âš ï¸ 30åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")
    except Exception as e:
        error_msg = f"30åˆ†é’Ÿæ•°æ®è·å–å¼‚å¸¸: {str(e)}"
        detailed_results['30min_data']['status'] = 'failed'
        detailed_results['30min_data']['error'] = error_msg
        safe_log(f"âš ï¸ 30åˆ†é’Ÿæ•°æ®è·³è¿‡: {error_msg}")

    # ç”Ÿæˆå•ä¸ªè‚¡ç¥¨çš„æ•°æ®æŠ¥å‘Š
    data_files_info = f"""è·å–çš„æ•°æ®æ–‡ä»¶:
1. {stock_code}_1minute_history.csv - 1åˆ†é’Ÿå†å²æ•°æ® (xtquant)
2. {stock_code}_5minute_history.csv - 5åˆ†é’Ÿå†å²æ•°æ® (xtquant)
3. {stock_code}_30minute_history.csv - 30åˆ†é’Ÿå†å²æ•°æ® (xtquant)
4. {stock_code}_daily_history.csv - æ—¥çº¿å†å²æ•°æ® (xtquant)"""
    encoding_info = "- æ–‡ä»¶ç¼–ç ï¼šUTF-8-BOMï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º"
    
    summary_content = f"""è‚¡ç¥¨ä»£ç : {stock_code}
è‚¡ç¥¨åç§°: {stock_name}
å®Œæ•´ä»£ç : {full_code}
æ•°æ®è·å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æˆåŠŸè·å–æ•°æ®ç±»å‹: {success_count}/{total_attempts}

{data_files_info}

æ•°æ®æ¥æºè¯´æ˜:
- å†å²ä»·æ ¼æ•°æ®ï¼šxtquant (è¿…æŠ•é‡åŒ–)
{encoding_info}
- æ•°æ®å‘¨æœŸï¼š1åˆ†é’ŸKçº¿ + 5åˆ†é’ŸKçº¿ + 30åˆ†é’ŸKçº¿ + æ—¥çº¿Kçº¿
"""
    
    report_filename = os.path.join(stock_folder, "data_summary.txt")
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(summary_content)
    
    # è®°å½•æœ€ç»ˆç»“æœ
    if success_count == total_attempts:
        safe_log(f"ğŸ‰ è‚¡ç¥¨ {stock_code} æ•°æ®ä¿å­˜å®Œæˆ: å…¨éƒ¨æˆåŠŸ ({success_count}/{total_attempts})")
    elif success_count > 0:
        safe_log(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ•°æ®ä¿å­˜å®Œæˆ: éƒ¨åˆ†æˆåŠŸ ({success_count}/{total_attempts})")
    else:
        safe_log(f"âŒ è‚¡ç¥¨ {stock_code} æ•°æ®ä¿å­˜å¤±è´¥: å…¨éƒ¨å¤±è´¥ ({success_count}/{total_attempts})", "error")
    
    return success_count, total_attempts, detailed_results

def clean_old_logs(logs_dir="logs", keep_days=7):
    """æ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶
    
    Args:
        logs_dir: æ—¥å¿—æ–‡ä»¶å¤¹è·¯å¾„
        keep_days: ä¿ç•™æœ€è¿‘å‡ å¤©çš„æ—¥å¿—ï¼Œé»˜è®¤7å¤©
    """
    if not os.path.exists(logs_dir):
        return
    
    current_time = time.time()
    cutoff_time = current_time - (keep_days * 24 * 60 * 60)  # è½¬æ¢ä¸ºç§’
    
    deleted_count = 0
    
    # æ¸…ç†ä¸»æ—¥å¿—æ–‡ä»¶å¤¹ä¸­çš„æ—§æ—¥å¿—
    for filename in os.listdir(logs_dir):
        if filename.endswith('.log'):
            file_path = os.path.join(logs_dir, filename)
            file_time = os.path.getmtime(file_path)
            
            if file_time < cutoff_time:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except Exception as e:
                    print(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    # æ¸…ç†è¿›ç¨‹æ—¥å¿—æ–‡ä»¶å¤¹ä¸­çš„æ—§æ—¥å¿—
    process_logs_dir = os.path.join(logs_dir, "process_logs")
    if os.path.exists(process_logs_dir):
        for filename in os.listdir(process_logs_dir):
            if filename.endswith('.log'):
                file_path = os.path.join(process_logs_dir, filename)
                file_time = os.path.getmtime(file_path)
                
                if file_time < cutoff_time:
                    try:
                        os.remove(file_path)
                        deleted_count += 1
                    except Exception as e:
                        print(f"åˆ é™¤è¿›ç¨‹æ—¥å¿—æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    if deleted_count > 0:
        print(f"å·²æ¸…ç† {deleted_count} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")

def setup_process_logging(process_id):
    """
    ä¸ºç‰¹å®šè¿›ç¨‹è®¾ç½®æ—¥å¿—é…ç½®
    
    Args:
        process_id: è¿›ç¨‹IDæˆ–åç§°
    
    Returns:
        logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
    """
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åˆ›å»ºè¿›ç¨‹æ—¥å¿—æ–‡ä»¶å¤¹ï¼ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹çš„logsæ–‡ä»¶å¤¹ä¸­ï¼‰
    logs_dir = os.path.join(script_dir, "logs")
    process_logs_dir = os.path.join(logs_dir, "process_logs")
    if not os.path.exists(process_logs_dir):
        os.makedirs(process_logs_dir)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³å’Œè¿›ç¨‹IDçš„æ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = os.path.join(process_logs_dir, f"save_stocks_csv_{process_id}_{timestamp}.log")
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger(f"process_{process_id}")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤
    logger.handlers.clear()
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)  # åªåœ¨æ§åˆ¶å°æ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
    console_formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
    logger.propagate = False
    
    return logger

def setup_logging():
    """
    è®¾ç½®ä¸»è¿›ç¨‹æ—¥å¿—é…ç½®
    """
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åˆ›å»ºlogsæ–‡ä»¶å¤¹ï¼ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹ï¼‰
    logs_dir = os.path.join(script_dir, "logs")
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
    
    # æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
    clean_old_logs(logs_dir)
    
    # åˆ›å»ºè¿›ç¨‹æ—¥å¿—æ–‡ä»¶å¤¹
    process_logs_dir = os.path.join(logs_dir, "process_logs")
    if not os.path.exists(process_logs_dir):
        os.makedirs(process_logs_dir)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = os.path.join(logs_dir, f"save_stocks_csv_main_{timestamp}.log")
    
    # é…ç½®ä¸»è¿›ç¨‹æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    # è·å–ä¸»æ—¥å¿—è®°å½•å™¨
    main_logger = logging.getLogger('main')
    main_logger.info(f"ğŸš€ ä¸»è¿›ç¨‹æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    main_logger.info(f"ğŸ“ ä¸»è¿›ç¨‹æ—¥å¿—æ–‡ä»¶: {log_filename}")
    main_logger.info(f"ğŸ“ è¿›ç¨‹æ—¥å¿—ç›®å½•: {process_logs_dir}")
    
    return log_filename

def process_stock_batch(stock_batch, base_folder):
    """
    å¤„ç†ä¸€æ‰¹è‚¡ç¥¨æ•°æ®
    
    Args:
        stock_batch: åŒ…å«è‚¡ç¥¨ä»£ç å’Œåç§°çš„DataFrameæ‰¹æ¬¡
        base_folder: æ•°æ®ä¿å­˜çš„åŸºç¡€æ–‡ä»¶å¤¹
    
    Returns:
        tuple: (successful_stocks, failed_stocks, total_success_count, total_attempts, detailed_results_list, failed_stocks_list)
    """
    process_id = multiprocessing.current_process().name
    batch_start_index = stock_batch.index[0] if not stock_batch.empty else 0
    batch_end_index = stock_batch.index[-1] if not stock_batch.empty else 0
    
    # ä¸ºå½“å‰è¿›ç¨‹è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨
    process_logger = setup_process_logging(process_id)
    
    # å°†è¿›ç¨‹æ—¥å¿—è®°å½•å™¨å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­ï¼Œä¾›safe_logä½¿ç”¨
    global process_loggers
    process_loggers[process_id] = process_logger
    
    # è®°å½•è¿›ç¨‹å¼€å§‹å¤„ç†çš„ä¿¡æ¯
    safe_log(f"{'='*80}")
    safe_log(f"ğŸš€ è¿›ç¨‹ {process_id} å¼€å§‹å¤„ç†æ‰¹æ¬¡ï¼ŒåŒ…å« {len(stock_batch)} åªè‚¡ç¥¨ï¼Œç´¢å¼•èŒƒå›´: {batch_start_index}-{batch_end_index}")
    safe_log(f"ğŸ“ è¿›ç¨‹ {process_id} æ—¥å¿—è®°å½•å™¨å·²åˆå§‹åŒ–")
    safe_log(f"{'='*80}")
    
    successful_stocks = 0
    failed_stocks = 0
    total_success_count = 0
    total_attempts = 0
    total_in_batch = len(stock_batch)
    processed_in_batch = 0
    
    # è¯¦ç»†ç»“æœåˆ—è¡¨å’Œå¤±è´¥è‚¡ç¥¨åˆ—è¡¨
    detailed_results_list = []
    failed_stocks_list = []
    
    for index, row in stock_batch.iterrows():
        stock_code = str(row['code']).zfill(6)  # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½æ•°å­—
        stock_name = row['name']
        
        processed_in_batch += 1
        safe_log(f"{'='*60}")
        safe_log(f"ğŸ“Š è¿›ç¨‹ {process_id} å¤„ç†è¿›åº¦: {processed_in_batch}/{total_in_batch} ({processed_in_batch/total_in_batch*100:.1f}%)")
        safe_log(f"ğŸ” è¿›ç¨‹ {process_id} å¤„ç†è‚¡ç¥¨: {stock_code} - {stock_name}")
        
        try:
            success_count, attempt_count, detailed_results = save_stock_data_to_csv(stock_code, stock_name, base_folder)
            total_success_count += success_count
            total_attempts += attempt_count
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results_list.append(detailed_results)
            
            if success_count > 0:
                successful_stocks += 1
                safe_log(f"âœ… è¿›ç¨‹ {process_id} - è‚¡ç¥¨ {stock_code} å¤„ç†æˆåŠŸ ({success_count}/{attempt_count})")
            else:
                failed_stocks += 1
                failed_stocks_list.append({
                    'stock_code': stock_code,
                    'stock_name': stock_name,
                    'reason': 'æ‰€æœ‰æ•°æ®ç±»å‹è·å–å¤±è´¥',
                    'details': detailed_results
                })
                safe_log(f"âŒ è¿›ç¨‹ {process_id} - è‚¡ç¥¨ {stock_code} å¤„ç†å¤±è´¥ (0/{attempt_count})", "error")
            
            # æ·»åŠ å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(1)
            
        except Exception as e:
            error_msg = f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            safe_log(f"ğŸ’¥ è¿›ç¨‹ {process_id} - {error_msg}", "error")
            failed_stocks += 1
            failed_stocks_list.append({
                'stock_code': stock_code,
                'stock_name': stock_name,
                'reason': f'å¤„ç†å¼‚å¸¸: {str(e)}',
                'details': None
            })
    
    # è®°å½•è¿›ç¨‹å®Œæˆå¤„ç†çš„ä¿¡æ¯
    safe_log(f"{'='*80}")
    safe_log(f"ğŸ è¿›ç¨‹ {process_id} å®Œæˆæ‰¹æ¬¡å¤„ç†")
    safe_log(f"ğŸ“ˆ æˆåŠŸå¤„ç†: {successful_stocks} åªè‚¡ç¥¨")
    safe_log(f"ğŸ“‰ å¤±è´¥å¤„ç†: {failed_stocks} åªè‚¡ç¥¨") 
    safe_log(f"ğŸ“Š æ€»è®¡å¤„ç†: {total_in_batch} åªè‚¡ç¥¨")
    safe_log(f"ğŸ¯ æ‰¹æ¬¡æˆåŠŸç‡: {successful_stocks/total_in_batch*100:.1f}%")
    safe_log(f"ğŸ“ è¿›ç¨‹ {process_id} æ—¥å¿—è®°å½•å®Œæˆ")
    safe_log(f"{'='*80}")
    
    # æ¸…ç†è¿›ç¨‹æ—¥å¿—è®°å½•å™¨
    if process_id in process_loggers:
        # å…³é—­æ‰€æœ‰å¤„ç†å™¨
        for handler in process_loggers[process_id].handlers:
            handler.close()
        # ä»å­—å…¸ä¸­ç§»é™¤
        del process_loggers[process_id]
    
    return (successful_stocks, failed_stocks, total_success_count, total_attempts, detailed_results_list, failed_stocks_list)

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰¹é‡å°†æ‰€æœ‰è‚¡ç¥¨æ•°æ®ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    """
    # è®¾ç½®æ—¥å¿—
    log_filename = setup_logging()
    logging.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_filename}")  # ä¸»è¿›ç¨‹æ—¥å¿—åˆå§‹åŒ–ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
    logging.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è‚¡ç¥¨æ•°æ®ä¸‹è½½ä»»åŠ¡")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # è¯»å–è‚¡ç¥¨åˆ—è¡¨CSVæ–‡ä»¶ï¼ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹ï¼‰
    csv_file = os.path.join(script_dir, "stock_data.csv")
    
    if not os.path.exists(csv_file):
        logging.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file}")  # ä¸»è¿›ç¨‹æ—¥å¿—ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
        return
    
    logging.info(f"ğŸ“‚ è¯»å–è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶: {csv_file}")  # ä¸»è¿›ç¨‹æ—¥å¿—ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
    
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file, encoding='utf-8')
        logging.info(f"ğŸ“Š å…±æ‰¾åˆ° {len(df)} åªè‚¡ç¥¨")  # ä¸»è¿›ç¨‹æ—¥å¿—ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
        
        # è¿‡æ»¤æ‰8å¼€å¤´å’Œ9å¼€å¤´çš„è‚¡ç¥¨ï¼ˆåŒ—äº¤æ‰€ç­‰ï¼‰
        original_count = len(df)
        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯6ä½æ•°å­—ï¼Œç„¶åæ£€æŸ¥ç¬¬ä¸€ä½æ•°å­—
        df['code_6digit'] = df['code'].astype(str).str.zfill(6)
        df = df[~df['code_6digit'].str[0].isin(['8', '9'])]
        df = df.drop('code_6digit', axis=1)  # åˆ é™¤ä¸´æ—¶åˆ—
        filtered_count = len(df)
        logging.info(f"ğŸ” è¿‡æ»¤8å¼€å¤´å’Œ9å¼€å¤´è‚¡ç¥¨åå‰©ä½™ {filtered_count} åªè‚¡ç¥¨ (è¿‡æ»¤æ‰ {original_count - filtered_count} åª)")  # ä¸»è¿›ç¨‹æ—¥å¿—ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
        
        # åˆ›å»ºæ€»çš„æ•°æ®æ–‡ä»¶å¤¹ï¼ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹ï¼‰
        base_folder = os.path.join(script_dir, "all_stocks_data")
        if not os.path.exists(base_folder):
            os.makedirs(base_folder)
            logging.info(f"ğŸ“ åˆ›å»ºæ€»æ–‡ä»¶å¤¹: {base_folder}")  # ä¸»è¿›ç¨‹æ—¥å¿—ï¼Œä¸éœ€è¦ä½¿ç”¨safe_log
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_stocks = len(df)
        
        # è®¾ç½®è¿›ç¨‹æ•°
        num_processes = 20
        logging.info(f"âš™ï¸ ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†è‚¡ç¥¨æ•°æ®")
        
        # å°†è‚¡ç¥¨åˆ—è¡¨åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
        batch_size = math.ceil(total_stocks / num_processes)
        batches = [df.iloc[i:i+batch_size] for i in range(0, total_stocks, batch_size)]
        logging.info(f"ğŸ“¦ å°† {total_stocks} åªè‚¡ç¥¨åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡çº¦ {batch_size} åªè‚¡ç¥¨")
        
        # ç¡®ä¿è¿›ç¨‹æ—¥å¿—æ–‡ä»¶å¤¹å­˜åœ¨
        logs_dir = os.path.join(script_dir, "logs")
        process_logs_dir = os.path.join(logs_dir, "process_logs")
        if not os.path.exists(process_logs_dir):
            os.makedirs(process_logs_dir)
            logging.info(f"ğŸ“ åˆ›å»ºè¿›ç¨‹æ—¥å¿—æ–‡ä»¶å¤¹: {process_logs_dir}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        logging.info(f"â° ä»»åŠ¡å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼Œä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®æ—¥å¿—è®°å½•å™¨
        with Pool(processes=num_processes) as pool:
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡æä¾›åŸºç¡€æ–‡ä»¶å¤¹å‚æ•°
            results = pool.starmap(process_stock_batch, [(batch, base_folder) for batch in batches])
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = datetime.now()
        duration = end_time - start_time
        logging.info(f"â° ä»»åŠ¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        
        # æ±‡æ€»ç»“æœ
        successful_stocks = sum(result[0] for result in results)
        failed_stocks = sum(result[1] for result in results)
        total_success_count = sum(result[2] for result in results)
        total_attempts = sum(result[3] for result in results)
        
        # æ±‡æ€»è¯¦ç»†ç»“æœå’Œå¤±è´¥åˆ—è¡¨
        all_detailed_results = []
        all_failed_stocks = []
        for result in results:
            all_detailed_results.extend(result[4])  # detailed_results_list
            all_failed_stocks.extend(result[5])     # failed_stocks_list
        
        # ç»Ÿè®¡å„ç§æ•°æ®ç±»å‹çš„æˆåŠŸç‡
        daily_success = sum(1 for r in all_detailed_results if r['daily_data']['status'] == 'success')
        min1_success = sum(1 for r in all_detailed_results if r['1min_data']['status'] == 'success')
        min5_success = sum(1 for r in all_detailed_results if r['5min_data']['status'] == 'success')
        min30_success = sum(1 for r in all_detailed_results if r['30min_data']['status'] == 'success')
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        logging.info(f"{'='*80}")
        logging.info("ğŸ‰ æ‰¹é‡æ•°æ®ä¿å­˜å®Œæˆï¼")
        logging.info(f"ğŸ“Š æ€»å…±å¤„ç†è‚¡ç¥¨: {total_stocks}")
        logging.info(f"âœ… æˆåŠŸä¿å­˜æ•°æ®çš„è‚¡ç¥¨: {successful_stocks}")
        logging.info(f"âŒ å¤±è´¥çš„è‚¡ç¥¨: {failed_stocks}")
        logging.info(f"ğŸ¯ æ€»ä½“æˆåŠŸç‡: {successful_stocks/total_stocks*100:.1f}%")
        logging.info(f"ğŸ“ˆ æ•°æ®ä¿å­˜æˆåŠŸç‡: {total_success_count/total_attempts*100:.1f}%")
        logging.info(f"")
        logging.info(f"ğŸ“‹ å„æ•°æ®ç±»å‹æˆåŠŸç‡:")
        logging.info(f"  ğŸ“ˆ æ—¥çº¿æ•°æ®: {daily_success}/{total_stocks} ({daily_success/total_stocks*100:.1f}%)")
        logging.info(f"  ğŸ“Š 1åˆ†é’Ÿæ•°æ®: {min1_success}/{total_stocks} ({min1_success/total_stocks*100:.1f}%)")
        logging.info(f"  ğŸ“Š 5åˆ†é’Ÿæ•°æ®: {min5_success}/{total_stocks} ({min5_success/total_stocks*100:.1f}%)")
        logging.info(f"  ğŸ“Š 30åˆ†é’Ÿæ•°æ®: {min30_success}/{total_stocks} ({min30_success/total_stocks*100:.1f}%)")
        logging.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        
        # å¦‚æœæœ‰å¤±è´¥çš„è‚¡ç¥¨ï¼Œè®°å½•å¤±è´¥è¯¦æƒ…
        if all_failed_stocks:
            logging.warning(f"âš ï¸ ä»¥ä¸‹ {len(all_failed_stocks)} åªè‚¡ç¥¨å¤„ç†å¤±è´¥:")
            for failed_stock in all_failed_stocks[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå¤±è´¥çš„è‚¡ç¥¨
                logging.warning(f"  âŒ {failed_stock['stock_code']} ({failed_stock['stock_name']}) - {failed_stock['reason']}")
            if len(all_failed_stocks) > 10:
                logging.warning(f"  ... è¿˜æœ‰ {len(all_failed_stocks) - 10} åªè‚¡ç¥¨å¤±è´¥ (è¯¦è§è¯¦ç»†æŠ¥å‘Š)")
        
        # ä¿å­˜è¯¦ç»†çš„å¤±è´¥æŠ¥å‘Š
        if all_failed_stocks:
            failed_report_filename = os.path.join(base_folder, "failed_stocks_report.txt")
            with open(failed_report_filename, 'w', encoding='utf-8') as f:
                f.write(f"å¤±è´¥è‚¡ç¥¨è¯¦ç»†æŠ¥å‘Š\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»å¤±è´¥æ•°é‡: {len(all_failed_stocks)}\n\n")
                
                for i, failed_stock in enumerate(all_failed_stocks, 1):
                    f.write(f"{i}. è‚¡ç¥¨ä»£ç : {failed_stock['stock_code']}\n")
                    f.write(f"   è‚¡ç¥¨åç§°: {failed_stock['stock_name']}\n")
                    f.write(f"   å¤±è´¥åŸå› : {failed_stock['reason']}\n")
                    
                    if failed_stock['details']:
                        f.write(f"   è¯¦ç»†ä¿¡æ¯:\n")
                        for data_type, info in failed_stock['details'].items():
                            if data_type not in ['stock_code', 'stock_name'] and info['status'] == 'failed':
                                f.write(f"     - {data_type}: {info['error']}\n")
                    f.write(f"\n")
            
            logging.info(f"ğŸ“„ å¤±è´¥è‚¡ç¥¨è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {failed_report_filename}")
        
        # ä¿å­˜æ€»ä½“æŠ¥å‘Š
        storage_info = f"æ•°æ®å­˜å‚¨ä½ç½®: {base_folder}/\næ¯ä¸ªè‚¡ç¥¨çš„æ•°æ®å­˜å‚¨åœ¨ç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹ä¸­"
        file_info = "- æ‰€æœ‰æ–‡ä»¶ä½¿ç”¨UTF-8-BOMç¼–ç "
        process_info = "- éœ€è¦å…ˆä¸‹è½½æ•°æ®åˆ°æœ¬åœ°ï¼Œç„¶åè¯»å–ä¿å­˜ä¸ºCSVæ–‡ä»¶"
        
        overall_report = f"""æ‰¹é‡è‚¡ç¥¨å¤šå‘¨æœŸæ•°æ®ä¿å­˜æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ä»»åŠ¡å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}
ä»»åŠ¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
æ€»è€—æ—¶: {duration}

ç»Ÿè®¡ä¿¡æ¯:
- æ€»å…±å¤„ç†è‚¡ç¥¨: {total_stocks}
- æˆåŠŸä¿å­˜æ•°æ®çš„è‚¡ç¥¨: {successful_stocks}
- å¤±è´¥æ•°é‡: {failed_stocks}
- è‚¡ç¥¨å¤„ç†æˆåŠŸç‡: {successful_stocks/total_stocks*100:.1f}%
- æ•°æ®ä¿å­˜æˆåŠŸç‡: {total_success_count/total_attempts*100:.1f}%

å„æ•°æ®ç±»å‹æˆåŠŸç‡:
- æ—¥çº¿æ•°æ®: {daily_success}/{total_stocks} ({daily_success/total_stocks*100:.1f}%)
- 1åˆ†é’Ÿæ•°æ®: {min1_success}/{total_stocks} ({min1_success/total_stocks*100:.1f}%)
- 5åˆ†é’Ÿæ•°æ®: {min5_success}/{total_stocks} ({min5_success/total_stocks*100:.1f}%)
- 30åˆ†é’Ÿæ•°æ®: {min30_success}/{total_stocks} ({min30_success/total_stocks*100:.1f}%)

{storage_info}

æ•°æ®æ¥æº:
- xtquantåº“ (è¿…æŠ•é‡åŒ–)

æ•°æ®ç±»å‹:
- 1åˆ†é’ŸKçº¿æ•°æ®
- 5åˆ†é’ŸKçº¿æ•°æ®
- 30åˆ†é’ŸKçº¿æ•°æ®
- æ—¥çº¿Kçº¿æ•°æ®

æ³¨æ„äº‹é¡¹:
{file_info}
- éƒ¨åˆ†è‚¡ç¥¨å¯èƒ½å› ä¸ºæ•°æ®æºé™åˆ¶æ— æ³•è·å–å®Œæ•´æ•°æ®
- å»ºè®®å®šæœŸæ›´æ–°æ•°æ®
{process_info}

å¤±è´¥è‚¡ç¥¨æ•°é‡: {len(all_failed_stocks)}
{f"è¯¦ç»†å¤±è´¥ä¿¡æ¯è¯·æŸ¥çœ‹: failed_stocks_report.txt" if all_failed_stocks else ""}
"""
        
        report_filename = os.path.join(base_folder, "batch_csv_save_report.txt")
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(overall_report)
        
        logging.info(f"")
        logging.info(f"ğŸ‰ æ‰¹é‡ä¿å­˜å®Œæˆ!")
        logging.info(f"ğŸ“Š æ€»å…±å¤„ç† {total_stocks} åªè‚¡ç¥¨ï¼ŒæˆåŠŸ {successful_stocks} åªï¼Œå¤±è´¥ {failed_stocks} åª")
        logging.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
        logging.info(f"{'='*80}")
        
    except Exception as e:
        logging.error(f"ğŸ’¥ è¯»å–CSVæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

if __name__ == "__main__":
    main()